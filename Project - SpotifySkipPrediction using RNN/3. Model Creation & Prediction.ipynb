{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ElGZQ3AUYCUT"
   },
   "source": [
    "\n",
    "# Spotify Sequential Skip Prediction Challenge\n",
    "\n",
    "### Anna Martignano, Daniele Montesi, ID2223\n",
    "With  this  project  we  would  like  to  use  Deep  Learning  to  model  a  solution  to  the  challenge  ”  Spotify Sequential  Skip  Prediction  Challenge  -  Predict  if  users  will  skip  or  listen  to  the  music  they’re  streamed” published by Spotify in collaboration with WSDM and CrowdAI.\n",
    "\n",
    "Citing the CrowdAI Challenge Website:The task is to predict whether individual tracks encountered in a listening session will beskippedby a particular user.  In order to do this, complete information about the first half of a user’slistening session is provided, while the prediction is to be carried out on the second half.  \n",
    "\n",
    "The output of a prediction is abinary variable for each track in the second half of the session indicating if it was skipped or not, with a 1 indicating that the track skipped, and a 0 indicating that the track was not skipped.The  problem  corresponds  hence  to  a  binary  classification.   However,  the  problem  is  hard  and  must take into account all the history of the user tracks listened, making the model that best fits this problem aRecurrent Neural Networks.\n",
    "\n",
    "\n",
    "### Code organization\n",
    "\n",
    "\n",
    "The problem will present the following sections:\n",
    "\n",
    "1. Data exploration \n",
    "2. Data filtering/preprocessing \n",
    "3. Dataset creation\n",
    "4. Model reasoning and creation and fitting (First Baseline)\n",
    "5. Re-Implementation of the 5th solution\n",
    "6. Further model improvement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "URzWmxdoK9RF"
   },
   "source": [
    "## 0 - Libraries import and useful paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AXf41_dSX9xb"
   },
   "outputs": [],
   "source": [
    "!pip install -q pyyaml h5py  # Required to save models in HDF5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wICoIwG8YQnY",
    "outputId": "413d89c8-afcf-41a9-e161-ad8615ac04ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gR7TY3KhYUoB"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, utils, Model, Input\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import model_from_json, load_model\n",
    "\n",
    "from pickle import dump\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from joblib import delayed, Parallel\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import GroupKFold, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "Iar1KBmXYcU6",
    "outputId": "909d76af-9779-41d2-fed2-9c41a56a1fb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nwXWPtOYYeF7",
    "outputId": "59c1ef1e-7fcc-47c3-8b5e-1730026c39b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/Project/Dataset/track_features\n"
     ]
    }
   ],
   "source": [
    "dirpath = \"/content/drive/My Drive/Colab Notebooks/Project\"\n",
    "modelpath = dirpath + \"/Model\"\n",
    "datapath = dirpath + \"/Dataset\"\n",
    "processedpath = dirpath + \"/ProcessedDataset\"\n",
    "testpath = datapath + \"/test_set\"\n",
    "trainpath = datapath + \"/training_set\"\n",
    "featurespath = datapath + \"/track_features\"\n",
    "\n",
    "print(featurespath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r4Pb8c6_krs7"
   },
   "source": [
    "# 4- Loading the already preprocessed dataset\n",
    "\n",
    "We have preprocessed both the dataset TRAIN and TEST. \n",
    "We here load it from the files.\n",
    "\n",
    "Note: We need to use a modified versoin of no_load and restore the normal one, that's why we save a declaration of the old np_load before overriding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ewm3a_NYk1NV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# save np.load\n",
    "np_load_old = np.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bCvZT99xvwVH"
   },
   "outputs": [],
   "source": [
    "# Load Training set\n",
    "np.load = np_load_old\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "restored = np.load(processedpath + \"/train.npz\")\n",
    "train_history = restored['history_train']\n",
    "train_future = restored['future_train']\n",
    "train_labels = restored['labels_train']\n",
    "train_session_len = restored['session_len_train']\n",
    "\n",
    "del restored\n",
    "\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nbjch25FCTNP"
   },
   "outputs": [],
   "source": [
    "# Load Test Set\n",
    "\n",
    "np.load = np_load_old\n",
    "import numpy as np\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "restored2 = np.load(processedpath + \"/test.npz\")\n",
    "test_history = restored2['history_test']\n",
    "test_future = restored2['future_test']\n",
    "test_session_len = restored2['session_len_test']\n",
    "\n",
    "del restored2\n",
    "\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BTqAzySxaIEw"
   },
   "source": [
    "# 5- Model Fitting ad Evaluation Metrics \n",
    "\n",
    "To evaluate the performance of our solution we are going to use the same metrics proposed for this competition. \n",
    "\n",
    "The primary one is the **Mean Average Precision**:\n",
    "\n",
    "\\begin{equation*}\n",
    "AA   =  \\sum_{i=1}^{T} \\frac{A(i)L(i)}{T}\n",
    "\\end{equation*}\n",
    "\n",
    "The metric aims to weight higher the correct good prediction:\n",
    "\n",
    "\n",
    "- T is the number of tracks to be predicted for the given session\n",
    "- A(i) is the accuracy at position i of the sequence\n",
    "- L(i) is the boolean indicator for if the i‘th prediction was correct\n",
    "\n",
    "Gives high scores whether the first predicted labels are correct. Weights over all the sequences of predicted tracks starting from the beginning, till the end.\n",
    "\n",
    "e.g. \n",
    "MAP([1,0,1,1,0,1])\n",
    "  -> AA([1])\\\n",
    "  -> AA([1,0])\\\n",
    "  -> AA([1,0,1])\\\n",
    "  -> AA([1,0,1,1])\\\n",
    "  -> AA([1,0,1,1,0])\\\n",
    "  -> AA([1,0,1,1,0,1])\n",
    "\n",
    "Moreover, we are going to use the **First Prediction Accuracy** for the second half of the dataset's track\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ULyS9rmZywn"
   },
   "outputs": [],
   "source": [
    "def evaluation_MAP_FPA(sizes, label_matrix, prediction_matrix):\n",
    "    '''\n",
    "    Return MAP and first-prediction-accuracy\n",
    "    :param sizes:    A list of session sizes\n",
    "    :param labels:      The correct labels list [ [1,0,1,1,0,... size_of_truth_labels], ...]\n",
    "    :param predictions: The predicted labels list [ [1,0,1,0,0,... size_of_predicted_labels], ...]\n",
    "    :returns:           Mean Average Precision and First Prediction Accuracy\n",
    "    '''\n",
    "    # Set variables\n",
    "    score = 0.0\n",
    "    first_acc = 0.0\n",
    "    # For every session\n",
    "    for size, label_row, prediction_row in zip(sizes, label_matrix, prediction_matrix):\n",
    "        # Set variables\n",
    "        n_correct    = 0\n",
    "        session      = 0.0\n",
    "        # For the first 'size' predictions\n",
    "        for i in range(size):        \n",
    "            # If the prediction is correct:\n",
    "            if label_row[i] == prediction_row[i]:\n",
    "                # Increase counter of correct predictions\n",
    "                n_correct += 1\n",
    "                session += n_correct / (i + 1)\n",
    "                # If first prediction\n",
    "                if i == 0:\n",
    "                    first_acc += 1\n",
    "        # Save session score\n",
    "        score += session / size\n",
    "    return score/sizes.shape[0], first_acc/sizes.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7nfXdbjlbo1F"
   },
   "source": [
    "## Model Description\n",
    "\n",
    "**RNN architecture:** \n",
    "- to capture the time-correlation among data input **LSTM layers** are used to model the input sequences.\n",
    "Instead, dense layers are connected to future input tracks, the ones needed to be predicted.\n",
    "\n",
    "- Input sequences are modelled using doubly\n",
    "the outputs of these LSTM and dense layers are concatenated and fed into the tail of the network. \n",
    "\n",
    "- All dense layers are followed by\n",
    "exponential linear units (ELUs).\n",
    "\n",
    "**Avoid overfitting:** \n",
    "- **dropout** is applied, and as well **batch normalization **to allow more effective weights updates. When training, \n",
    "- we have also adopted **early stopping**, when the results do not improve for 5 epochs consecutively the best weights are \n",
    "\n",
    "**Personalization of the loss measure:** \n",
    "since the challenge gives more importance to the accuracy of the prediction of the first tracks in the session, we have opt to use a weighted variant of binary cross-entropy as loss function.\n",
    "\n",
    "**Optimization of the training procedure**\n",
    "We decided to employ Adam optimization which combines RMSProp(weighte version of the Adagrad optimization on the lr) and Momentum updates.\n",
    "Adam has amsgrad, in orer to apply the AMSGrad variant of this algorithm from the paper \"On the Convergence of Adam and Beyond\".\n",
    "\n",
    "Declared are the parameter of the best tuned model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ClTFbsThZ2G2"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "def generate_model(history, future, dropout_1=.35, dropout_2=.35, dropout_3=.25, lr= 0.001, amsgrad=True):\n",
    "    h_input  = tf.keras.layers.Input(shape = (history.shape[1], history.shape[2]))\n",
    "    h_subnet = tf.keras.layers.Dropout(dropout_1)(h_input)\n",
    "    h_subnet = tf.compat.v1.keras.layers.CuDNNLSTM(256, return_sequences = True)(h_subnet)\n",
    "    h_subnet = tf.compat.v1.keras.layers.CuDNNLSTM(256)(h_subnet)\n",
    "\n",
    "    f_input  = tf.keras.layers.Input(shape = (future.shape[1], future.shape[2]))\n",
    "    f_subnet = tf.keras.layers.Dropout(dropout_2)(f_input)\n",
    "    f_subnet = tf.compat.v1.keras.layers.CuDNNLSTM(128, return_sequences = True)(f_subnet)\n",
    "    f_subnet = tf.compat.v1.keras.layers.CuDNNLSTM(128)(f_subnet)\n",
    "\n",
    "    td_subnet = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(32))(f_input)\n",
    "    td_subnet = tf.keras.layers.Flatten()(td_subnet)\n",
    "\n",
    "    concat = tf.keras.layers.concatenate([h_subnet, f_subnet, td_subnet])\n",
    "    concat = tf.keras.layers.Dropout(dropout_3)(concat)\n",
    "\n",
    "    dense1  = tf.keras.layers.Dense(512, activation = 'elu')(concat)\n",
    "    dense1  = tf.keras.layers.BatchNormalization()(dense1)\n",
    "    dense2  = tf.keras.layers.Dense(512, activation = 'elu')(dense1)\n",
    "\n",
    "    outputs = [tf.keras.layers.Dense(1, activation = 'sigmoid')(dense2) for _ in range(10)]\n",
    "\n",
    "    model = tf.keras.Model(inputs = [h_input, f_input], outputs = outputs)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                                   optimizer = tf.keras.optimizers.Adam(lr = lr, amsgrad = amsgrad),\n",
    "                                   metrics=['accuracy'],\n",
    "                                   loss_weights = weights.tolist())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yOOVQ_o_vGUL"
   },
   "source": [
    "# The model architecture\n",
    "\n",
    "2 Inputes:\n",
    "- History Session & Tracks features (2x2) x num of batch samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 973
    },
    "colab_type": "code",
    "id": "ACD3ls7BthPw",
    "outputId": "4ac594e7-5f09-4b08-a9f1-072fc7e19c9a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABz0AAAOoCAIAAADEeH5QAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\nAACAtHwDyuHQ+pddK9kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = generate_model(train_history, train_future)\n",
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ddPUibP8JOD1"
   },
   "source": [
    "# Validation on hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "B9rjk52cJMjw",
    "outputId": "7b0927cc-9c43-4505-9609-f5a900697bd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "PARAMETERS CONFIG: {'dropout_1': 0.2, 'dropout_2': 0.2, 'dropout_3': 0.2, 'lr': 0.001, 'amsgrad': True}\n",
      "=========================================================\n",
      "2020-01-12 15:12:25.811698\t----- FOLD 0 -----\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 10s - loss: 2.0731 - dense_185_loss: 0.6140 - dense_186_loss: 0.6603 - dense_187_loss: 0.6736 - dense_188_loss: 0.6704 - dense_189_loss: 0.6678 - dense_190_loss: 0.6298 - dense_191_loss: 0.5578 - dense_192_loss: 0.5057 - dense_193_loss: 0.4679 - dense_194_loss: 0.4605 - dense_185_accuracy: 0.6789 - dense_186_accuracy: 0.6329 - dense_187_accuracy: 0.6140 - dense_188_accuracy: 0.6045 - dense_189_accuracy: 0.6174 - dense_190_accuracy: 0.6503 - dense_191_accuracy: 0.6946 - dense_192_accuracy: 0.7262 - dense_193_accuracy: 0.7456 - dense_194_accuracy: 0.7469 - val_loss: 1.9815 - val_dense_185_loss: 0.5421 - val_dense_186_loss: 0.6132 - val_dense_187_loss: 0.6346 - val_dense_188_loss: 0.6494 - val_dense_189_loss: 0.6490 - val_dense_190_loss: 0.6619 - val_dense_191_loss: 0.6717 - val_dense_192_loss: 0.5665 - val_dense_193_loss: 0.5395 - val_dense_194_loss: 0.5232 - val_dense_185_accuracy: 0.7482 - val_dense_186_accuracy: 0.6785 - val_dense_187_accuracy: 0.6474 - val_dense_188_accuracy: 0.6258 - val_dense_189_accuracy: 0.6327 - val_dense_190_accuracy: 0.6162 - val_dense_191_accuracy: 0.6318 - val_dense_192_accuracy: 0.6905 - val_dense_193_accuracy: 0.7219 - val_dense_194_accuracy: 0.7328\n",
      "2020-01-12 15:12:38.346386\t\tValid\tMAP:\t0.5189260990124598\tFPA:\t0.7482337505046427\n",
      "2020-01-12 15:12:38.776658\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5189260990124598\n",
      "2020-01-12 15:12:38.776855\t\tPredicting for test set...\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 4s - loss: 1.8998 - dense_185_loss: 0.5350 - dense_186_loss: 0.6091 - dense_187_loss: 0.6345 - dense_188_loss: 0.6456 - dense_189_loss: 0.6420 - dense_190_loss: 0.5966 - dense_191_loss: 0.5163 - dense_192_loss: 0.4502 - dense_193_loss: 0.4052 - dense_194_loss: 0.3809 - dense_185_accuracy: 0.7487 - dense_186_accuracy: 0.6796 - dense_187_accuracy: 0.6481 - dense_188_accuracy: 0.6341 - dense_189_accuracy: 0.6414 - dense_190_accuracy: 0.6775 - dense_191_accuracy: 0.7200 - dense_192_accuracy: 0.7534 - dense_193_accuracy: 0.7754 - dense_194_accuracy: 0.7833 - val_loss: 1.9040 - val_dense_185_loss: 0.5356 - val_dense_186_loss: 0.5944 - val_dense_187_loss: 0.6259 - val_dense_188_loss: 0.6399 - val_dense_189_loss: 0.6414 - val_dense_190_loss: 0.6053 - val_dense_191_loss: 0.5471 - val_dense_192_loss: 0.4958 - val_dense_193_loss: 0.4535 - val_dense_194_loss: 0.4167 - val_dense_185_accuracy: 0.7492 - val_dense_186_accuracy: 0.6944 - val_dense_187_accuracy: 0.6539 - val_dense_188_accuracy: 0.6370 - val_dense_189_accuracy: 0.6454 - val_dense_190_accuracy: 0.6757 - val_dense_191_accuracy: 0.7050 - val_dense_192_accuracy: 0.7268 - val_dense_193_accuracy: 0.7504 - val_dense_194_accuracy: 0.7674\n",
      "2020-01-12 15:12:45.775485\t\tValid\tMAP:\t0.5426171147048625\tFPA:\t0.7492093930830306\n",
      "2020-01-12 15:12:46.191980\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5426171147048625\n",
      "2020-01-12 15:12:46.192575\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:12:47.779879\t\tAverage best MAP over all folds:\t\t0.5307716068586612...\n",
      "=========================================================\n",
      "2020-01-12 15:12:47.791521\t----- FOLD 1 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8807 - dense_185_loss: 0.5280 - dense_186_loss: 0.6047 - dense_187_loss: 0.6287 - dense_188_loss: 0.6394 - dense_189_loss: 0.6372 - dense_190_loss: 0.5923 - dense_191_loss: 0.5081 - dense_192_loss: 0.4422 - dense_193_loss: 0.3964 - dense_194_loss: 0.3682 - dense_185_accuracy: 0.7520 - dense_186_accuracy: 0.6833 - dense_187_accuracy: 0.6531 - dense_188_accuracy: 0.6372 - dense_189_accuracy: 0.6448 - dense_190_accuracy: 0.6791 - dense_191_accuracy: 0.7243 - dense_192_accuracy: 0.7588 - dense_193_accuracy: 0.7815 - dense_194_accuracy: 0.7914 - val_loss: 1.8600 - val_dense_185_loss: 0.5213 - val_dense_186_loss: 0.5884 - val_dense_187_loss: 0.6191 - val_dense_188_loss: 0.6382 - val_dense_189_loss: 0.6291 - val_dense_190_loss: 0.5956 - val_dense_191_loss: 0.5177 - val_dense_192_loss: 0.4474 - val_dense_193_loss: 0.3993 - val_dense_194_loss: 0.3756 - val_dense_185_accuracy: 0.7629 - val_dense_186_accuracy: 0.6959 - val_dense_187_accuracy: 0.6647 - val_dense_188_accuracy: 0.6450 - val_dense_189_accuracy: 0.6487 - val_dense_190_accuracy: 0.6797 - val_dense_191_accuracy: 0.7226 - val_dense_192_accuracy: 0.7550 - val_dense_193_accuracy: 0.7832 - val_dense_194_accuracy: 0.7902\n",
      "2020-01-12 15:12:53.725378\t\tValid\tMAP:\t0.5606865415030583\tFPA:\t0.7629148653422376\n",
      "2020-01-12 15:12:54.076680\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5606865415030583\n",
      "2020-01-12 15:12:54.076881\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8619 - dense_185_loss: 0.5226 - dense_186_loss: 0.5985 - dense_187_loss: 0.6248 - dense_188_loss: 0.6336 - dense_189_loss: 0.6324 - dense_190_loss: 0.5866 - dense_191_loss: 0.5054 - dense_192_loss: 0.4381 - dense_193_loss: 0.3928 - dense_194_loss: 0.3633 - dense_185_accuracy: 0.7563 - dense_186_accuracy: 0.6869 - dense_187_accuracy: 0.6550 - dense_188_accuracy: 0.6440 - dense_189_accuracy: 0.6507 - dense_190_accuracy: 0.6829 - dense_191_accuracy: 0.7261 - dense_192_accuracy: 0.7615 - dense_193_accuracy: 0.7834 - dense_194_accuracy: 0.7946 - val_loss: 1.8336 - val_dense_185_loss: 0.5030 - val_dense_186_loss: 0.5868 - val_dense_187_loss: 0.6139 - val_dense_188_loss: 0.6253 - val_dense_189_loss: 0.6316 - val_dense_190_loss: 0.5907 - val_dense_191_loss: 0.5096 - val_dense_192_loss: 0.4438 - val_dense_193_loss: 0.3982 - val_dense_194_loss: 0.3748 - val_dense_185_accuracy: 0.7682 - val_dense_186_accuracy: 0.6962 - val_dense_187_accuracy: 0.6658 - val_dense_188_accuracy: 0.6515 - val_dense_189_accuracy: 0.6462 - val_dense_190_accuracy: 0.6761 - val_dense_191_accuracy: 0.7213 - val_dense_192_accuracy: 0.7533 - val_dense_193_accuracy: 0.7828 - val_dense_194_accuracy: 0.7894\n",
      "2020-01-12 15:13:01.064296\t\tValid\tMAP:\t0.5612720808502221\tFPA:\t0.7682473463757633\n",
      "2020-01-12 15:13:01.473364\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5612720808502221\n",
      "2020-01-12 15:13:01.473917\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:13:03.027635\t\tAverage best MAP over all folds:\t\t0.5458754590176507...\n",
      "=========================================================\n",
      "2020-01-12 15:13:03.073608\t----- FOLD 2 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8535 - dense_185_loss: 0.5197 - dense_186_loss: 0.5941 - dense_187_loss: 0.6217 - dense_188_loss: 0.6334 - dense_189_loss: 0.6309 - dense_190_loss: 0.5837 - dense_191_loss: 0.5007 - dense_192_loss: 0.4349 - dense_193_loss: 0.3915 - dense_194_loss: 0.3618 - dense_185_accuracy: 0.7565 - dense_186_accuracy: 0.6899 - dense_187_accuracy: 0.6589 - dense_188_accuracy: 0.6442 - dense_189_accuracy: 0.6513 - dense_190_accuracy: 0.6823 - dense_191_accuracy: 0.7292 - dense_192_accuracy: 0.7611 - dense_193_accuracy: 0.7834 - dense_194_accuracy: 0.7939 - val_loss: 1.8122 - val_dense_185_loss: 0.5015 - val_dense_186_loss: 0.5857 - val_dense_187_loss: 0.6170 - val_dense_188_loss: 0.6223 - val_dense_189_loss: 0.6210 - val_dense_190_loss: 0.5747 - val_dense_191_loss: 0.4890 - val_dense_192_loss: 0.4286 - val_dense_193_loss: 0.3774 - val_dense_194_loss: 0.3466 - val_dense_185_accuracy: 0.7730 - val_dense_186_accuracy: 0.6961 - val_dense_187_accuracy: 0.6633 - val_dense_188_accuracy: 0.6530 - val_dense_189_accuracy: 0.6570 - val_dense_190_accuracy: 0.6935 - val_dense_191_accuracy: 0.7348 - val_dense_192_accuracy: 0.7657 - val_dense_193_accuracy: 0.7924 - val_dense_194_accuracy: 0.8026\n",
      "2020-01-12 15:13:09.008433\t\tValid\tMAP:\t0.5683670976805818\tFPA:\t0.7729574242602655\n",
      "2020-01-12 15:13:09.555455\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5683670976805818\n",
      "2020-01-12 15:13:09.555940\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8417 - dense_185_loss: 0.5172 - dense_186_loss: 0.5901 - dense_187_loss: 0.6157 - dense_188_loss: 0.6289 - dense_189_loss: 0.6274 - dense_190_loss: 0.5798 - dense_191_loss: 0.4962 - dense_192_loss: 0.4324 - dense_193_loss: 0.3882 - dense_194_loss: 0.3592 - dense_185_accuracy: 0.7577 - dense_186_accuracy: 0.6917 - dense_187_accuracy: 0.6623 - dense_188_accuracy: 0.6472 - dense_189_accuracy: 0.6537 - dense_190_accuracy: 0.6864 - dense_191_accuracy: 0.7312 - dense_192_accuracy: 0.7639 - dense_193_accuracy: 0.7848 - dense_194_accuracy: 0.7967 - val_loss: 1.8153 - val_dense_185_loss: 0.4996 - val_dense_186_loss: 0.5867 - val_dense_187_loss: 0.6188 - val_dense_188_loss: 0.6243 - val_dense_189_loss: 0.6311 - val_dense_190_loss: 0.5760 - val_dense_191_loss: 0.4978 - val_dense_192_loss: 0.4326 - val_dense_193_loss: 0.3815 - val_dense_194_loss: 0.3471 - val_dense_185_accuracy: 0.7738 - val_dense_186_accuracy: 0.6949 - val_dense_187_accuracy: 0.6669 - val_dense_188_accuracy: 0.6498 - val_dense_189_accuracy: 0.6540 - val_dense_190_accuracy: 0.6933 - val_dense_191_accuracy: 0.7354 - val_dense_192_accuracy: 0.7670 - val_dense_193_accuracy: 0.7909 - val_dense_194_accuracy: 0.8042\n",
      "2020-01-12 15:13:16.631017\t\tValid\tMAP:\t0.5702102038952052\tFPA:\t0.773764866183323\n",
      "2020-01-12 15:13:16.997395\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5702102038952052\n",
      "2020-01-12 15:13:16.997646\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:13:18.600428\t\tAverage best MAP over all folds:\t\t0.5536798562743982...\n",
      "=========================================================\n",
      "=========================================================\n",
      "PARAMETERS CONFIG: {'dropout_1': 0.3, 'dropout_2': 0.3, 'dropout_3': 0.3, 'lr': 0.001, 'amsgrad': True}\n",
      "=========================================================\n",
      "2020-01-12 15:13:19.958908\t----- FOLD 0 -----\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 11s - loss: 2.0748 - dense_198_loss: 0.6210 - dense_199_loss: 0.6484 - dense_200_loss: 0.6677 - dense_201_loss: 0.6751 - dense_202_loss: 0.6786 - dense_203_loss: 0.6322 - dense_204_loss: 0.5561 - dense_205_loss: 0.5002 - dense_206_loss: 0.4684 - dense_207_loss: 0.4555 - dense_198_accuracy: 0.6706 - dense_199_accuracy: 0.6362 - dense_200_accuracy: 0.6127 - dense_201_accuracy: 0.6021 - dense_202_accuracy: 0.6079 - dense_203_accuracy: 0.6501 - dense_204_accuracy: 0.6963 - dense_205_accuracy: 0.7260 - dense_206_accuracy: 0.7454 - dense_207_accuracy: 0.7509 - val_loss: 1.9801 - val_dense_198_loss: 0.5503 - val_dense_199_loss: 0.6054 - val_dense_200_loss: 0.6243 - val_dense_201_loss: 0.6421 - val_dense_202_loss: 0.6478 - val_dense_203_loss: 0.6445 - val_dense_204_loss: 0.6204 - val_dense_205_loss: 0.6067 - val_dense_206_loss: 0.6062 - val_dense_207_loss: 0.5951 - val_dense_198_accuracy: 0.7518 - val_dense_199_accuracy: 0.6882 - val_dense_200_accuracy: 0.6597 - val_dense_201_accuracy: 0.6324 - val_dense_202_accuracy: 0.6337 - val_dense_203_accuracy: 0.6385 - val_dense_204_accuracy: 0.6505 - val_dense_205_accuracy: 0.6766 - val_dense_206_accuracy: 0.7021 - val_dense_207_accuracy: 0.7098\n",
      "2020-01-12 15:13:33.280456\t\tValid\tMAP:\t0.5247611342620272\tFPA:\t0.7518167137666532\n",
      "2020-01-12 15:13:33.680349\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5247611342620272\n",
      "2020-01-12 15:13:33.680945\t\tPredicting for test set...\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 4s - loss: 1.9182 - dense_198_loss: 0.5502 - dense_199_loss: 0.6162 - dense_200_loss: 0.6356 - dense_201_loss: 0.6468 - dense_202_loss: 0.6421 - dense_203_loss: 0.5990 - dense_204_loss: 0.5146 - dense_205_loss: 0.4490 - dense_206_loss: 0.4048 - dense_207_loss: 0.3801 - dense_198_accuracy: 0.7357 - dense_199_accuracy: 0.6737 - dense_200_accuracy: 0.6455 - dense_201_accuracy: 0.6317 - dense_202_accuracy: 0.6408 - dense_203_accuracy: 0.6769 - dense_204_accuracy: 0.7227 - dense_205_accuracy: 0.7524 - dense_206_accuracy: 0.7752 - dense_207_accuracy: 0.7837 - val_loss: 1.9453 - val_dense_198_loss: 0.5354 - val_dense_199_loss: 0.6141 - val_dense_200_loss: 0.6295 - val_dense_201_loss: 0.6639 - val_dense_202_loss: 0.6589 - val_dense_203_loss: 0.6341 - val_dense_204_loss: 0.5723 - val_dense_205_loss: 0.5361 - val_dense_206_loss: 0.4642 - val_dense_207_loss: 0.4388 - val_dense_198_accuracy: 0.7505 - val_dense_199_accuracy: 0.6798 - val_dense_200_accuracy: 0.6526 - val_dense_201_accuracy: 0.6168 - val_dense_202_accuracy: 0.6212 - val_dense_203_accuracy: 0.6536 - val_dense_204_accuracy: 0.6765 - val_dense_205_accuracy: 0.6932 - val_dense_206_accuracy: 0.7384 - val_dense_207_accuracy: 0.7552\n",
      "2020-01-12 15:13:40.677136\t\tValid\tMAP:\t0.5288161935183026\tFPA:\t0.7504709998654286\n",
      "2020-01-12 15:13:41.009562\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5288161935183026\n",
      "2020-01-12 15:13:41.009783\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:13:42.618439\t\tAverage best MAP over all folds:\t\t0.5267886638901649...\n",
      "=========================================================\n",
      "2020-01-12 15:13:42.629560\t----- FOLD 1 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.9027 - dense_198_loss: 0.5409 - dense_199_loss: 0.6108 - dense_200_loss: 0.6322 - dense_201_loss: 0.6472 - dense_202_loss: 0.6411 - dense_203_loss: 0.5942 - dense_204_loss: 0.5100 - dense_205_loss: 0.4431 - dense_206_loss: 0.3966 - dense_207_loss: 0.3682 - dense_198_accuracy: 0.7415 - dense_199_accuracy: 0.6776 - dense_200_accuracy: 0.6480 - dense_201_accuracy: 0.6298 - dense_202_accuracy: 0.6432 - dense_203_accuracy: 0.6780 - dense_204_accuracy: 0.7231 - dense_205_accuracy: 0.7570 - dense_206_accuracy: 0.7806 - dense_207_accuracy: 0.7908 - val_loss: 1.9048 - val_dense_198_loss: 0.5626 - val_dense_199_loss: 0.5973 - val_dense_200_loss: 0.6207 - val_dense_201_loss: 0.6330 - val_dense_202_loss: 0.6294 - val_dense_203_loss: 0.5869 - val_dense_204_loss: 0.5032 - val_dense_205_loss: 0.4374 - val_dense_206_loss: 0.3981 - val_dense_207_loss: 0.3680 - val_dense_198_accuracy: 0.7476 - val_dense_199_accuracy: 0.6908 - val_dense_200_accuracy: 0.6618 - val_dense_201_accuracy: 0.6432 - val_dense_202_accuracy: 0.6516 - val_dense_203_accuracy: 0.6860 - val_dense_204_accuracy: 0.7321 - val_dense_205_accuracy: 0.7635 - val_dense_206_accuracy: 0.7844 - val_dense_207_accuracy: 0.7998\n",
      "2020-01-12 15:13:48.869093\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5588408853975176\n",
      "2020-01-12 15:13:48.869245\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8857 - dense_198_loss: 0.5369 - dense_199_loss: 0.6056 - dense_200_loss: 0.6271 - dense_201_loss: 0.6384 - dense_202_loss: 0.6357 - dense_203_loss: 0.5884 - dense_204_loss: 0.5062 - dense_205_loss: 0.4423 - dense_206_loss: 0.3970 - dense_207_loss: 0.3694 - dense_198_accuracy: 0.7468 - dense_199_accuracy: 0.6821 - dense_200_accuracy: 0.6516 - dense_201_accuracy: 0.6387 - dense_202_accuracy: 0.6463 - dense_203_accuracy: 0.6789 - dense_204_accuracy: 0.7254 - dense_205_accuracy: 0.7590 - dense_206_accuracy: 0.7811 - dense_207_accuracy: 0.7897 - val_loss: 1.8360 - val_dense_198_loss: 0.5092 - val_dense_199_loss: 0.5940 - val_dense_200_loss: 0.6187 - val_dense_201_loss: 0.6266 - val_dense_202_loss: 0.6262 - val_dense_203_loss: 0.5807 - val_dense_204_loss: 0.4930 - val_dense_205_loss: 0.4283 - val_dense_206_loss: 0.3825 - val_dense_207_loss: 0.3529 - val_dense_198_accuracy: 0.7621 - val_dense_199_accuracy: 0.6944 - val_dense_200_accuracy: 0.6627 - val_dense_201_accuracy: 0.6494 - val_dense_202_accuracy: 0.6542 - val_dense_203_accuracy: 0.6871 - val_dense_204_accuracy: 0.7339 - val_dense_205_accuracy: 0.7650 - val_dense_206_accuracy: 0.7904 - val_dense_207_accuracy: 0.8027\n",
      "2020-01-12 15:13:55.845940\t\tValid\tMAP:\t0.5641517882346876\tFPA:\t0.762056958298989\n",
      "2020-01-12 15:13:56.242838\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5641517882346876\n",
      "2020-01-12 15:13:56.242968\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:13:57.812193\t\tAverage best MAP over all folds:\t\t0.5441425003531337...\n",
      "=========================================================\n",
      "2020-01-12 15:13:57.860470\t----- FOLD 2 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8760 - dense_198_loss: 0.5355 - dense_199_loss: 0.6007 - dense_200_loss: 0.6236 - dense_201_loss: 0.6365 - dense_202_loss: 0.6337 - dense_203_loss: 0.5848 - dense_204_loss: 0.5033 - dense_205_loss: 0.4383 - dense_206_loss: 0.3927 - dense_207_loss: 0.3647 - dense_198_accuracy: 0.7457 - dense_199_accuracy: 0.6844 - dense_200_accuracy: 0.6552 - dense_201_accuracy: 0.6403 - dense_202_accuracy: 0.6476 - dense_203_accuracy: 0.6829 - dense_204_accuracy: 0.7270 - dense_205_accuracy: 0.7607 - dense_206_accuracy: 0.7827 - dense_207_accuracy: 0.7924 - val_loss: 1.8276 - val_dense_198_loss: 0.5065 - val_dense_199_loss: 0.5909 - val_dense_200_loss: 0.6186 - val_dense_201_loss: 0.6342 - val_dense_202_loss: 0.6245 - val_dense_203_loss: 0.5798 - val_dense_204_loss: 0.4917 - val_dense_205_loss: 0.4327 - val_dense_206_loss: 0.3814 - val_dense_207_loss: 0.3479 - val_dense_198_accuracy: 0.7688 - val_dense_199_accuracy: 0.6932 - val_dense_200_accuracy: 0.6618 - val_dense_201_accuracy: 0.6403 - val_dense_202_accuracy: 0.6555 - val_dense_203_accuracy: 0.6862 - val_dense_204_accuracy: 0.7319 - val_dense_205_accuracy: 0.7609 - val_dense_206_accuracy: 0.7883 - val_dense_207_accuracy: 0.8007\n",
      "2020-01-12 15:14:03.820513\t\tValid\tMAP:\t0.5628369802095858\tFPA:\t0.7687688192844045\n",
      "2020-01-12 15:14:04.230877\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5628369802095858\n",
      "2020-01-12 15:14:04.231515\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8599 - dense_198_loss: 0.5275 - dense_199_loss: 0.5982 - dense_200_loss: 0.6207 - dense_201_loss: 0.6337 - dense_202_loss: 0.6323 - dense_203_loss: 0.5794 - dense_204_loss: 0.4987 - dense_205_loss: 0.4339 - dense_206_loss: 0.3879 - dense_207_loss: 0.3593 - dense_198_accuracy: 0.7499 - dense_199_accuracy: 0.6862 - dense_200_accuracy: 0.6585 - dense_201_accuracy: 0.6444 - dense_202_accuracy: 0.6494 - dense_203_accuracy: 0.6845 - dense_204_accuracy: 0.7302 - dense_205_accuracy: 0.7637 - dense_206_accuracy: 0.7849 - dense_207_accuracy: 0.7951 - val_loss: 1.8181 - val_dense_198_loss: 0.5055 - val_dense_199_loss: 0.5871 - val_dense_200_loss: 0.6157 - val_dense_201_loss: 0.6236 - val_dense_202_loss: 0.6321 - val_dense_203_loss: 0.5751 - val_dense_204_loss: 0.4894 - val_dense_205_loss: 0.4266 - val_dense_206_loss: 0.3804 - val_dense_207_loss: 0.3495 - val_dense_198_accuracy: 0.7707 - val_dense_199_accuracy: 0.6970 - val_dense_200_accuracy: 0.6657 - val_dense_201_accuracy: 0.6535 - val_dense_202_accuracy: 0.6507 - val_dense_203_accuracy: 0.6909 - val_dense_204_accuracy: 0.7333 - val_dense_205_accuracy: 0.7642 - val_dense_206_accuracy: 0.7915 - val_dense_207_accuracy: 0.8004\n",
      "2020-01-12 15:14:11.282110\t\tValid\tMAP:\t0.5677431317265506\tFPA:\t0.7707033155583966\n",
      "2020-01-12 15:14:11.728210\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5677431317265506\n",
      "2020-01-12 15:14:11.728854\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:14:13.323327\t\tAverage best MAP over all folds:\t\t0.5511916855581119...\n",
      "=========================================================\n",
      "=========================================================\n",
      "PARAMETERS CONFIG: {'dropout_1': 0.4, 'dropout_2': 0.4, 'dropout_3': 0.4, 'lr': 0.001, 'amsgrad': True}\n",
      "=========================================================\n",
      "2020-01-12 15:14:14.720023\t----- FOLD 0 -----\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 10s - loss: 2.0873 - dense_211_loss: 0.6263 - dense_212_loss: 0.6578 - dense_213_loss: 0.6680 - dense_214_loss: 0.6763 - dense_215_loss: 0.6741 - dense_216_loss: 0.6351 - dense_217_loss: 0.5632 - dense_218_loss: 0.5055 - dense_219_loss: 0.4699 - dense_220_loss: 0.4599 - dense_211_accuracy: 0.6620 - dense_212_accuracy: 0.6276 - dense_213_accuracy: 0.6083 - dense_214_accuracy: 0.6003 - dense_215_accuracy: 0.6091 - dense_216_accuracy: 0.6478 - dense_217_accuracy: 0.6942 - dense_218_accuracy: 0.7235 - dense_219_accuracy: 0.7442 - dense_220_accuracy: 0.7452 - val_loss: 1.9827 - val_dense_211_loss: 0.5341 - val_dense_212_loss: 0.6108 - val_dense_213_loss: 0.6293 - val_dense_214_loss: 0.6417 - val_dense_215_loss: 0.6463 - val_dense_216_loss: 0.6572 - val_dense_217_loss: 0.6506 - val_dense_218_loss: 0.6321 - val_dense_219_loss: 0.6164 - val_dense_220_loss: 0.6445 - val_dense_211_accuracy: 0.7505 - val_dense_212_accuracy: 0.6777 - val_dense_213_accuracy: 0.6519 - val_dense_214_accuracy: 0.6342 - val_dense_215_accuracy: 0.6340 - val_dense_216_accuracy: 0.6330 - val_dense_217_accuracy: 0.6418 - val_dense_218_accuracy: 0.6796 - val_dense_219_accuracy: 0.7060 - val_dense_220_accuracy: 0.6927\n",
      "2020-01-12 15:14:27.409912\t\tValid\tMAP:\t0.5212179567875981\tFPA:\t0.7505382855604898\n",
      "2020-01-12 15:14:27.924897\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5212179567875981\n",
      "2020-01-12 15:14:27.925055\t\tPredicting for test set...\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 4s - loss: 1.9422 - dense_211_loss: 0.5626 - dense_212_loss: 0.6200 - dense_213_loss: 0.6406 - dense_214_loss: 0.6493 - dense_215_loss: 0.6457 - dense_216_loss: 0.6034 - dense_217_loss: 0.5241 - dense_218_loss: 0.4547 - dense_219_loss: 0.4097 - dense_220_loss: 0.3842 - dense_211_accuracy: 0.7255 - dense_212_accuracy: 0.6679 - dense_213_accuracy: 0.6422 - dense_214_accuracy: 0.6281 - dense_215_accuracy: 0.6376 - dense_216_accuracy: 0.6726 - dense_217_accuracy: 0.7172 - dense_218_accuracy: 0.7520 - dense_219_accuracy: 0.7733 - dense_220_accuracy: 0.7820 - val_loss: 1.9471 - val_dense_211_loss: 0.5583 - val_dense_212_loss: 0.6068 - val_dense_213_loss: 0.6270 - val_dense_214_loss: 0.6397 - val_dense_215_loss: 0.6500 - val_dense_216_loss: 0.6217 - val_dense_217_loss: 0.5530 - val_dense_218_loss: 0.5065 - val_dense_219_loss: 0.4708 - val_dense_220_loss: 0.4262 - val_dense_211_accuracy: 0.7387 - val_dense_212_accuracy: 0.6808 - val_dense_213_accuracy: 0.6499 - val_dense_214_accuracy: 0.6394 - val_dense_215_accuracy: 0.6369 - val_dense_216_accuracy: 0.6660 - val_dense_217_accuracy: 0.7015 - val_dense_218_accuracy: 0.7047 - val_dense_219_accuracy: 0.7393 - val_dense_220_accuracy: 0.7651\n",
      "2020-01-12 15:14:34.963049\t\tValid\tMAP:\t0.5348670689370487\tFPA:\t0.738729646077244\n",
      "2020-01-12 15:14:35.344406\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5348670689370487\n",
      "2020-01-12 15:14:35.344624\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:14:36.943832\t\tAverage best MAP over all folds:\t\t0.5280425128623234...\n",
      "=========================================================\n",
      "2020-01-12 15:14:36.955560\t----- FOLD 1 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.9205 - dense_211_loss: 0.5528 - dense_212_loss: 0.6148 - dense_213_loss: 0.6353 - dense_214_loss: 0.6453 - dense_215_loss: 0.6429 - dense_216_loss: 0.5986 - dense_217_loss: 0.5171 - dense_218_loss: 0.4465 - dense_219_loss: 0.4009 - dense_220_loss: 0.3744 - dense_211_accuracy: 0.7325 - dense_212_accuracy: 0.6718 - dense_213_accuracy: 0.6439 - dense_214_accuracy: 0.6330 - dense_215_accuracy: 0.6398 - dense_216_accuracy: 0.6743 - dense_217_accuracy: 0.7199 - dense_218_accuracy: 0.7552 - dense_219_accuracy: 0.7778 - dense_220_accuracy: 0.7873 - val_loss: 1.9044 - val_dense_211_loss: 0.5418 - val_dense_212_loss: 0.6121 - val_dense_213_loss: 0.6345 - val_dense_214_loss: 0.6326 - val_dense_215_loss: 0.6377 - val_dense_216_loss: 0.5927 - val_dense_217_loss: 0.5141 - val_dense_218_loss: 0.4468 - val_dense_219_loss: 0.4025 - val_dense_220_loss: 0.3836 - val_dense_211_accuracy: 0.7471 - val_dense_212_accuracy: 0.6717 - val_dense_213_accuracy: 0.6579 - val_dense_214_accuracy: 0.6437 - val_dense_215_accuracy: 0.6453 - val_dense_216_accuracy: 0.6742 - val_dense_217_accuracy: 0.7267 - val_dense_218_accuracy: 0.7510 - val_dense_219_accuracy: 0.7771 - val_dense_220_accuracy: 0.7894\n",
      "2020-01-12 15:14:42.902218\t\tValid\tMAP:\t0.5483436967777129\tFPA:\t0.7470519958955035\n",
      "2020-01-12 15:14:43.318947\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5483436967777129\n",
      "2020-01-12 15:14:43.319107\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.9062 - dense_211_loss: 0.5489 - dense_212_loss: 0.6099 - dense_213_loss: 0.6323 - dense_214_loss: 0.6405 - dense_215_loss: 0.6389 - dense_216_loss: 0.5908 - dense_217_loss: 0.5105 - dense_218_loss: 0.4422 - dense_219_loss: 0.3981 - dense_220_loss: 0.3706 - dense_211_accuracy: 0.7354 - dense_212_accuracy: 0.6764 - dense_213_accuracy: 0.6476 - dense_214_accuracy: 0.6358 - dense_215_accuracy: 0.6433 - dense_216_accuracy: 0.6778 - dense_217_accuracy: 0.7205 - dense_218_accuracy: 0.7580 - dense_219_accuracy: 0.7778 - dense_220_accuracy: 0.7875 - val_loss: 1.8524 - val_dense_211_loss: 0.5218 - val_dense_212_loss: 0.5937 - val_dense_213_loss: 0.6191 - val_dense_214_loss: 0.6355 - val_dense_215_loss: 0.6255 - val_dense_216_loss: 0.5759 - val_dense_217_loss: 0.4933 - val_dense_218_loss: 0.4321 - val_dense_219_loss: 0.3860 - val_dense_220_loss: 0.3555 - val_dense_211_accuracy: 0.7562 - val_dense_212_accuracy: 0.6916 - val_dense_213_accuracy: 0.6623 - val_dense_214_accuracy: 0.6406 - val_dense_215_accuracy: 0.6538 - val_dense_216_accuracy: 0.6888 - val_dense_217_accuracy: 0.7343 - val_dense_218_accuracy: 0.7663 - val_dense_219_accuracy: 0.7899 - val_dense_220_accuracy: 0.8031\n",
      "2020-01-12 15:14:50.328311\t\tValid\tMAP:\t0.5618194343933325\tFPA:\t0.7562030043568221\n",
      "2020-01-12 15:14:50.691884\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5618194343933325\n",
      "2020-01-12 15:14:50.692002\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:14:52.266421\t\tAverage best MAP over all folds:\t\t0.5415620392239231...\n",
      "=========================================================\n",
      "2020-01-12 15:14:52.316383\t----- FOLD 2 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8929 - dense_211_loss: 0.5454 - dense_212_loss: 0.6065 - dense_213_loss: 0.6278 - dense_214_loss: 0.6402 - dense_215_loss: 0.6364 - dense_216_loss: 0.5892 - dense_217_loss: 0.5060 - dense_218_loss: 0.4399 - dense_219_loss: 0.3933 - dense_220_loss: 0.3666 - dense_211_accuracy: 0.7381 - dense_212_accuracy: 0.6791 - dense_213_accuracy: 0.6509 - dense_214_accuracy: 0.6359 - dense_215_accuracy: 0.6455 - dense_216_accuracy: 0.6792 - dense_217_accuracy: 0.7267 - dense_218_accuracy: 0.7597 - dense_219_accuracy: 0.7819 - dense_220_accuracy: 0.7923 - val_loss: 1.8367 - val_dense_211_loss: 0.5150 - val_dense_212_loss: 0.5939 - val_dense_213_loss: 0.6225 - val_dense_214_loss: 0.6241 - val_dense_215_loss: 0.6239 - val_dense_216_loss: 0.5812 - val_dense_217_loss: 0.4926 - val_dense_218_loss: 0.4301 - val_dense_219_loss: 0.3798 - val_dense_220_loss: 0.3504 - val_dense_211_accuracy: 0.7677 - val_dense_212_accuracy: 0.6935 - val_dense_213_accuracy: 0.6560 - val_dense_214_accuracy: 0.6500 - val_dense_215_accuracy: 0.6553 - val_dense_216_accuracy: 0.6840 - val_dense_217_accuracy: 0.7277 - val_dense_218_accuracy: 0.7631 - val_dense_219_accuracy: 0.7913 - val_dense_220_accuracy: 0.8041\n",
      "2020-01-12 15:14:58.295355\t\tValid\tMAP:\t0.563324552703824\tFPA:\t0.7676922300536613\n",
      "2020-01-12 15:14:58.705142\t\tStopping at epoch 0, best epoch was 0 with MAP 0.563324552703824\n",
      "2020-01-12 15:14:58.705621\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8865 - dense_211_loss: 0.5413 - dense_212_loss: 0.6045 - dense_213_loss: 0.6271 - dense_214_loss: 0.6376 - dense_215_loss: 0.6346 - dense_216_loss: 0.5864 - dense_217_loss: 0.5019 - dense_218_loss: 0.4376 - dense_219_loss: 0.3920 - dense_220_loss: 0.3659 - dense_211_accuracy: 0.7398 - dense_212_accuracy: 0.6797 - dense_213_accuracy: 0.6497 - dense_214_accuracy: 0.6374 - dense_215_accuracy: 0.6461 - dense_216_accuracy: 0.6815 - dense_217_accuracy: 0.7282 - dense_218_accuracy: 0.7599 - dense_219_accuracy: 0.7831 - dense_220_accuracy: 0.7913 - val_loss: 1.8291 - val_dense_211_loss: 0.5057 - val_dense_212_loss: 0.5974 - val_dense_213_loss: 0.6281 - val_dense_214_loss: 0.6252 - val_dense_215_loss: 0.6228 - val_dense_216_loss: 0.5769 - val_dense_217_loss: 0.4962 - val_dense_218_loss: 0.4300 - val_dense_219_loss: 0.3800 - val_dense_220_loss: 0.3540 - val_dense_211_accuracy: 0.7683 - val_dense_212_accuracy: 0.6881 - val_dense_213_accuracy: 0.6655 - val_dense_214_accuracy: 0.6489 - val_dense_215_accuracy: 0.6585 - val_dense_216_accuracy: 0.6847 - val_dense_217_accuracy: 0.7316 - val_dense_218_accuracy: 0.7650 - val_dense_219_accuracy: 0.7918 - val_dense_220_accuracy: 0.8009\n",
      "2020-01-12 15:15:05.756107\t\tValid\tMAP:\t0.5641760488792489\tFPA:\t0.7683314549094151\n",
      "2020-01-12 15:15:06.084126\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5641760488792489\n",
      "2020-01-12 15:15:06.084667\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:15:07.669316\t\tAverage best MAP over all folds:\t\t0.5489581264131275...\n",
      "=========================================================\n",
      "=========================================================\n",
      "PARAMETERS CONFIG: {'dropout_1': 0.35, 'dropout_2': 0.35, 'dropout_3': 0.35, 'lr': 0.001, 'amsgrad': True}\n",
      "=========================================================\n",
      "2020-01-12 15:15:09.041086\t----- FOLD 0 -----\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 10s - loss: 2.0954 - dense_224_loss: 0.6265 - dense_225_loss: 0.6671 - dense_226_loss: 0.6697 - dense_227_loss: 0.6773 - dense_228_loss: 0.6731 - dense_229_loss: 0.6347 - dense_230_loss: 0.5615 - dense_231_loss: 0.5014 - dense_232_loss: 0.4724 - dense_233_loss: 0.4530 - dense_224_accuracy: 0.6663 - dense_225_accuracy: 0.6239 - dense_226_accuracy: 0.6092 - dense_227_accuracy: 0.5992 - dense_228_accuracy: 0.6082 - dense_229_accuracy: 0.6462 - dense_230_accuracy: 0.6925 - dense_231_accuracy: 0.7249 - dense_232_accuracy: 0.7424 - dense_233_accuracy: 0.7496 - val_loss: 1.9933 - val_dense_224_loss: 0.5510 - val_dense_225_loss: 0.6016 - val_dense_226_loss: 0.6293 - val_dense_227_loss: 0.6420 - val_dense_228_loss: 0.6561 - val_dense_229_loss: 0.6507 - val_dense_230_loss: 0.6662 - val_dense_231_loss: 0.6410 - val_dense_232_loss: 0.5721 - val_dense_233_loss: 0.5888 - val_dense_224_accuracy: 0.7349 - val_dense_225_accuracy: 0.6874 - val_dense_226_accuracy: 0.6570 - val_dense_227_accuracy: 0.6354 - val_dense_228_accuracy: 0.6221 - val_dense_229_accuracy: 0.6317 - val_dense_230_accuracy: 0.6373 - val_dense_231_accuracy: 0.6754 - val_dense_232_accuracy: 0.7186 - val_dense_233_accuracy: 0.7234\n",
      "2020-01-12 15:15:22.353523\t\tValid\tMAP:\t0.5179075742664276\tFPA:\t0.7349280043062845\n",
      "2020-01-12 15:15:22.736878\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5179075742664276\n",
      "2020-01-12 15:15:22.737027\t\tPredicting for test set...\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 4s - loss: 1.9316 - dense_224_loss: 0.5559 - dense_225_loss: 0.6178 - dense_226_loss: 0.6380 - dense_227_loss: 0.6464 - dense_228_loss: 0.6449 - dense_229_loss: 0.5993 - dense_230_loss: 0.5196 - dense_231_loss: 0.4537 - dense_232_loss: 0.4095 - dense_233_loss: 0.3825 - dense_224_accuracy: 0.7296 - dense_225_accuracy: 0.6689 - dense_226_accuracy: 0.6420 - dense_227_accuracy: 0.6306 - dense_228_accuracy: 0.6401 - dense_229_accuracy: 0.6756 - dense_230_accuracy: 0.7186 - dense_231_accuracy: 0.7510 - dense_232_accuracy: 0.7738 - dense_233_accuracy: 0.7824 - val_loss: 1.9131 - val_dense_224_loss: 0.5359 - val_dense_225_loss: 0.6062 - val_dense_226_loss: 0.6230 - val_dense_227_loss: 0.6407 - val_dense_228_loss: 0.6390 - val_dense_229_loss: 0.6122 - val_dense_230_loss: 0.5465 - val_dense_231_loss: 0.5057 - val_dense_232_loss: 0.4485 - val_dense_233_loss: 0.4397 - val_dense_224_accuracy: 0.7450 - val_dense_225_accuracy: 0.6841 - val_dense_226_accuracy: 0.6572 - val_dense_227_accuracy: 0.6370 - val_dense_228_accuracy: 0.6407 - val_dense_229_accuracy: 0.6668 - val_dense_230_accuracy: 0.7015 - val_dense_231_accuracy: 0.7191 - val_dense_232_accuracy: 0.7548 - val_dense_233_accuracy: 0.7575\n",
      "2020-01-12 15:15:29.741981\t\tValid\tMAP:\t0.5408883721953324\tFPA:\t0.7450376799892343\n",
      "2020-01-12 15:15:30.111761\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5408883721953324\n",
      "2020-01-12 15:15:30.112425\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:15:31.706683\t\tAverage best MAP over all folds:\t\t0.52939797323088...\n",
      "=========================================================\n",
      "2020-01-12 15:15:31.717899\t----- FOLD 1 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.9080 - dense_224_loss: 0.5454 - dense_225_loss: 0.6120 - dense_226_loss: 0.6352 - dense_227_loss: 0.6446 - dense_228_loss: 0.6409 - dense_229_loss: 0.5936 - dense_230_loss: 0.5132 - dense_231_loss: 0.4435 - dense_232_loss: 0.4012 - dense_233_loss: 0.3715 - dense_224_accuracy: 0.7375 - dense_225_accuracy: 0.6751 - dense_226_accuracy: 0.6472 - dense_227_accuracy: 0.6338 - dense_228_accuracy: 0.6436 - dense_229_accuracy: 0.6780 - dense_230_accuracy: 0.7221 - dense_231_accuracy: 0.7565 - dense_232_accuracy: 0.7773 - dense_233_accuracy: 0.7881 - val_loss: 1.8776 - val_dense_224_loss: 0.5212 - val_dense_225_loss: 0.5945 - val_dense_226_loss: 0.6343 - val_dense_227_loss: 0.6323 - val_dense_228_loss: 0.6388 - val_dense_229_loss: 0.5895 - val_dense_230_loss: 0.5201 - val_dense_231_loss: 0.4782 - val_dense_232_loss: 0.4001 - val_dense_233_loss: 0.3876 - val_dense_224_accuracy: 0.7578 - val_dense_225_accuracy: 0.6928 - val_dense_226_accuracy: 0.6530 - val_dense_227_accuracy: 0.6443 - val_dense_228_accuracy: 0.6409 - val_dense_229_accuracy: 0.6859 - val_dense_230_accuracy: 0.7159 - val_dense_231_accuracy: 0.7405 - val_dense_232_accuracy: 0.7802 - val_dense_233_accuracy: 0.7824\n",
      "2020-01-12 15:15:37.782678\t\tValid\tMAP:\t0.5534374680950283\tFPA:\t0.7577506013760156\n",
      "2020-01-12 15:15:38.193610\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5534374680950283\n",
      "2020-01-12 15:15:38.193826\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8930 - dense_224_loss: 0.5384 - dense_225_loss: 0.6078 - dense_226_loss: 0.6304 - dense_227_loss: 0.6409 - dense_228_loss: 0.6359 - dense_229_loss: 0.5891 - dense_230_loss: 0.5068 - dense_231_loss: 0.4390 - dense_232_loss: 0.3948 - dense_233_loss: 0.3673 - dense_224_accuracy: 0.7419 - dense_225_accuracy: 0.6786 - dense_226_accuracy: 0.6487 - dense_227_accuracy: 0.6374 - dense_228_accuracy: 0.6462 - dense_229_accuracy: 0.6792 - dense_230_accuracy: 0.7221 - dense_231_accuracy: 0.7585 - dense_232_accuracy: 0.7816 - dense_233_accuracy: 0.7902 - val_loss: 1.8930 - val_dense_224_loss: 0.5401 - val_dense_225_loss: 0.5961 - val_dense_226_loss: 0.6377 - val_dense_227_loss: 0.6394 - val_dense_228_loss: 0.6303 - val_dense_229_loss: 0.5918 - val_dense_230_loss: 0.5174 - val_dense_231_loss: 0.4504 - val_dense_232_loss: 0.3941 - val_dense_233_loss: 0.3782 - val_dense_224_accuracy: 0.7556 - val_dense_225_accuracy: 0.6950 - val_dense_226_accuracy: 0.6639 - val_dense_227_accuracy: 0.6427 - val_dense_228_accuracy: 0.6485 - val_dense_229_accuracy: 0.6806 - val_dense_230_accuracy: 0.7239 - val_dense_231_accuracy: 0.7551 - val_dense_232_accuracy: 0.7835 - val_dense_233_accuracy: 0.7927\n",
      "2020-01-12 15:15:45.248253\t\tValid\tMAP:\t0.5611532695864321\tFPA:\t0.75564788803472\n",
      "2020-01-12 15:15:45.564966\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5611532695864321\n",
      "2020-01-12 15:15:45.565410\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:15:47.147269\t\tAverage best MAP over all folds:\t\t0.543346671035805...\n",
      "=========================================================\n",
      "2020-01-12 15:15:47.198556\t----- FOLD 2 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8861 - dense_224_loss: 0.5428 - dense_225_loss: 0.6042 - dense_226_loss: 0.6271 - dense_227_loss: 0.6375 - dense_228_loss: 0.6340 - dense_229_loss: 0.5867 - dense_230_loss: 0.5042 - dense_231_loss: 0.4387 - dense_232_loss: 0.3931 - dense_233_loss: 0.3652 - dense_224_accuracy: 0.7413 - dense_225_accuracy: 0.6818 - dense_226_accuracy: 0.6510 - dense_227_accuracy: 0.6397 - dense_228_accuracy: 0.6474 - dense_229_accuracy: 0.6806 - dense_230_accuracy: 0.7256 - dense_231_accuracy: 0.7591 - dense_232_accuracy: 0.7821 - dense_233_accuracy: 0.7918 - val_loss: 1.8300 - val_dense_224_loss: 0.5152 - val_dense_225_loss: 0.5903 - val_dense_226_loss: 0.6183 - val_dense_227_loss: 0.6260 - val_dense_228_loss: 0.6250 - val_dense_229_loss: 0.5776 - val_dense_230_loss: 0.4899 - val_dense_231_loss: 0.4275 - val_dense_232_loss: 0.3807 - val_dense_233_loss: 0.3482 - val_dense_224_accuracy: 0.7712 - val_dense_225_accuracy: 0.6973 - val_dense_226_accuracy: 0.6608 - val_dense_227_accuracy: 0.6505 - val_dense_228_accuracy: 0.6569 - val_dense_229_accuracy: 0.6891 - val_dense_230_accuracy: 0.7325 - val_dense_231_accuracy: 0.7662 - val_dense_232_accuracy: 0.7921 - val_dense_233_accuracy: 0.8041\n",
      "2020-01-12 15:15:53.168389\t\tValid\tMAP:\t0.5671974685657785\tFPA:\t0.7712416101737682\n",
      "2020-01-12 15:15:53.557116\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5671974685657785\n",
      "2020-01-12 15:15:53.557341\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8736 - dense_224_loss: 0.5369 - dense_225_loss: 0.6006 - dense_226_loss: 0.6225 - dense_227_loss: 0.6347 - dense_228_loss: 0.6324 - dense_229_loss: 0.5830 - dense_230_loss: 0.4999 - dense_231_loss: 0.4340 - dense_232_loss: 0.3896 - dense_233_loss: 0.3611 - dense_224_accuracy: 0.7444 - dense_225_accuracy: 0.6829 - dense_226_accuracy: 0.6564 - dense_227_accuracy: 0.6431 - dense_228_accuracy: 0.6488 - dense_229_accuracy: 0.6822 - dense_230_accuracy: 0.7286 - dense_231_accuracy: 0.7623 - dense_232_accuracy: 0.7853 - dense_233_accuracy: 0.7958 - val_loss: 1.8344 - val_dense_224_loss: 0.5198 - val_dense_225_loss: 0.5928 - val_dense_226_loss: 0.6179 - val_dense_227_loss: 0.6251 - val_dense_228_loss: 0.6196 - val_dense_229_loss: 0.5731 - val_dense_230_loss: 0.4874 - val_dense_231_loss: 0.4275 - val_dense_232_loss: 0.3778 - val_dense_233_loss: 0.3508 - val_dense_224_accuracy: 0.7679 - val_dense_225_accuracy: 0.6959 - val_dense_226_accuracy: 0.6603 - val_dense_227_accuracy: 0.6487 - val_dense_228_accuracy: 0.6603 - val_dense_229_accuracy: 0.6919 - val_dense_230_accuracy: 0.7342 - val_dense_231_accuracy: 0.7658 - val_dense_232_accuracy: 0.7912 - val_dense_233_accuracy: 0.7978\n",
      "2020-01-12 15:16:00.559154\t\tValid\tMAP:\t0.5654485927476298\tFPA:\t0.7678604471209649\n",
      "2020-01-12 15:16:00.559367\t\tStopping at epoch 1, best epoch was 0 with MAP 0.5671974685657785\n",
      "2020-01-12 15:16:00.559416\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:16:02.158249\t\tAverage best MAP over all folds:\t\t0.5512969368791295...\n",
      "=========================================================\n",
      "=========================================================\n",
      "PARAMETERS CONFIG: {'dropout_1': 0.35, 'dropout_2': 0.35, 'dropout_3': 0.45, 'lr': 0.001, 'amsgrad': True}\n",
      "=========================================================\n",
      "2020-01-12 15:16:03.509837\t----- FOLD 0 -----\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 10s - loss: 2.0900 - dense_237_loss: 0.6278 - dense_238_loss: 0.6580 - dense_239_loss: 0.6683 - dense_240_loss: 0.6794 - dense_241_loss: 0.6719 - dense_242_loss: 0.6324 - dense_243_loss: 0.5695 - dense_244_loss: 0.5087 - dense_245_loss: 0.4793 - dense_246_loss: 0.4541 - dense_237_accuracy: 0.6643 - dense_238_accuracy: 0.6254 - dense_239_accuracy: 0.6089 - dense_240_accuracy: 0.5962 - dense_241_accuracy: 0.6097 - dense_242_accuracy: 0.6493 - dense_243_accuracy: 0.6913 - dense_244_accuracy: 0.7236 - dense_245_accuracy: 0.7409 - dense_246_accuracy: 0.7513 - val_loss: 2.0005 - val_dense_237_loss: 0.5416 - val_dense_238_loss: 0.6133 - val_dense_239_loss: 0.6340 - val_dense_240_loss: 0.6448 - val_dense_241_loss: 0.6503 - val_dense_242_loss: 0.6847 - val_dense_243_loss: 0.6302 - val_dense_244_loss: 0.6473 - val_dense_245_loss: 0.5995 - val_dense_246_loss: 0.6668 - val_dense_237_accuracy: 0.7379 - val_dense_238_accuracy: 0.6721 - val_dense_239_accuracy: 0.6435 - val_dense_240_accuracy: 0.6278 - val_dense_241_accuracy: 0.6241 - val_dense_242_accuracy: 0.6086 - val_dense_243_accuracy: 0.6541 - val_dense_244_accuracy: 0.6701 - val_dense_245_accuracy: 0.7076 - val_dense_246_accuracy: 0.6799\n",
      "2020-01-12 15:16:16.755318\t\tValid\tMAP:\t0.5131664451336838\tFPA:\t0.737854932041448\n",
      "2020-01-12 15:16:17.195388\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5131664451336838\n",
      "2020-01-12 15:16:17.195797\t\tPredicting for test set...\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 4s - loss: 1.9334 - dense_237_loss: 0.5550 - dense_238_loss: 0.6174 - dense_239_loss: 0.6406 - dense_240_loss: 0.6485 - dense_241_loss: 0.6476 - dense_242_loss: 0.6024 - dense_243_loss: 0.5218 - dense_244_loss: 0.4539 - dense_245_loss: 0.4105 - dense_246_loss: 0.3836 - dense_237_accuracy: 0.7316 - dense_238_accuracy: 0.6717 - dense_239_accuracy: 0.6402 - dense_240_accuracy: 0.6299 - dense_241_accuracy: 0.6364 - dense_242_accuracy: 0.6752 - dense_243_accuracy: 0.7181 - dense_244_accuracy: 0.7494 - dense_245_accuracy: 0.7737 - dense_246_accuracy: 0.7826 - val_loss: 1.8757 - val_dense_237_loss: 0.5148 - val_dense_238_loss: 0.5935 - val_dense_239_loss: 0.6226 - val_dense_240_loss: 0.6341 - val_dense_241_loss: 0.6370 - val_dense_242_loss: 0.6013 - val_dense_243_loss: 0.5431 - val_dense_244_loss: 0.4813 - val_dense_245_loss: 0.4538 - val_dense_246_loss: 0.4637 - val_dense_237_accuracy: 0.7599 - val_dense_238_accuracy: 0.6939 - val_dense_239_accuracy: 0.6582 - val_dense_240_accuracy: 0.6416 - val_dense_241_accuracy: 0.6496 - val_dense_242_accuracy: 0.6753 - val_dense_243_accuracy: 0.7057 - val_dense_244_accuracy: 0.7149 - val_dense_245_accuracy: 0.7446 - val_dense_246_accuracy: 0.7486\n",
      "2020-01-12 15:16:24.269522\t\tValid\tMAP:\t0.5454044199978388\tFPA:\t0.7598909971740008\n",
      "2020-01-12 15:16:24.729154\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5454044199978388\n",
      "2020-01-12 15:16:24.729298\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:16:26.330696\t\tAverage best MAP over all folds:\t\t0.5292854325657613...\n",
      "=========================================================\n",
      "2020-01-12 15:16:26.341818\t----- FOLD 1 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.9145 - dense_237_loss: 0.5497 - dense_238_loss: 0.6127 - dense_239_loss: 0.6367 - dense_240_loss: 0.6465 - dense_241_loss: 0.6422 - dense_242_loss: 0.5987 - dense_243_loss: 0.5159 - dense_244_loss: 0.4443 - dense_245_loss: 0.3990 - dense_246_loss: 0.3728 - dense_237_accuracy: 0.7369 - dense_238_accuracy: 0.6769 - dense_239_accuracy: 0.6453 - dense_240_accuracy: 0.6325 - dense_241_accuracy: 0.6432 - dense_242_accuracy: 0.6762 - dense_243_accuracy: 0.7214 - dense_244_accuracy: 0.7585 - dense_245_accuracy: 0.7814 - dense_246_accuracy: 0.7897 - val_loss: 1.8808 - val_dense_237_loss: 0.5269 - val_dense_238_loss: 0.5947 - val_dense_239_loss: 0.6205 - val_dense_240_loss: 0.6396 - val_dense_241_loss: 0.6359 - val_dense_242_loss: 0.5960 - val_dense_243_loss: 0.5131 - val_dense_244_loss: 0.4537 - val_dense_245_loss: 0.4323 - val_dense_246_loss: 0.4346 - val_dense_237_accuracy: 0.7557 - val_dense_238_accuracy: 0.6914 - val_dense_239_accuracy: 0.6617 - val_dense_240_accuracy: 0.6374 - val_dense_241_accuracy: 0.6473 - val_dense_242_accuracy: 0.6797 - val_dense_243_accuracy: 0.7218 - val_dense_244_accuracy: 0.7448 - val_dense_245_accuracy: 0.7638 - val_dense_246_accuracy: 0.7748\n",
      "2020-01-12 15:16:32.266544\t\tValid\tMAP:\t0.5548869539003964\tFPA:\t0.7556983531549111\n",
      "2020-01-12 15:16:32.658692\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5548869539003964\n",
      "2020-01-12 15:16:32.660173\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8966 - dense_237_loss: 0.5403 - dense_238_loss: 0.6073 - dense_239_loss: 0.6309 - dense_240_loss: 0.6414 - dense_241_loss: 0.6402 - dense_242_loss: 0.5919 - dense_243_loss: 0.5103 - dense_244_loss: 0.4432 - dense_245_loss: 0.3987 - dense_246_loss: 0.3692 - dense_237_accuracy: 0.7412 - dense_238_accuracy: 0.6794 - dense_239_accuracy: 0.6464 - dense_240_accuracy: 0.6339 - dense_241_accuracy: 0.6426 - dense_242_accuracy: 0.6798 - dense_243_accuracy: 0.7242 - dense_244_accuracy: 0.7583 - dense_245_accuracy: 0.7803 - dense_246_accuracy: 0.7912 - val_loss: 1.8537 - val_dense_237_loss: 0.5092 - val_dense_238_loss: 0.5946 - val_dense_239_loss: 0.6260 - val_dense_240_loss: 0.6486 - val_dense_241_loss: 0.6425 - val_dense_242_loss: 0.5818 - val_dense_243_loss: 0.5004 - val_dense_244_loss: 0.4324 - val_dense_245_loss: 0.3837 - val_dense_246_loss: 0.3624 - val_dense_237_accuracy: 0.7640 - val_dense_238_accuracy: 0.6891 - val_dense_239_accuracy: 0.6646 - val_dense_240_accuracy: 0.6423 - val_dense_241_accuracy: 0.6337 - val_dense_242_accuracy: 0.6855 - val_dense_243_accuracy: 0.7280 - val_dense_244_accuracy: 0.7627 - val_dense_245_accuracy: 0.7896 - val_dense_246_accuracy: 0.8029\n",
      "2020-01-12 15:16:39.701405\t\tValid\tMAP:\t0.5612727286967856\tFPA:\t0.7639746328662506\n",
      "2020-01-12 15:16:40.089511\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5612727286967856\n",
      "2020-01-12 15:16:40.089671\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:16:41.636805\t\tAverage best MAP over all folds:\t\t0.5436826369321761...\n",
      "=========================================================\n",
      "2020-01-12 15:16:41.685691\t----- FOLD 2 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8873 - dense_237_loss: 0.5397 - dense_238_loss: 0.6050 - dense_239_loss: 0.6262 - dense_240_loss: 0.6397 - dense_241_loss: 0.6372 - dense_242_loss: 0.5870 - dense_243_loss: 0.5038 - dense_244_loss: 0.4386 - dense_245_loss: 0.3940 - dense_246_loss: 0.3664 - dense_237_accuracy: 0.7420 - dense_238_accuracy: 0.6816 - dense_239_accuracy: 0.6541 - dense_240_accuracy: 0.6373 - dense_241_accuracy: 0.6445 - dense_242_accuracy: 0.6801 - dense_243_accuracy: 0.7258 - dense_244_accuracy: 0.7598 - dense_245_accuracy: 0.7843 - dense_246_accuracy: 0.7938 - val_loss: 1.8496 - val_dense_237_loss: 0.5176 - val_dense_238_loss: 0.5951 - val_dense_239_loss: 0.6298 - val_dense_240_loss: 0.6367 - val_dense_241_loss: 0.6366 - val_dense_242_loss: 0.5836 - val_dense_243_loss: 0.4957 - val_dense_244_loss: 0.4303 - val_dense_245_loss: 0.3821 - val_dense_246_loss: 0.3623 - val_dense_237_accuracy: 0.7600 - val_dense_238_accuracy: 0.6918 - val_dense_239_accuracy: 0.6608 - val_dense_240_accuracy: 0.6329 - val_dense_241_accuracy: 0.6540 - val_dense_242_accuracy: 0.6895 - val_dense_243_accuracy: 0.7300 - val_dense_244_accuracy: 0.7639 - val_dense_245_accuracy: 0.7888 - val_dense_246_accuracy: 0.7947\n",
      "2020-01-12 15:16:47.603890\t\tValid\tMAP:\t0.5607768162237171\tFPA:\t0.7599710666644238\n",
      "2020-01-12 15:16:48.004894\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5607768162237171\n",
      "2020-01-12 15:16:48.005090\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8753 - dense_237_loss: 0.5348 - dense_238_loss: 0.6018 - dense_239_loss: 0.6266 - dense_240_loss: 0.6375 - dense_241_loss: 0.6352 - dense_242_loss: 0.5852 - dense_243_loss: 0.5021 - dense_244_loss: 0.4375 - dense_245_loss: 0.3898 - dense_246_loss: 0.3629 - dense_237_accuracy: 0.7438 - dense_238_accuracy: 0.6815 - dense_239_accuracy: 0.6527 - dense_240_accuracy: 0.6408 - dense_241_accuracy: 0.6482 - dense_242_accuracy: 0.6817 - dense_243_accuracy: 0.7292 - dense_244_accuracy: 0.7620 - dense_245_accuracy: 0.7851 - dense_246_accuracy: 0.7942 - val_loss: 1.8736 - val_dense_237_loss: 0.5457 - val_dense_238_loss: 0.5998 - val_dense_239_loss: 0.6166 - val_dense_240_loss: 0.6381 - val_dense_241_loss: 0.6313 - val_dense_242_loss: 0.5837 - val_dense_243_loss: 0.4903 - val_dense_244_loss: 0.4287 - val_dense_245_loss: 0.3803 - val_dense_246_loss: 0.3486 - val_dense_237_accuracy: 0.7470 - val_dense_238_accuracy: 0.6930 - val_dense_239_accuracy: 0.6640 - val_dense_240_accuracy: 0.6369 - val_dense_241_accuracy: 0.6521 - val_dense_242_accuracy: 0.6867 - val_dense_243_accuracy: 0.7331 - val_dense_244_accuracy: 0.7645 - val_dense_245_accuracy: 0.7910 - val_dense_246_accuracy: 0.8043\n",
      "2020-01-12 15:16:55.158148\t\tValid\tMAP:\t0.5560297440806589\tFPA:\t0.746984709068582\n",
      "2020-01-12 15:16:55.159322\t\tStopping at epoch 1, best epoch was 0 with MAP 0.5607768162237171\n",
      "2020-01-12 15:16:55.159459\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:16:56.796299\t\tAverage best MAP over all folds:\t\t0.5493806966960232...\n",
      "=========================================================\n",
      "=========================================================\n",
      "PARAMETERS CONFIG: {'dropout_1': 0.35, 'dropout_2': 0.35, 'dropout_3': 0.25, 'lr': 0.1, 'amsgrad': True}\n",
      "=========================================================\n",
      "2020-01-12 15:16:58.172134\t----- FOLD 0 -----\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 10s - loss: 12.1134 - dense_250_loss: 4.0996 - dense_251_loss: 3.6656 - dense_252_loss: 2.6222 - dense_253_loss: 3.4575 - dense_254_loss: 3.9590 - dense_255_loss: 3.6958 - dense_256_loss: 3.4853 - dense_257_loss: 3.2985 - dense_258_loss: 3.7410 - dense_259_loss: 3.3765 - dense_250_accuracy: 0.5099 - dense_251_accuracy: 0.5132 - dense_252_accuracy: 0.5106 - dense_253_accuracy: 0.5085 - dense_254_accuracy: 0.5158 - dense_255_accuracy: 0.5368 - dense_256_accuracy: 0.5764 - dense_257_accuracy: 0.6171 - dense_258_accuracy: 0.6189 - dense_259_accuracy: 0.6370 - val_loss: 2.7939 - val_dense_250_loss: 0.7948 - val_dense_251_loss: 0.7483 - val_dense_252_loss: 0.7857 - val_dense_253_loss: 0.9730 - val_dense_254_loss: 0.6854 - val_dense_255_loss: 0.6500 - val_dense_256_loss: 1.8164 - val_dense_257_loss: 0.8129 - val_dense_258_loss: 1.3880 - val_dense_259_loss: 0.6479 - val_dense_250_accuracy: 0.5439 - val_dense_251_accuracy: 0.4718 - val_dense_252_accuracy: 0.5362 - val_dense_253_accuracy: 0.5216 - val_dense_254_accuracy: 0.5619 - val_dense_255_accuracy: 0.5838 - val_dense_256_accuracy: 0.4478 - val_dense_257_accuracy: 0.6616 - val_dense_258_accuracy: 0.4521 - val_dense_259_accuracy: 0.7254\n",
      "2020-01-12 15:17:10.872784\t\tValid\tMAP:\t0.3634321991085778\tFPA:\t0.5439207374512178\n",
      "2020-01-12 15:17:11.239355\t\tStopping at epoch 0, best epoch was 0 with MAP 0.3634321991085778\n",
      "2020-01-12 15:17:11.239999\t\tPredicting for test set...\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 4s - loss: 2.3829 - dense_250_loss: 0.7440 - dense_251_loss: 0.7409 - dense_252_loss: 0.7865 - dense_253_loss: 0.7129 - dense_254_loss: 0.7405 - dense_255_loss: 0.7244 - dense_256_loss: 0.6295 - dense_257_loss: 0.5844 - dense_258_loss: 0.5118 - dense_259_loss: 0.4829 - dense_250_accuracy: 0.5040 - dense_251_accuracy: 0.5197 - dense_252_accuracy: 0.5072 - dense_253_accuracy: 0.5377 - dense_254_accuracy: 0.5238 - dense_255_accuracy: 0.5874 - dense_256_accuracy: 0.6548 - dense_257_accuracy: 0.6718 - dense_258_accuracy: 0.6931 - dense_259_accuracy: 0.7065 - val_loss: 2.6926 - val_dense_250_loss: 0.8338 - val_dense_251_loss: 0.6889 - val_dense_252_loss: 0.9956 - val_dense_253_loss: 0.6992 - val_dense_254_loss: 0.6977 - val_dense_255_loss: 1.4303 - val_dense_256_loss: 0.6526 - val_dense_257_loss: 0.6305 - val_dense_258_loss: 0.5369 - val_dense_259_loss: 0.7922 - val_dense_250_accuracy: 0.5463 - val_dense_251_accuracy: 0.5439 - val_dense_252_accuracy: 0.5362 - val_dense_253_accuracy: 0.5277 - val_dense_254_accuracy: 0.5435 - val_dense_255_accuracy: 0.4409 - val_dense_256_accuracy: 0.6070 - val_dense_257_accuracy: 0.5793 - val_dense_258_accuracy: 0.6314 - val_dense_259_accuracy: 0.5090\n",
      "2020-01-12 15:17:18.204817\t\tValid\tMAP:\t0.4117379195333443\tFPA:\t0.5463430224734221\n",
      "2020-01-12 15:17:18.610288\t\tStopping at epoch 1, best epoch was 1 with MAP 0.4117379195333443\n",
      "2020-01-12 15:17:18.610702\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:17:20.217981\t\tAverage best MAP over all folds:\t\t0.3875850593209611...\n",
      "=========================================================\n",
      "2020-01-12 15:17:20.229651\t----- FOLD 1 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 2.5901 - dense_250_loss: 0.8277 - dense_251_loss: 0.8580 - dense_252_loss: 0.8425 - dense_253_loss: 0.8302 - dense_254_loss: 0.7566 - dense_255_loss: 0.7542 - dense_256_loss: 0.5850 - dense_257_loss: 0.4968 - dense_258_loss: 0.4489 - dense_259_loss: 0.4594 - dense_250_accuracy: 0.5040 - dense_251_accuracy: 0.5038 - dense_252_accuracy: 0.5067 - dense_253_accuracy: 0.5070 - dense_254_accuracy: 0.5240 - dense_255_accuracy: 0.5751 - dense_256_accuracy: 0.6458 - dense_257_accuracy: 0.6804 - dense_258_accuracy: 0.7036 - dense_259_accuracy: 0.7124 - val_loss: 2.6817 - val_dense_250_loss: 1.1358 - val_dense_251_loss: 0.6891 - val_dense_252_loss: 0.6896 - val_dense_253_loss: 0.7755 - val_dense_254_loss: 0.7127 - val_dense_255_loss: 0.7064 - val_dense_256_loss: 0.5836 - val_dense_257_loss: 0.4803 - val_dense_258_loss: 0.4479 - val_dense_259_loss: 0.4681 - val_dense_250_accuracy: 0.5457 - val_dense_251_accuracy: 0.5454 - val_dense_252_accuracy: 0.5499 - val_dense_253_accuracy: 0.4747 - val_dense_254_accuracy: 0.5029 - val_dense_255_accuracy: 0.5559 - val_dense_256_accuracy: 0.6664 - val_dense_257_accuracy: 0.7063 - val_dense_258_accuracy: 0.6914 - val_dense_259_accuracy: 0.7209\n",
      "2020-01-12 15:17:26.137370\t\tValid\tMAP:\t0.36658431628002225\tFPA:\t0.5456793446263057\n",
      "2020-01-12 15:17:26.628789\t\tStopping at epoch 0, best epoch was 0 with MAP 0.36658431628002225\n",
      "2020-01-12 15:17:26.629399\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 2.9611 - dense_250_loss: 0.9960 - dense_251_loss: 1.1030 - dense_252_loss: 0.8681 - dense_253_loss: 0.8696 - dense_254_loss: 0.8675 - dense_255_loss: 0.7438 - dense_256_loss: 0.6038 - dense_257_loss: 0.5276 - dense_258_loss: 0.4583 - dense_259_loss: 0.4557 - dense_250_accuracy: 0.5033 - dense_251_accuracy: 0.5150 - dense_252_accuracy: 0.5041 - dense_253_accuracy: 0.5013 - dense_254_accuracy: 0.5213 - dense_255_accuracy: 0.5846 - dense_256_accuracy: 0.6389 - dense_257_accuracy: 0.6809 - dense_258_accuracy: 0.7023 - dense_259_accuracy: 0.7125 - val_loss: 2.7369 - val_dense_250_loss: 0.6876 - val_dense_251_loss: 1.0072 - val_dense_252_loss: 0.8822 - val_dense_253_loss: 0.9164 - val_dense_254_loss: 1.2131 - val_dense_255_loss: 0.7411 - val_dense_256_loss: 0.5965 - val_dense_257_loss: 0.6579 - val_dense_258_loss: 0.4905 - val_dense_259_loss: 0.5206 - val_dense_250_accuracy: 0.5456 - val_dense_251_accuracy: 0.5454 - val_dense_252_accuracy: 0.4568 - val_dense_253_accuracy: 0.4747 - val_dense_254_accuracy: 0.4971 - val_dense_255_accuracy: 0.5559 - val_dense_256_accuracy: 0.6621 - val_dense_257_accuracy: 0.6532 - val_dense_258_accuracy: 0.6914 - val_dense_259_accuracy: 0.7209\n",
      "2020-01-12 15:17:33.541861\t\tValid\tMAP:\t0.3403231566442056\tFPA:\t0.5456457012128451\n",
      "2020-01-12 15:17:33.542525\t\tStopping at epoch 1, best epoch was 0 with MAP 0.36658431628002225\n",
      "2020-01-12 15:17:33.542809\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:17:35.128903\t\tAverage best MAP over all folds:\t\t0.37708468780049165...\n",
      "=========================================================\n",
      "2020-01-12 15:17:35.176278\t----- FOLD 2 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 8.3853 - dense_250_loss: 3.1820 - dense_251_loss: 4.2582 - dense_252_loss: 1.7296 - dense_253_loss: 0.8798 - dense_254_loss: 1.9790 - dense_255_loss: 1.5196 - dense_256_loss: 1.2204 - dense_257_loss: 1.1897 - dense_258_loss: 1.8246 - dense_259_loss: 3.0401 - dense_250_accuracy: 0.5025 - dense_251_accuracy: 0.5049 - dense_252_accuracy: 0.5052 - dense_253_accuracy: 0.5004 - dense_254_accuracy: 0.5039 - dense_255_accuracy: 0.5252 - dense_256_accuracy: 0.5514 - dense_257_accuracy: 0.5754 - dense_258_accuracy: 0.5926 - dense_259_accuracy: 0.6180 - val_loss: 4.7999 - val_dense_250_loss: 1.5076 - val_dense_251_loss: 1.2176 - val_dense_252_loss: 2.0999 - val_dense_253_loss: 0.7292 - val_dense_254_loss: 1.6920 - val_dense_255_loss: 0.7392 - val_dense_256_loss: 0.9353 - val_dense_257_loss: 1.7208 - val_dense_258_loss: 0.6448 - val_dense_259_loss: 6.5943 - val_dense_250_accuracy: 0.4569 - val_dense_251_accuracy: 0.5423 - val_dense_252_accuracy: 0.4622 - val_dense_253_accuracy: 0.5273 - val_dense_254_accuracy: 0.5012 - val_dense_255_accuracy: 0.4487 - val_dense_256_accuracy: 0.3890 - val_dense_257_accuracy: 0.3432 - val_dense_258_accuracy: 0.6949 - val_dense_259_accuracy: 0.7233\n",
      "2020-01-12 15:17:41.203807\t\tValid\tMAP:\t0.3192060915776177\tFPA:\t0.45687755479670966\n",
      "2020-01-12 15:17:41.596654\t\tStopping at epoch 0, best epoch was 0 with MAP 0.3192060915776177\n",
      "2020-01-12 15:17:41.597184\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 8.3116 - dense_250_loss: 4.1699 - dense_251_loss: 1.9972 - dense_252_loss: 3.2658 - dense_253_loss: 0.9002 - dense_254_loss: 1.7315 - dense_255_loss: 0.8525 - dense_256_loss: 1.1949 - dense_257_loss: 1.1680 - dense_258_loss: 1.1725 - dense_259_loss: 1.4153 - dense_250_accuracy: 0.5023 - dense_251_accuracy: 0.5037 - dense_252_accuracy: 0.5049 - dense_253_accuracy: 0.5002 - dense_254_accuracy: 0.5004 - dense_255_accuracy: 0.4982 - dense_256_accuracy: 0.5399 - dense_257_accuracy: 0.5497 - dense_258_accuracy: 0.5793 - dense_259_accuracy: 0.6848 - val_loss: 6.0001 - val_dense_250_loss: 3.3541 - val_dense_251_loss: 0.7992 - val_dense_252_loss: 1.0333 - val_dense_253_loss: 1.5545 - val_dense_254_loss: 2.2754 - val_dense_255_loss: 0.8457 - val_dense_256_loss: 1.1373 - val_dense_257_loss: 1.1158 - val_dense_258_loss: 0.7010 - val_dense_259_loss: 0.5931 - val_dense_250_accuracy: 0.5431 - val_dense_251_accuracy: 0.4577 - val_dense_252_accuracy: 0.4622 - val_dense_253_accuracy: 0.5273 - val_dense_254_accuracy: 0.5012 - val_dense_255_accuracy: 0.4487 - val_dense_256_accuracy: 0.3890 - val_dense_257_accuracy: 0.6568 - val_dense_258_accuracy: 0.6949 - val_dense_259_accuracy: 0.7233\n",
      "2020-01-12 15:17:48.498484\t\tValid\tMAP:\t0.3223532217455016\tFPA:\t0.5431224452032903\n",
      "2020-01-12 15:17:48.913709\t\tStopping at epoch 1, best epoch was 1 with MAP 0.3223532217455016\n",
      "2020-01-12 15:17:48.913889\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:17:50.493531\t\tAverage best MAP over all folds:\t\t0.35831634408751434...\n",
      "=========================================================\n",
      "=========================================================\n",
      "PARAMETERS CONFIG: {'dropout_1': 0.35, 'dropout_2': 0.35, 'dropout_3': 0.25, 'lr': 0.01, 'amsgrad': True}\n",
      "=========================================================\n",
      "2020-01-12 15:17:51.868893\t----- FOLD 0 -----\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 11s - loss: 2.7266 - dense_263_loss: 0.9618 - dense_264_loss: 0.8601 - dense_265_loss: 0.8183 - dense_266_loss: 0.7685 - dense_267_loss: 0.7907 - dense_268_loss: 0.7605 - dense_269_loss: 0.6125 - dense_270_loss: 0.5483 - dense_271_loss: 0.4955 - dense_272_loss: 0.4749 - dense_263_accuracy: 0.5329 - dense_264_accuracy: 0.5293 - dense_265_accuracy: 0.5340 - dense_266_accuracy: 0.5371 - dense_267_accuracy: 0.5557 - dense_268_accuracy: 0.5966 - dense_269_accuracy: 0.6566 - dense_270_accuracy: 0.6961 - dense_271_accuracy: 0.7193 - dense_272_accuracy: 0.7295 - val_loss: 7.7045 - val_dense_263_loss: 2.8409 - val_dense_264_loss: 3.1617 - val_dense_265_loss: 0.7060 - val_dense_266_loss: 2.4321 - val_dense_267_loss: 1.9508 - val_dense_268_loss: 1.6392 - val_dense_269_loss: 1.3827 - val_dense_270_loss: 2.2673 - val_dense_271_loss: 4.0368 - val_dense_272_loss: 2.2561 - val_dense_263_accuracy: 0.4536 - val_dense_264_accuracy: 0.5431 - val_dense_265_accuracy: 0.5079 - val_dense_266_accuracy: 0.5232 - val_dense_267_accuracy: 0.5063 - val_dense_268_accuracy: 0.5590 - val_dense_269_accuracy: 0.6144 - val_dense_270_accuracy: 0.6616 - val_dense_271_accuracy: 0.6970 - val_dense_272_accuracy: 0.7254\n",
      "2020-01-12 15:18:06.307370\t\tValid\tMAP:\t0.33602371280856674\tFPA:\t0.45364015610281255\n",
      "2020-01-12 15:18:06.756818\t\tStopping at epoch 0, best epoch was 0 with MAP 0.33602371280856674\n",
      "2020-01-12 15:18:06.757004\t\tPredicting for test set...\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 4s - loss: 2.1633 - dense_263_loss: 0.6885 - dense_264_loss: 0.6891 - dense_265_loss: 0.6885 - dense_266_loss: 0.6853 - dense_267_loss: 0.6760 - dense_268_loss: 0.6254 - dense_269_loss: 0.5421 - dense_270_loss: 0.4734 - dense_271_loss: 0.4224 - dense_272_loss: 0.3986 - dense_263_accuracy: 0.5461 - dense_264_accuracy: 0.5460 - dense_265_accuracy: 0.5519 - dense_266_accuracy: 0.5568 - dense_267_accuracy: 0.5789 - dense_268_accuracy: 0.6235 - dense_269_accuracy: 0.6729 - dense_270_accuracy: 0.7123 - dense_271_accuracy: 0.7338 - dense_272_accuracy: 0.7411 - val_loss: 3.9864 - val_dense_263_loss: 1.3019 - val_dense_264_loss: 1.4915 - val_dense_265_loss: 0.7478 - val_dense_266_loss: 1.3294 - val_dense_267_loss: 1.7130 - val_dense_268_loss: 0.7022 - val_dense_269_loss: 0.6998 - val_dense_270_loss: 1.1039 - val_dense_271_loss: 1.5076 - val_dense_272_loss: 0.4065 - val_dense_263_accuracy: 0.4537 - val_dense_264_accuracy: 0.5432 - val_dense_265_accuracy: 0.5405 - val_dense_266_accuracy: 0.5250 - val_dense_267_accuracy: 0.5063 - val_dense_268_accuracy: 0.5648 - val_dense_269_accuracy: 0.6153 - val_dense_270_accuracy: 0.6616 - val_dense_271_accuracy: 0.6970 - val_dense_272_accuracy: 0.7302\n",
      "2020-01-12 15:18:13.784323\t\tValid\tMAP:\t0.3397572713449219\tFPA:\t0.45369062037410846\n",
      "2020-01-12 15:18:14.127040\t\tStopping at epoch 1, best epoch was 1 with MAP 0.3397572713449219\n",
      "2020-01-12 15:18:14.127678\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:18:15.717020\t\tAverage best MAP over all folds:\t\t0.3378904920767443...\n",
      "=========================================================\n",
      "2020-01-12 15:18:15.727983\t----- FOLD 1 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 2.1588 - dense_263_loss: 0.6879 - dense_264_loss: 0.6876 - dense_265_loss: 0.6872 - dense_266_loss: 0.6862 - dense_267_loss: 0.6763 - dense_268_loss: 0.6203 - dense_269_loss: 0.5374 - dense_270_loss: 0.4660 - dense_271_loss: 0.4156 - dense_272_loss: 0.3878 - dense_263_accuracy: 0.5461 - dense_264_accuracy: 0.5483 - dense_265_accuracy: 0.5493 - dense_266_accuracy: 0.5569 - dense_267_accuracy: 0.5788 - dense_268_accuracy: 0.6231 - dense_269_accuracy: 0.6725 - dense_270_accuracy: 0.7119 - dense_271_accuracy: 0.7353 - dense_272_accuracy: 0.7404 - val_loss: 2.6127 - val_dense_263_loss: 0.8669 - val_dense_264_loss: 0.8291 - val_dense_265_loss: 0.7080 - val_dense_266_loss: 0.8666 - val_dense_267_loss: 0.9364 - val_dense_268_loss: 0.6357 - val_dense_269_loss: 0.6023 - val_dense_270_loss: 0.5753 - val_dense_271_loss: 0.5135 - val_dense_272_loss: 0.5753 - val_dense_263_accuracy: 0.4545 - val_dense_264_accuracy: 0.5462 - val_dense_265_accuracy: 0.5508 - val_dense_266_accuracy: 0.5267 - val_dense_267_accuracy: 0.5023 - val_dense_268_accuracy: 0.6242 - val_dense_269_accuracy: 0.6342 - val_dense_270_accuracy: 0.6539 - val_dense_271_accuracy: 0.6916 - val_dense_272_accuracy: 0.7515\n",
      "2020-01-12 15:18:21.634123\t\tValid\tMAP:\t0.34403615247604297\tFPA:\t0.4544720507342675\n",
      "2020-01-12 15:18:22.053887\t\tStopping at epoch 0, best epoch was 0 with MAP 0.34403615247604297\n",
      "2020-01-12 15:18:22.054318\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 2.1519 - dense_263_loss: 0.6869 - dense_264_loss: 0.6858 - dense_265_loss: 0.6857 - dense_266_loss: 0.6843 - dense_267_loss: 0.6731 - dense_268_loss: 0.6158 - dense_269_loss: 0.5331 - dense_270_loss: 0.4645 - dense_271_loss: 0.4115 - dense_272_loss: 0.3873 - dense_263_accuracy: 0.5495 - dense_264_accuracy: 0.5496 - dense_265_accuracy: 0.5533 - dense_266_accuracy: 0.5586 - dense_267_accuracy: 0.5809 - dense_268_accuracy: 0.6251 - dense_269_accuracy: 0.6755 - dense_270_accuracy: 0.7133 - dense_271_accuracy: 0.7409 - dense_272_accuracy: 0.7416 - val_loss: 2.3125 - val_dense_263_loss: 0.7391 - val_dense_264_loss: 0.7649 - val_dense_265_loss: 0.6943 - val_dense_266_loss: 0.6853 - val_dense_267_loss: 0.7312 - val_dense_268_loss: 0.6329 - val_dense_269_loss: 0.5966 - val_dense_270_loss: 0.5253 - val_dense_271_loss: 0.4374 - val_dense_272_loss: 0.6529 - val_dense_263_accuracy: 0.4712 - val_dense_264_accuracy: 0.5523 - val_dense_265_accuracy: 0.5178 - val_dense_266_accuracy: 0.5490 - val_dense_267_accuracy: 0.5273 - val_dense_268_accuracy: 0.6301 - val_dense_269_accuracy: 0.6398 - val_dense_270_accuracy: 0.6752 - val_dense_271_accuracy: 0.7083 - val_dense_272_accuracy: 0.7504\n",
      "2020-01-12 15:18:29.083176\t\tValid\tMAP:\t0.3621134543920349\tFPA:\t0.4711760055175198\n",
      "2020-01-12 15:18:29.502971\t\tStopping at epoch 1, best epoch was 1 with MAP 0.3621134543920349\n",
      "2020-01-12 15:18:29.503458\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:18:31.085147\t\tAverage best MAP over all folds:\t\t0.3454826477553916...\n",
      "=========================================================\n",
      "2020-01-12 15:18:31.136240\t----- FOLD 2 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 2.1516 - dense_263_loss: 0.6860 - dense_264_loss: 0.6864 - dense_265_loss: 0.6853 - dense_266_loss: 0.6873 - dense_267_loss: 0.6734 - dense_268_loss: 0.6148 - dense_269_loss: 0.5320 - dense_270_loss: 0.4640 - dense_271_loss: 0.4114 - dense_272_loss: 0.3813 - dense_263_accuracy: 0.5504 - dense_264_accuracy: 0.5527 - dense_265_accuracy: 0.5514 - dense_266_accuracy: 0.5533 - dense_267_accuracy: 0.5815 - dense_268_accuracy: 0.6259 - dense_269_accuracy: 0.6767 - dense_270_accuracy: 0.7119 - dense_271_accuracy: 0.7365 - dense_272_accuracy: 0.7443 - val_loss: 2.2516 - val_dense_263_loss: 0.6962 - val_dense_264_loss: 0.7832 - val_dense_265_loss: 0.6955 - val_dense_266_loss: 0.6950 - val_dense_267_loss: 0.6850 - val_dense_268_loss: 0.6251 - val_dense_269_loss: 0.5834 - val_dense_270_loss: 0.4732 - val_dense_271_loss: 0.4128 - val_dense_272_loss: 0.5443 - val_dense_263_accuracy: 0.5363 - val_dense_264_accuracy: 0.5461 - val_dense_265_accuracy: 0.5543 - val_dense_266_accuracy: 0.5629 - val_dense_267_accuracy: 0.5704 - val_dense_268_accuracy: 0.6132 - val_dense_269_accuracy: 0.6352 - val_dense_270_accuracy: 0.7211 - val_dense_271_accuracy: 0.7480 - val_dense_272_accuracy: 0.7510\n",
      "2020-01-12 15:18:37.073560\t\tValid\tMAP:\t0.40345882915408704\tFPA:\t0.5363264756842229\n",
      "2020-01-12 15:18:37.416761\t\tStopping at epoch 0, best epoch was 0 with MAP 0.40345882915408704\n",
      "2020-01-12 15:18:37.417389\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 2.1465 - dense_263_loss: 0.6858 - dense_264_loss: 0.6856 - dense_265_loss: 0.6849 - dense_266_loss: 0.6834 - dense_267_loss: 0.6712 - dense_268_loss: 0.6113 - dense_269_loss: 0.5276 - dense_270_loss: 0.4593 - dense_271_loss: 0.4089 - dense_272_loss: 0.3765 - dense_263_accuracy: 0.5537 - dense_264_accuracy: 0.5514 - dense_265_accuracy: 0.5533 - dense_266_accuracy: 0.5621 - dense_267_accuracy: 0.5842 - dense_268_accuracy: 0.6276 - dense_269_accuracy: 0.6789 - dense_270_accuracy: 0.7175 - dense_271_accuracy: 0.7418 - dense_272_accuracy: 0.7471 - val_loss: 2.1681 - val_dense_263_loss: 0.6918 - val_dense_264_loss: 0.6913 - val_dense_265_loss: 0.6871 - val_dense_266_loss: 0.6838 - val_dense_267_loss: 0.6918 - val_dense_268_loss: 0.6196 - val_dense_269_loss: 0.5311 - val_dense_270_loss: 0.4593 - val_dense_271_loss: 0.4063 - val_dense_272_loss: 0.4306 - val_dense_263_accuracy: 0.5410 - val_dense_264_accuracy: 0.5493 - val_dense_265_accuracy: 0.5568 - val_dense_266_accuracy: 0.5640 - val_dense_267_accuracy: 0.5796 - val_dense_268_accuracy: 0.6317 - val_dense_269_accuracy: 0.6802 - val_dense_270_accuracy: 0.7206 - val_dense_271_accuracy: 0.7384 - val_dense_272_accuracy: 0.7502\n",
      "2020-01-12 15:18:44.436960\t\tValid\tMAP:\t0.4237500264362166\tFPA:\t0.5410029101552644\n",
      "2020-01-12 15:18:44.727824\t\tStopping at epoch 1, best epoch was 1 with MAP 0.4237500264362166\n",
      "2020-01-12 15:18:44.728013\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:18:46.309364\t\tAverage best MAP over all folds:\t\t0.368189907768645...\n",
      "=========================================================\n",
      "=========================================================\n",
      "PARAMETERS CONFIG: {'dropout_1': 0.35, 'dropout_2': 0.35, 'dropout_3': 0.25, 'lr': 0.001, 'amsgrad': True}\n",
      "=========================================================\n",
      "2020-01-12 15:18:47.724164\t----- FOLD 0 -----\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 10s - loss: 2.1052 - dense_276_loss: 0.6271 - dense_277_loss: 0.6615 - dense_278_loss: 0.6859 - dense_279_loss: 0.6851 - dense_280_loss: 0.6743 - dense_281_loss: 0.6383 - dense_282_loss: 0.5697 - dense_283_loss: 0.5076 - dense_284_loss: 0.4707 - dense_285_loss: 0.4722 - dense_276_accuracy: 0.6684 - dense_277_accuracy: 0.6328 - dense_278_accuracy: 0.6082 - dense_279_accuracy: 0.5974 - dense_280_accuracy: 0.6082 - dense_281_accuracy: 0.6460 - dense_282_accuracy: 0.6902 - dense_283_accuracy: 0.7230 - dense_284_accuracy: 0.7439 - dense_285_accuracy: 0.7422 - val_loss: 2.0095 - val_dense_276_loss: 0.5483 - val_dense_277_loss: 0.6100 - val_dense_278_loss: 0.6326 - val_dense_279_loss: 0.6480 - val_dense_280_loss: 0.6631 - val_dense_281_loss: 0.6941 - val_dense_282_loss: 0.6566 - val_dense_283_loss: 0.6620 - val_dense_284_loss: 0.5858 - val_dense_285_loss: 0.5490 - val_dense_276_accuracy: 0.7403 - val_dense_277_accuracy: 0.6798 - val_dense_278_accuracy: 0.6514 - val_dense_279_accuracy: 0.6278 - val_dense_280_accuracy: 0.6092 - val_dense_281_accuracy: 0.5840 - val_dense_282_accuracy: 0.6357 - val_dense_283_accuracy: 0.6697 - val_dense_284_accuracy: 0.7105 - val_dense_285_accuracy: 0.7256\n",
      "2020-01-12 15:19:00.280722\t\tValid\tMAP:\t0.5098832757023363\tFPA:\t0.7403108599111828\n",
      "2020-01-12 15:19:00.648091\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5098832757023363\n",
      "2020-01-12 15:19:00.648298\t\tPredicting for test set...\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 4s - loss: 1.9292 - dense_276_loss: 0.5564 - dense_277_loss: 0.6150 - dense_278_loss: 0.6382 - dense_279_loss: 0.6493 - dense_280_loss: 0.6476 - dense_281_loss: 0.6019 - dense_282_loss: 0.5235 - dense_283_loss: 0.4556 - dense_284_loss: 0.4093 - dense_285_loss: 0.3864 - dense_276_accuracy: 0.7313 - dense_277_accuracy: 0.6723 - dense_278_accuracy: 0.6444 - dense_279_accuracy: 0.6283 - dense_280_accuracy: 0.6368 - dense_281_accuracy: 0.6742 - dense_282_accuracy: 0.7174 - dense_283_accuracy: 0.7495 - dense_284_accuracy: 0.7718 - dense_285_accuracy: 0.7795 - val_loss: 1.9025 - val_dense_276_loss: 0.5201 - val_dense_277_loss: 0.6087 - val_dense_278_loss: 0.6348 - val_dense_279_loss: 0.6586 - val_dense_280_loss: 0.6426 - val_dense_281_loss: 0.6053 - val_dense_282_loss: 0.5385 - val_dense_283_loss: 0.4963 - val_dense_284_loss: 0.4297 - val_dense_285_loss: 0.4026 - val_dense_276_accuracy: 0.7667 - val_dense_277_accuracy: 0.6842 - val_dense_278_accuracy: 0.6487 - val_dense_279_accuracy: 0.6105 - val_dense_280_accuracy: 0.6430 - val_dense_281_accuracy: 0.6760 - val_dense_282_accuracy: 0.7076 - val_dense_283_accuracy: 0.7213 - val_dense_284_accuracy: 0.7645 - val_dense_285_accuracy: 0.7808\n",
      "2020-01-12 15:19:07.584074\t\tValid\tMAP:\t0.5390588417455625\tFPA:\t0.7667204952227157\n",
      "2020-01-12 15:19:07.940519\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5390588417455625\n",
      "2020-01-12 15:19:07.940648\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:19:09.552400\t\tAverage best MAP over all folds:\t\t0.5244710587239494...\n",
      "=========================================================\n",
      "2020-01-12 15:19:09.564147\t----- FOLD 1 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.9091 - dense_276_loss: 0.5467 - dense_277_loss: 0.6108 - dense_278_loss: 0.6339 - dense_279_loss: 0.6456 - dense_280_loss: 0.6425 - dense_281_loss: 0.5959 - dense_282_loss: 0.5145 - dense_283_loss: 0.4458 - dense_284_loss: 0.4019 - dense_285_loss: 0.3743 - dense_276_accuracy: 0.7375 - dense_277_accuracy: 0.6770 - dense_278_accuracy: 0.6460 - dense_279_accuracy: 0.6322 - dense_280_accuracy: 0.6412 - dense_281_accuracy: 0.6766 - dense_282_accuracy: 0.7205 - dense_283_accuracy: 0.7544 - dense_284_accuracy: 0.7775 - dense_285_accuracy: 0.7875 - val_loss: 1.8811 - val_dense_276_loss: 0.5225 - val_dense_277_loss: 0.5991 - val_dense_278_loss: 0.6205 - val_dense_279_loss: 0.6377 - val_dense_280_loss: 0.6499 - val_dense_281_loss: 0.6080 - val_dense_282_loss: 0.5219 - val_dense_283_loss: 0.4605 - val_dense_284_loss: 0.4171 - val_dense_285_loss: 0.3854 - val_dense_276_accuracy: 0.7528 - val_dense_277_accuracy: 0.6887 - val_dense_278_accuracy: 0.6611 - val_dense_279_accuracy: 0.6400 - val_dense_280_accuracy: 0.6469 - val_dense_281_accuracy: 0.6669 - val_dense_282_accuracy: 0.7199 - val_dense_283_accuracy: 0.7443 - val_dense_284_accuracy: 0.7746 - val_dense_285_accuracy: 0.7926\n",
      "2020-01-12 15:19:15.508040\t\tValid\tMAP:\t0.5533261812391472\tFPA:\t0.7527545544770973\n",
      "2020-01-12 15:19:15.787995\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5533261812391472\n",
      "2020-01-12 15:19:15.788442\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8908 - dense_276_loss: 0.5389 - dense_277_loss: 0.6070 - dense_278_loss: 0.6282 - dense_279_loss: 0.6398 - dense_280_loss: 0.6376 - dense_281_loss: 0.5891 - dense_282_loss: 0.5097 - dense_283_loss: 0.4406 - dense_284_loss: 0.3956 - dense_285_loss: 0.3671 - dense_276_accuracy: 0.7417 - dense_277_accuracy: 0.6798 - dense_278_accuracy: 0.6504 - dense_279_accuracy: 0.6364 - dense_280_accuracy: 0.6442 - dense_281_accuracy: 0.6796 - dense_282_accuracy: 0.7240 - dense_283_accuracy: 0.7587 - dense_284_accuracy: 0.7825 - dense_285_accuracy: 0.7910 - val_loss: 1.8661 - val_dense_276_loss: 0.5263 - val_dense_277_loss: 0.5925 - val_dense_278_loss: 0.6193 - val_dense_279_loss: 0.6470 - val_dense_280_loss: 0.6364 - val_dense_281_loss: 0.5835 - val_dense_282_loss: 0.5040 - val_dense_283_loss: 0.4348 - val_dense_284_loss: 0.3917 - val_dense_285_loss: 0.3637 - val_dense_276_accuracy: 0.7576 - val_dense_277_accuracy: 0.6929 - val_dense_278_accuracy: 0.6637 - val_dense_279_accuracy: 0.6346 - val_dense_280_accuracy: 0.6496 - val_dense_281_accuracy: 0.6822 - val_dense_282_accuracy: 0.7309 - val_dense_283_accuracy: 0.7597 - val_dense_284_accuracy: 0.7792 - val_dense_285_accuracy: 0.7936\n",
      "2020-01-12 15:19:22.803031\t\tValid\tMAP:\t0.5577779893178948\tFPA:\t0.7576160277221727\n",
      "2020-01-12 15:19:23.207055\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5577779893178948\n",
      "2020-01-12 15:19:23.207557\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:19:24.760368\t\tAverage best MAP over all folds:\t\t0.5400115720012352...\n",
      "=========================================================\n",
      "2020-01-12 15:19:24.805486\t----- FOLD 2 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8852 - dense_276_loss: 0.5396 - dense_277_loss: 0.6046 - dense_278_loss: 0.6260 - dense_279_loss: 0.6388 - dense_280_loss: 0.6365 - dense_281_loss: 0.5861 - dense_282_loss: 0.5025 - dense_283_loss: 0.4373 - dense_284_loss: 0.3931 - dense_285_loss: 0.3631 - dense_276_accuracy: 0.7427 - dense_277_accuracy: 0.6822 - dense_278_accuracy: 0.6533 - dense_279_accuracy: 0.6366 - dense_280_accuracy: 0.6456 - dense_281_accuracy: 0.6791 - dense_282_accuracy: 0.7260 - dense_283_accuracy: 0.7592 - dense_284_accuracy: 0.7831 - dense_285_accuracy: 0.7931 - val_loss: 1.8285 - val_dense_276_loss: 0.5128 - val_dense_277_loss: 0.5901 - val_dense_278_loss: 0.6162 - val_dense_279_loss: 0.6270 - val_dense_280_loss: 0.6246 - val_dense_281_loss: 0.5779 - val_dense_282_loss: 0.4936 - val_dense_283_loss: 0.4321 - val_dense_284_loss: 0.3854 - val_dense_285_loss: 0.3642 - val_dense_276_accuracy: 0.7627 - val_dense_277_accuracy: 0.6956 - val_dense_278_accuracy: 0.6649 - val_dense_279_accuracy: 0.6496 - val_dense_280_accuracy: 0.6547 - val_dense_281_accuracy: 0.6910 - val_dense_282_accuracy: 0.7341 - val_dense_283_accuracy: 0.7618 - val_dense_284_accuracy: 0.7879 - val_dense_285_accuracy: 0.7926\n",
      "2020-01-12 15:19:30.712150\t\tValid\tMAP:\t0.5647902713263893\tFPA:\t0.7626625397412822\n",
      "2020-01-12 15:19:31.171524\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5647902713263893\n",
      "2020-01-12 15:19:31.171741\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8700 - dense_276_loss: 0.5343 - dense_277_loss: 0.5978 - dense_278_loss: 0.6217 - dense_279_loss: 0.6353 - dense_280_loss: 0.6330 - dense_281_loss: 0.5843 - dense_282_loss: 0.5019 - dense_283_loss: 0.4366 - dense_284_loss: 0.3903 - dense_285_loss: 0.3623 - dense_276_accuracy: 0.7437 - dense_277_accuracy: 0.6860 - dense_278_accuracy: 0.6573 - dense_279_accuracy: 0.6413 - dense_280_accuracy: 0.6491 - dense_281_accuracy: 0.6821 - dense_282_accuracy: 0.7288 - dense_283_accuracy: 0.7604 - dense_284_accuracy: 0.7842 - dense_285_accuracy: 0.7940 - val_loss: 1.8408 - val_dense_276_loss: 0.5129 - val_dense_277_loss: 0.5945 - val_dense_278_loss: 0.6193 - val_dense_279_loss: 0.6242 - val_dense_280_loss: 0.6385 - val_dense_281_loss: 0.5836 - val_dense_282_loss: 0.5021 - val_dense_283_loss: 0.4319 - val_dense_284_loss: 0.3811 - val_dense_285_loss: 0.3561 - val_dense_276_accuracy: 0.7651 - val_dense_277_accuracy: 0.6946 - val_dense_278_accuracy: 0.6646 - val_dense_279_accuracy: 0.6504 - val_dense_280_accuracy: 0.6399 - val_dense_281_accuracy: 0.6834 - val_dense_282_accuracy: 0.7255 - val_dense_283_accuracy: 0.7633 - val_dense_284_accuracy: 0.7888 - val_dense_285_accuracy: 0.7906\n",
      "2020-01-12 15:19:38.133312\t\tValid\tMAP:\t0.557981511366034\tFPA:\t0.7651016872171851\n",
      "2020-01-12 15:19:38.133791\t\tStopping at epoch 1, best epoch was 0 with MAP 0.5647902713263893\n",
      "2020-01-12 15:19:38.133941\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:19:39.722804\t\tAverage best MAP over all folds:\t\t0.5482711384429532...\n",
      "=========================================================\n",
      "=========================================================\n",
      "PARAMETERS CONFIG: {'dropout_1': 0.35, 'dropout_2': 0.35, 'dropout_3': 0.25, 'lr': 0.005, 'amsgrad': True}\n",
      "=========================================================\n",
      "2020-01-12 15:19:41.913660\t----- FOLD 0 -----\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 10s - loss: 2.2685 - dense_289_loss: 0.7183 - dense_290_loss: 0.7218 - dense_291_loss: 0.7276 - dense_292_loss: 0.6969 - dense_293_loss: 0.7185 - dense_294_loss: 0.6399 - dense_295_loss: 0.5914 - dense_296_loss: 0.4992 - dense_297_loss: 0.4604 - dense_298_loss: 0.4442 - dense_289_accuracy: 0.6680 - dense_290_accuracy: 0.6240 - dense_291_accuracy: 0.6026 - dense_292_accuracy: 0.5966 - dense_293_accuracy: 0.6033 - dense_294_accuracy: 0.6467 - dense_295_accuracy: 0.6866 - dense_296_accuracy: 0.7247 - dense_297_accuracy: 0.7438 - dense_298_accuracy: 0.7493 - val_loss: 2.2253 - val_dense_289_loss: 0.6107 - val_dense_290_loss: 0.7944 - val_dense_291_loss: 0.7409 - val_dense_292_loss: 0.7169 - val_dense_293_loss: 0.7452 - val_dense_294_loss: 0.6688 - val_dense_295_loss: 0.5410 - val_dense_296_loss: 0.4667 - val_dense_297_loss: 0.5011 - val_dense_298_loss: 0.5158 - val_dense_289_accuracy: 0.6789 - val_dense_290_accuracy: 0.6108 - val_dense_291_accuracy: 0.5506 - val_dense_292_accuracy: 0.5912 - val_dense_293_accuracy: 0.5271 - val_dense_294_accuracy: 0.6053 - val_dense_295_accuracy: 0.7005 - val_dense_296_accuracy: 0.7546 - val_dense_297_accuracy: 0.6953 - val_dense_298_accuracy: 0.7223\n",
      "2020-01-12 15:19:54.623930\t\tValid\tMAP:\t0.4563421387182105\tFPA:\t0.6789126631678105\n",
      "2020-01-12 15:19:54.995363\t\tStopping at epoch 0, best epoch was 0 with MAP 0.4563421387182105\n",
      "2020-01-12 15:19:54.995636\t\tPredicting for test set...\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 4s - loss: 1.9088 - dense_289_loss: 0.5436 - dense_290_loss: 0.6104 - dense_291_loss: 0.6337 - dense_292_loss: 0.6449 - dense_293_loss: 0.6429 - dense_294_loss: 0.5970 - dense_295_loss: 0.5154 - dense_296_loss: 0.4532 - dense_297_loss: 0.4053 - dense_298_loss: 0.3834 - dense_289_accuracy: 0.7393 - dense_290_accuracy: 0.6765 - dense_291_accuracy: 0.6475 - dense_292_accuracy: 0.6345 - dense_293_accuracy: 0.6409 - dense_294_accuracy: 0.6764 - dense_295_accuracy: 0.7224 - dense_296_accuracy: 0.7513 - dense_297_accuracy: 0.7723 - dense_298_accuracy: 0.7789 - val_loss: 1.9228 - val_dense_289_loss: 0.5333 - val_dense_290_loss: 0.6098 - val_dense_291_loss: 0.6255 - val_dense_292_loss: 0.6527 - val_dense_293_loss: 0.6449 - val_dense_294_loss: 0.6242 - val_dense_295_loss: 0.5256 - val_dense_296_loss: 0.4967 - val_dense_297_loss: 0.4510 - val_dense_298_loss: 0.4750 - val_dense_289_accuracy: 0.7556 - val_dense_290_accuracy: 0.6832 - val_dense_291_accuracy: 0.6548 - val_dense_292_accuracy: 0.6281 - val_dense_293_accuracy: 0.6345 - val_dense_294_accuracy: 0.6436 - val_dense_295_accuracy: 0.7175 - val_dense_296_accuracy: 0.7036 - val_dense_297_accuracy: 0.7026 - val_dense_298_accuracy: 0.7258\n",
      "2020-01-12 15:20:02.079310\t\tValid\tMAP:\t0.525559066299288\tFPA:\t0.755635176961378\n",
      "2020-01-12 15:20:02.476784\t\tStopping at epoch 1, best epoch was 1 with MAP 0.525559066299288\n",
      "2020-01-12 15:20:02.476938\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:20:04.059994\t\tAverage best MAP over all folds:\t\t0.4909506025087492...\n",
      "=========================================================\n",
      "2020-01-12 15:20:04.070929\t----- FOLD 1 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8882 - dense_289_loss: 0.5346 - dense_290_loss: 0.6064 - dense_291_loss: 0.6287 - dense_292_loss: 0.6407 - dense_293_loss: 0.6373 - dense_294_loss: 0.5885 - dense_295_loss: 0.5096 - dense_296_loss: 0.4458 - dense_297_loss: 0.3961 - dense_298_loss: 0.3741 - dense_289_accuracy: 0.7453 - dense_290_accuracy: 0.6801 - dense_291_accuracy: 0.6523 - dense_292_accuracy: 0.6381 - dense_293_accuracy: 0.6466 - dense_294_accuracy: 0.6782 - dense_295_accuracy: 0.7240 - dense_296_accuracy: 0.7573 - dense_297_accuracy: 0.7797 - dense_298_accuracy: 0.7836 - val_loss: 1.9010 - val_dense_289_loss: 0.5472 - val_dense_290_loss: 0.6019 - val_dense_291_loss: 0.6298 - val_dense_292_loss: 0.6331 - val_dense_293_loss: 0.6512 - val_dense_294_loss: 0.5877 - val_dense_295_loss: 0.5196 - val_dense_296_loss: 0.4411 - val_dense_297_loss: 0.3878 - val_dense_298_loss: 0.3707 - val_dense_289_accuracy: 0.7480 - val_dense_290_accuracy: 0.6928 - val_dense_291_accuracy: 0.6538 - val_dense_292_accuracy: 0.6423 - val_dense_293_accuracy: 0.6440 - val_dense_294_accuracy: 0.6823 - val_dense_295_accuracy: 0.7234 - val_dense_296_accuracy: 0.7627 - val_dense_297_accuracy: 0.7887 - val_dense_298_accuracy: 0.7896\n",
      "2020-01-12 15:20:10.045073\t\tValid\tMAP:\t0.5548914821616551\tFPA:\t0.7480276548858648\n",
      "2020-01-12 15:20:10.485561\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5548914821616551\n",
      "2020-01-12 15:20:10.485762\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8749 - dense_289_loss: 0.5334 - dense_290_loss: 0.6029 - dense_291_loss: 0.6267 - dense_292_loss: 0.6378 - dense_293_loss: 0.6357 - dense_294_loss: 0.5848 - dense_295_loss: 0.5021 - dense_296_loss: 0.4422 - dense_297_loss: 0.3926 - dense_298_loss: 0.3637 - dense_289_accuracy: 0.7477 - dense_290_accuracy: 0.6824 - dense_291_accuracy: 0.6516 - dense_292_accuracy: 0.6399 - dense_293_accuracy: 0.6474 - dense_294_accuracy: 0.6811 - dense_295_accuracy: 0.7272 - dense_296_accuracy: 0.7591 - dense_297_accuracy: 0.7820 - dense_298_accuracy: 0.7887 - val_loss: 1.8724 - val_dense_289_loss: 0.5141 - val_dense_290_loss: 0.5923 - val_dense_291_loss: 0.6427 - val_dense_292_loss: 0.6420 - val_dense_293_loss: 0.6399 - val_dense_294_loss: 0.6271 - val_dense_295_loss: 0.5000 - val_dense_296_loss: 0.4417 - val_dense_297_loss: 0.3891 - val_dense_298_loss: 0.3616 - val_dense_289_accuracy: 0.7620 - val_dense_290_accuracy: 0.6908 - val_dense_291_accuracy: 0.6314 - val_dense_292_accuracy: 0.6341 - val_dense_293_accuracy: 0.6419 - val_dense_294_accuracy: 0.6115 - val_dense_295_accuracy: 0.7262 - val_dense_296_accuracy: 0.7584 - val_dense_297_accuracy: 0.7852 - val_dense_298_accuracy: 0.7858\n",
      "2020-01-12 15:20:17.463758\t\tValid\tMAP:\t0.5388205202744535\tFPA:\t0.7620064931787979\n",
      "2020-01-12 15:20:17.464193\t\tStopping at epoch 1, best epoch was 0 with MAP 0.5548914821616551\n",
      "2020-01-12 15:20:17.464333\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:20:19.031370\t\tAverage best MAP over all folds:\t\t0.5229210423352022...\n",
      "=========================================================\n",
      "2020-01-12 15:20:19.085066\t----- FOLD 2 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8865 - dense_289_loss: 0.5364 - dense_290_loss: 0.6034 - dense_291_loss: 0.6287 - dense_292_loss: 0.6409 - dense_293_loss: 0.6390 - dense_294_loss: 0.5871 - dense_295_loss: 0.5073 - dense_296_loss: 0.4434 - dense_297_loss: 0.3947 - dense_298_loss: 0.3669 - dense_289_accuracy: 0.7437 - dense_290_accuracy: 0.6829 - dense_291_accuracy: 0.6511 - dense_292_accuracy: 0.6374 - dense_293_accuracy: 0.6433 - dense_294_accuracy: 0.6784 - dense_295_accuracy: 0.7254 - dense_296_accuracy: 0.7576 - dense_297_accuracy: 0.7815 - dense_298_accuracy: 0.7858 - val_loss: 1.8641 - val_dense_289_loss: 0.5200 - val_dense_290_loss: 0.6049 - val_dense_291_loss: 0.6349 - val_dense_292_loss: 0.6352 - val_dense_293_loss: 0.6382 - val_dense_294_loss: 0.5815 - val_dense_295_loss: 0.4986 - val_dense_296_loss: 0.4463 - val_dense_297_loss: 0.3913 - val_dense_298_loss: 0.3990 - val_dense_289_accuracy: 0.7604 - val_dense_290_accuracy: 0.6893 - val_dense_291_accuracy: 0.6537 - val_dense_292_accuracy: 0.6392 - val_dense_293_accuracy: 0.6451 - val_dense_294_accuracy: 0.6844 - val_dense_295_accuracy: 0.7300 - val_dense_296_accuracy: 0.7570 - val_dense_297_accuracy: 0.7852 - val_dense_298_accuracy: 0.7296\n",
      "2020-01-12 15:20:25.067635\t\tValid\tMAP:\t0.5509247555744106\tFPA:\t0.7603747876259526\n",
      "2020-01-12 15:20:25.442419\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5509247555744106\n",
      "2020-01-12 15:20:25.442908\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8686 - dense_289_loss: 0.5316 - dense_290_loss: 0.5985 - dense_291_loss: 0.6230 - dense_292_loss: 0.6367 - dense_293_loss: 0.6350 - dense_294_loss: 0.5818 - dense_295_loss: 0.5017 - dense_296_loss: 0.4385 - dense_297_loss: 0.3886 - dense_298_loss: 0.3597 - dense_289_accuracy: 0.7472 - dense_290_accuracy: 0.6855 - dense_291_accuracy: 0.6558 - dense_292_accuracy: 0.6409 - dense_293_accuracy: 0.6495 - dense_294_accuracy: 0.6812 - dense_295_accuracy: 0.7290 - dense_296_accuracy: 0.7616 - dense_297_accuracy: 0.7846 - dense_298_accuracy: 0.7907 - val_loss: 1.8318 - val_dense_289_loss: 0.5064 - val_dense_290_loss: 0.5951 - val_dense_291_loss: 0.6212 - val_dense_292_loss: 0.6310 - val_dense_293_loss: 0.6262 - val_dense_294_loss: 0.5784 - val_dense_295_loss: 0.4906 - val_dense_296_loss: 0.4383 - val_dense_297_loss: 0.3904 - val_dense_298_loss: 0.3561 - val_dense_289_accuracy: 0.7691 - val_dense_290_accuracy: 0.6929 - val_dense_291_accuracy: 0.6602 - val_dense_292_accuracy: 0.6413 - val_dense_293_accuracy: 0.6528 - val_dense_294_accuracy: 0.6848 - val_dense_295_accuracy: 0.7322 - val_dense_296_accuracy: 0.7618 - val_dense_297_accuracy: 0.7890 - val_dense_298_accuracy: 0.7915\n",
      "2020-01-12 15:20:32.474342\t\tValid\tMAP:\t0.5619163423534127\tFPA:\t0.7690716100055511\n",
      "2020-01-12 15:20:32.853195\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5619163423534127\n",
      "2020-01-12 15:20:32.853336\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:20:34.462109\t\tAverage best MAP over all folds:\t\t0.534087544544772...\n",
      "=========================================================\n",
      "=========================================================\n",
      "PARAMETERS CONFIG: {'dropout_1': 0.25, 'dropout_2': 0.35, 'dropout_3': 0.25, 'lr': 0.001, 'amsgrad': True}\n",
      "=========================================================\n",
      "2020-01-12 15:20:35.919398\t----- FOLD 0 -----\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 10s - loss: 2.1145 - dense_302_loss: 0.6467 - dense_303_loss: 0.6553 - dense_304_loss: 0.6699 - dense_305_loss: 0.6837 - dense_306_loss: 0.6800 - dense_307_loss: 0.6425 - dense_308_loss: 0.5617 - dense_309_loss: 0.5053 - dense_310_loss: 0.4838 - dense_311_loss: 0.4651 - dense_302_accuracy: 0.6661 - dense_303_accuracy: 0.6295 - dense_304_accuracy: 0.6115 - dense_305_accuracy: 0.5983 - dense_306_accuracy: 0.6098 - dense_307_accuracy: 0.6451 - dense_308_accuracy: 0.6928 - dense_309_accuracy: 0.7247 - dense_310_accuracy: 0.7383 - dense_311_accuracy: 0.7445 - val_loss: 1.9945 - val_dense_302_loss: 0.5639 - val_dense_303_loss: 0.6100 - val_dense_304_loss: 0.6351 - val_dense_305_loss: 0.6437 - val_dense_306_loss: 0.6483 - val_dense_307_loss: 0.6735 - val_dense_308_loss: 0.6188 - val_dense_309_loss: 0.6054 - val_dense_310_loss: 0.5075 - val_dense_311_loss: 0.5343 - val_dense_302_accuracy: 0.7402 - val_dense_303_accuracy: 0.6835 - val_dense_304_accuracy: 0.6473 - val_dense_305_accuracy: 0.6316 - val_dense_306_accuracy: 0.6337 - val_dense_307_accuracy: 0.5967 - val_dense_308_accuracy: 0.6506 - val_dense_309_accuracy: 0.6803 - val_dense_310_accuracy: 0.7326 - val_dense_311_accuracy: 0.7327\n",
      "2020-01-12 15:20:48.627689\t\tValid\tMAP:\t0.5150824513459755\tFPA:\t0.7402435742161216\n",
      "2020-01-12 15:20:49.053170\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5150824513459755\n",
      "2020-01-12 15:20:49.053461\t\tPredicting for test set...\n",
      "Train on 118894 samples, validate on 59448 samples\n",
      "118894/118894 - 4s - loss: 1.9126 - dense_302_loss: 0.5428 - dense_303_loss: 0.6140 - dense_304_loss: 0.6357 - dense_305_loss: 0.6474 - dense_306_loss: 0.6437 - dense_307_loss: 0.6000 - dense_308_loss: 0.5210 - dense_309_loss: 0.4528 - dense_310_loss: 0.4087 - dense_311_loss: 0.3853 - dense_302_accuracy: 0.7415 - dense_303_accuracy: 0.6750 - dense_304_accuracy: 0.6450 - dense_305_accuracy: 0.6305 - dense_306_accuracy: 0.6412 - dense_307_accuracy: 0.6750 - dense_308_accuracy: 0.7184 - dense_309_accuracy: 0.7522 - dense_310_accuracy: 0.7723 - dense_311_accuracy: 0.7804 - val_loss: 1.9067 - val_dense_302_loss: 0.5286 - val_dense_303_loss: 0.6050 - val_dense_304_loss: 0.6262 - val_dense_305_loss: 0.6351 - val_dense_306_loss: 0.6384 - val_dense_307_loss: 0.6075 - val_dense_308_loss: 0.5633 - val_dense_309_loss: 0.5042 - val_dense_310_loss: 0.4518 - val_dense_311_loss: 0.4703 - val_dense_302_accuracy: 0.7510 - val_dense_303_accuracy: 0.6831 - val_dense_304_accuracy: 0.6531 - val_dense_305_accuracy: 0.6407 - val_dense_306_accuracy: 0.6501 - val_dense_307_accuracy: 0.6744 - val_dense_308_accuracy: 0.6841 - val_dense_309_accuracy: 0.7139 - val_dense_310_accuracy: 0.7462 - val_dense_311_accuracy: 0.7497\n",
      "2020-01-12 15:20:56.070137\t\tValid\tMAP:\t0.5387905757354094\tFPA:\t0.7510092854259185\n",
      "2020-01-12 15:20:56.570714\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5387905757354094\n",
      "2020-01-12 15:20:56.571554\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:20:58.184241\t\tAverage best MAP over all folds:\t\t0.5269365135406925...\n",
      "=========================================================\n",
      "2020-01-12 15:20:58.196178\t----- FOLD 1 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8965 - dense_302_loss: 0.5389 - dense_303_loss: 0.6077 - dense_304_loss: 0.6323 - dense_305_loss: 0.6440 - dense_306_loss: 0.6417 - dense_307_loss: 0.5934 - dense_308_loss: 0.5127 - dense_309_loss: 0.4434 - dense_310_loss: 0.3991 - dense_311_loss: 0.3705 - dense_302_accuracy: 0.7448 - dense_303_accuracy: 0.6806 - dense_304_accuracy: 0.6474 - dense_305_accuracy: 0.6347 - dense_306_accuracy: 0.6433 - dense_307_accuracy: 0.6776 - dense_308_accuracy: 0.7241 - dense_309_accuracy: 0.7590 - dense_310_accuracy: 0.7794 - dense_311_accuracy: 0.7907 - val_loss: 1.8508 - val_dense_302_loss: 0.5101 - val_dense_303_loss: 0.5909 - val_dense_304_loss: 0.6178 - val_dense_305_loss: 0.6411 - val_dense_306_loss: 0.6281 - val_dense_307_loss: 0.5982 - val_dense_308_loss: 0.5124 - val_dense_309_loss: 0.4384 - val_dense_310_loss: 0.3927 - val_dense_311_loss: 0.3878 - val_dense_302_accuracy: 0.7619 - val_dense_303_accuracy: 0.6920 - val_dense_304_accuracy: 0.6629 - val_dense_305_accuracy: 0.6361 - val_dense_306_accuracy: 0.6534 - val_dense_307_accuracy: 0.6782 - val_dense_308_accuracy: 0.7264 - val_dense_309_accuracy: 0.7591 - val_dense_310_accuracy: 0.7840 - val_dense_311_accuracy: 0.7904\n",
      "2020-01-12 15:21:04.129215\t\tValid\tMAP:\t0.5582821833743508\tFPA:\t0.761871919524955\n",
      "2020-01-12 15:21:04.526552\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5582821833743508\n",
      "2020-01-12 15:21:04.527030\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8753 - dense_302_loss: 0.5293 - dense_303_loss: 0.6025 - dense_304_loss: 0.6265 - dense_305_loss: 0.6395 - dense_306_loss: 0.6352 - dense_307_loss: 0.5894 - dense_308_loss: 0.5067 - dense_309_loss: 0.4385 - dense_310_loss: 0.3918 - dense_311_loss: 0.3638 - dense_302_accuracy: 0.7512 - dense_303_accuracy: 0.6849 - dense_304_accuracy: 0.6542 - dense_305_accuracy: 0.6392 - dense_306_accuracy: 0.6461 - dense_307_accuracy: 0.6795 - dense_308_accuracy: 0.7254 - dense_309_accuracy: 0.7591 - dense_310_accuracy: 0.7838 - dense_311_accuracy: 0.7952 - val_loss: 1.8358 - val_dense_302_loss: 0.5049 - val_dense_303_loss: 0.5907 - val_dense_304_loss: 0.6172 - val_dense_305_loss: 0.6318 - val_dense_306_loss: 0.6316 - val_dense_307_loss: 0.5839 - val_dense_308_loss: 0.4983 - val_dense_309_loss: 0.4316 - val_dense_310_loss: 0.3857 - val_dense_311_loss: 0.3563 - val_dense_302_accuracy: 0.7655 - val_dense_303_accuracy: 0.6950 - val_dense_304_accuracy: 0.6623 - val_dense_305_accuracy: 0.6460 - val_dense_306_accuracy: 0.6513 - val_dense_307_accuracy: 0.6852 - val_dense_308_accuracy: 0.7353 - val_dense_309_accuracy: 0.7654 - val_dense_310_accuracy: 0.7891 - val_dense_311_accuracy: 0.8027\n",
      "2020-01-12 15:21:11.458323\t\tValid\tMAP:\t0.5644248024475667\tFPA:\t0.7655390515921745\n",
      "2020-01-12 15:21:11.849675\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5644248024475667\n",
      "2020-01-12 15:21:11.849794\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:21:13.408792\t\tAverage best MAP over all folds:\t\t0.5441450032258256...\n",
      "=========================================================\n",
      "2020-01-12 15:21:13.455355\t----- FOLD 2 -----\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8702 - dense_302_loss: 0.5288 - dense_303_loss: 0.5999 - dense_304_loss: 0.6233 - dense_305_loss: 0.6366 - dense_306_loss: 0.6360 - dense_307_loss: 0.5864 - dense_308_loss: 0.5028 - dense_309_loss: 0.4380 - dense_310_loss: 0.3920 - dense_311_loss: 0.3624 - dense_302_accuracy: 0.7512 - dense_303_accuracy: 0.6848 - dense_304_accuracy: 0.6560 - dense_305_accuracy: 0.6400 - dense_306_accuracy: 0.6449 - dense_307_accuracy: 0.6801 - dense_308_accuracy: 0.7286 - dense_309_accuracy: 0.7607 - dense_310_accuracy: 0.7841 - dense_311_accuracy: 0.7939 - val_loss: 1.8123 - val_dense_302_loss: 0.4944 - val_dense_303_loss: 0.5898 - val_dense_304_loss: 0.6228 - val_dense_305_loss: 0.6224 - val_dense_306_loss: 0.6223 - val_dense_307_loss: 0.5788 - val_dense_308_loss: 0.4938 - val_dense_309_loss: 0.4306 - val_dense_310_loss: 0.3829 - val_dense_311_loss: 0.3565 - val_dense_302_accuracy: 0.7749 - val_dense_303_accuracy: 0.6975 - val_dense_304_accuracy: 0.6639 - val_dense_305_accuracy: 0.6509 - val_dense_306_accuracy: 0.6565 - val_dense_307_accuracy: 0.6877 - val_dense_308_accuracy: 0.7322 - val_dense_309_accuracy: 0.7628 - val_dense_310_accuracy: 0.7896 - val_dense_311_accuracy: 0.8000\n",
      "2020-01-12 15:21:19.490235\t\tValid\tMAP:\t0.5689176845414573\tFPA:\t0.7748919205342574\n",
      "2020-01-12 15:21:20.062416\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5689176845414573\n",
      "2020-01-12 15:21:20.062633\t\tPredicting for test set...\n",
      "Train on 118895 samples, validate on 59447 samples\n",
      "118895/118895 - 4s - loss: 1.8564 - dense_302_loss: 0.5242 - dense_303_loss: 0.5960 - dense_304_loss: 0.6205 - dense_305_loss: 0.6320 - dense_306_loss: 0.6302 - dense_307_loss: 0.5815 - dense_308_loss: 0.4998 - dense_309_loss: 0.4347 - dense_310_loss: 0.3895 - dense_311_loss: 0.3611 - dense_302_accuracy: 0.7524 - dense_303_accuracy: 0.6884 - dense_304_accuracy: 0.6576 - dense_305_accuracy: 0.6432 - dense_306_accuracy: 0.6512 - dense_307_accuracy: 0.6842 - dense_308_accuracy: 0.7300 - dense_309_accuracy: 0.7623 - dense_310_accuracy: 0.7851 - dense_311_accuracy: 0.7957 - val_loss: 1.8209 - val_dense_302_loss: 0.5038 - val_dense_303_loss: 0.5917 - val_dense_304_loss: 0.6184 - val_dense_305_loss: 0.6249 - val_dense_306_loss: 0.6248 - val_dense_307_loss: 0.5767 - val_dense_308_loss: 0.4960 - val_dense_309_loss: 0.4266 - val_dense_310_loss: 0.3770 - val_dense_311_loss: 0.3487 - val_dense_302_accuracy: 0.7675 - val_dense_303_accuracy: 0.6899 - val_dense_304_accuracy: 0.6579 - val_dense_305_accuracy: 0.6495 - val_dense_306_accuracy: 0.6525 - val_dense_307_accuracy: 0.6892 - val_dense_308_accuracy: 0.7316 - val_dense_309_accuracy: 0.7655 - val_dense_310_accuracy: 0.7931 - val_dense_311_accuracy: 0.7993\n",
      "2020-01-12 15:21:27.070058\t\tValid\tMAP:\t0.5630932637061765\tFPA:\t0.7674903695728968\n",
      "2020-01-12 15:21:27.070556\t\tStopping at epoch 1, best epoch was 0 with MAP 0.5689176845414573\n",
      "2020-01-12 15:21:27.070710\t\tPredicting for test set...\n",
      "=========================================================\n",
      "2020-01-12 15:21:28.657050\t\tAverage best MAP over all folds:\t\t0.5524025636643695...\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, utils, Model, Input\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "\n",
    "# k-fold Cross-validation grouped on sessions\n",
    "k = 3\n",
    "n_epochs = 2\n",
    "param_config_list = [{'dropout_1': 0.2, 'dropout_2':.2, 'dropout_3':.2, 'lr': 0.001, 'amsgrad':True},\n",
    "                     {'dropout_1': 0.3, 'dropout_2':.3, 'dropout_3':.3, 'lr': 0.001, 'amsgrad':True},\n",
    "                     {'dropout_1': 0.4, 'dropout_2':.4, 'dropout_3':.4, 'lr': 0.001, 'amsgrad':True},\n",
    "                     {'dropout_1': 0.35, 'dropout_2':.35, 'dropout_3':.35, 'lr': 0.001, 'amsgrad':True},\n",
    "                     {'dropout_1': 0.35, 'dropout_2':.35, 'dropout_3':.45, 'lr': 0.001, 'amsgrad':True},\n",
    "                     {'dropout_1': 0.35, 'dropout_2':.35, 'dropout_3':.25, 'lr': 0.1, 'amsgrad':True},\n",
    "                     {'dropout_1': 0.35, 'dropout_2':.35, 'dropout_3':.25, 'lr': 0.01, 'amsgrad':True},\n",
    "                     {'dropout_1': 0.35, 'dropout_2':.35, 'dropout_3':.25, 'lr': 0.001, 'amsgrad':True},\n",
    "                     {'dropout_1': 0.35, 'dropout_2':.35, 'dropout_3':.25, 'lr': 0.005, 'amsgrad':True},\n",
    "                     {'dropout_1': 0.25, 'dropout_2':.35, 'dropout_3':.25, 'lr': 0.001, 'amsgrad':True},\n",
    "                     ]\n",
    "# List for the results on the configurations\n",
    "res_map = []\n",
    "for conf in param_config_list:\n",
    "  test_predictions = []\n",
    "  all_maps = []\n",
    "\n",
    "  print('=========================================================')\n",
    "  print('PARAMETERS CONFIG:', conf)\n",
    "  print('=========================================================')\n",
    "\n",
    "  # Generate model\n",
    "  \n",
    "  # Loss weights\n",
    "  weights = np.asarray([(1 / val + sum((1 / (2*n)) for n in range(val + 1,11))) for val in range(1,11)])\n",
    "  weights /= weights.max()\n",
    "  model = generate_model(train_history, train_future, **conf)\n",
    "  plot_model(model)\n",
    "  for fold_id, (train_idx, valid_idx) in enumerate(KFold(n_splits = k).split(train_history)):\n",
    "      print('{0}\\t----- FOLD {1} -----'.format(datetime.datetime.now(),fold_id))\n",
    "      # Filter out training and testing data\n",
    "      h_train = train_history[train_idx]\n",
    "      h_valid = train_history[valid_idx]\n",
    "      f_train = train_future[train_idx]\n",
    "      f_valid = train_future[valid_idx]\n",
    "      l_train = train_labels[train_idx]\n",
    "      l_valid = train_labels[valid_idx]\n",
    "      s_train = train_session_len[train_idx]\n",
    "      s_valid = train_session_len[valid_idx]\n",
    "\n",
    "      # Loss weights\n",
    "      weights = np.asarray([(1 / val + sum((1 / (2*n)) for n in range(val + 1,11))) for val in range(1,11)])\n",
    "      weights /= weights.max()\n",
    "      \n",
    "      # Early stopping\n",
    "      best_map = .0\n",
    "      best_weights = None\n",
    "      best_epoch = 0\n",
    "\n",
    "      # For every epoch\n",
    "      for epoch_id in range(n_epochs):\n",
    "          model.fit([h_train, f_train], [l_train[:,i] for i in range(10)],\n",
    "                    validation_data = ([h_valid, f_valid], [l_valid[:,i] for i in range(10)]),\n",
    "                    batch_size = 2048, epochs = 1, verbose = 2)\n",
    "          p_valid = model.predict([h_valid, f_valid], batch_size = 4096)\n",
    "          MAP, FPA = evaluation_MAP_FPA(s_valid, l_valid, np.swapaxes(np.round(p_valid),0,1))\n",
    "          print('{0}\\t\\tValid\\tMAP:\\t{1}\\tFPA:\\t{2}'.format(datetime.datetime.now(),MAP, FPA))\n",
    "          if MAP > best_map:\n",
    "              best_map = MAP\n",
    "              best_epoch = epoch_id\n",
    "              best_weights = model.get_weights()\n",
    "              model.save_weights(modelpath + '/model_weights_epoch_{}.h5'.format(epoch_id+1))\n",
    "          elif epoch_id - best_epoch >= 5:\n",
    "              break\n",
    "\n",
    "          print('{0}\\t\\tStopping at epoch {1}, best epoch was {2} with MAP {3}'.format(datetime.datetime.now(),epoch_id, best_epoch, best_map))\n",
    "\n",
    "          all_maps.append(best_map)\n",
    "\n",
    "          print('{0}\\t\\tPredicting for test set...'.format(datetime.datetime.now()))\n",
    "          # Reload best weights\n",
    "          model.set_weights(best_weights)\n",
    "\n",
    "          # Predict for test set\n",
    "          p_test = model.predict([test_history, test_future], batch_size = 4096)\n",
    "          test_predictions.append(np.swapaxes(p_test,0,1))\n",
    "      print('=========================================================')\n",
    "      print('{0}\\t\\tAverage best MAP over all folds:\\t\\t{1}...'.format(datetime.datetime.now(), np.mean(all_maps)))\n",
    "      print('=========================================================')\n",
    "      #print('{0}\\t\\tGenerating submission...'.format(datetime.datetime.now()))\n",
    "      # Geometric mean of predictions over folds\n",
    "      p_test = np.prod(test_predictions, axis = 0) ** (1.0 / len(test_predictions))\n",
    "      \n",
    "  res_map = res_map+[np.mean(all_maps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-OCKxmX3LKji",
    "outputId": "d6711d96-0bf6-4377-bf8a-d248f9c726f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config found is: {'dropout_1': 0.35, 'dropout_2': 0.35, 'dropout_3': 0.25, 'lr': 0.001, 'amsgrad': True}\n"
     ]
    }
   ],
   "source": [
    "best_config = np.argmax(res_map)\n",
    "print(\"Best config found is:\", param_config_list[best_config])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZJmPv2PvS-ux"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42JkPQPfjF9w"
   },
   "source": [
    "# Training:\n",
    "\n",
    "Training implemented using 5-fold cross-validation with **early stopping**, when the results do not improve for 5 epochs consecutively the best weights are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iGclA9DAZ5oM",
    "outputId": "3740f8a0-39a5-49be-a3c7-12f5f7d55317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.55 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.55 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "2020-01-10 18:25:16.109007\t----- FOLD 0 -----\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 9s - loss: 2.0943 - dense_133_loss: 0.6390 - dense_134_loss: 0.6540 - dense_135_loss: 0.6823 - dense_136_loss: 0.6777 - dense_137_loss: 0.6681 - dense_138_loss: 0.6288 - dense_139_loss: 0.5521 - dense_140_loss: 0.4942 - dense_141_loss: 0.4529 - dense_142_loss: 0.4287 - dense_133_acc: 0.6648 - dense_134_acc: 0.6322 - dense_135_acc: 0.6088 - dense_136_acc: 0.5994 - dense_137_acc: 0.6142 - dense_138_acc: 0.6532 - dense_139_acc: 0.6999 - dense_140_acc: 0.7298 - dense_141_acc: 0.7493 - dense_142_acc: 0.7590 - val_loss: 1.9587 - val_dense_133_loss: 0.5583 - val_dense_134_loss: 0.6085 - val_dense_135_loss: 0.6269 - val_dense_136_loss: 0.6387 - val_dense_137_loss: 0.6554 - val_dense_138_loss: 0.6160 - val_dense_139_loss: 0.5795 - val_dense_140_loss: 0.5294 - val_dense_141_loss: 0.5013 - val_dense_142_loss: 0.5546 - val_dense_133_acc: 0.7295 - val_dense_134_acc: 0.6810 - val_dense_135_acc: 0.6526 - val_dense_136_acc: 0.6370 - val_dense_137_acc: 0.6310 - val_dense_138_acc: 0.6678 - val_dense_139_acc: 0.6655 - val_dense_140_acc: 0.7075 - val_dense_141_acc: 0.7249 - val_dense_142_acc: 0.7083\n",
      "2020-01-10 18:26:13.311201\t\tValid\tMAP:\t0.5284044709741235\tFPA:\t0.7294849869634696\n",
      "=========================================================\n",
      "2020-01-10 18:26:27.687467\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5284044709741235\n",
      "=========================================================\n",
      "2020-01-10 18:26:27.688031\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.9421 - dense_133_loss: 0.5671 - dense_134_loss: 0.6208 - dense_135_loss: 0.6400 - dense_136_loss: 0.6511 - dense_137_loss: 0.6450 - dense_138_loss: 0.5974 - dense_139_loss: 0.5132 - dense_140_loss: 0.4479 - dense_141_loss: 0.4028 - dense_142_loss: 0.3771 - dense_133_acc: 0.7179 - dense_134_acc: 0.6661 - dense_135_acc: 0.6399 - dense_136_acc: 0.6270 - dense_137_acc: 0.6367 - dense_138_acc: 0.6760 - dense_139_acc: 0.7223 - dense_140_acc: 0.7545 - dense_141_acc: 0.7752 - dense_142_acc: 0.7845 - val_loss: 1.8982 - val_dense_133_loss: 0.5371 - val_dense_134_loss: 0.6065 - val_dense_135_loss: 0.6314 - val_dense_136_loss: 0.6434 - val_dense_137_loss: 0.6371 - val_dense_138_loss: 0.5995 - val_dense_139_loss: 0.5157 - val_dense_140_loss: 0.4629 - val_dense_141_loss: 0.4063 - val_dense_142_loss: 0.4031 - val_dense_133_acc: 0.7392 - val_dense_134_acc: 0.6796 - val_dense_135_acc: 0.6451 - val_dense_136_acc: 0.6396 - val_dense_137_acc: 0.6479 - val_dense_138_acc: 0.6723 - val_dense_139_acc: 0.7182 - val_dense_140_acc: 0.7453 - val_dense_141_acc: 0.7770 - val_dense_142_acc: 0.7840\n",
      "2020-01-10 18:26:35.600904\t\tValid\tMAP:\t0.5451604537816587\tFPA:\t0.739213322492921\n",
      "=========================================================\n",
      "2020-01-10 18:26:35.961670\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5451604537816587\n",
      "=========================================================\n",
      "2020-01-10 18:26:35.961755\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.9215 - dense_133_loss: 0.5604 - dense_134_loss: 0.6149 - dense_135_loss: 0.6342 - dense_136_loss: 0.6434 - dense_137_loss: 0.6392 - dense_138_loss: 0.5925 - dense_139_loss: 0.5080 - dense_140_loss: 0.4419 - dense_141_loss: 0.3948 - dense_142_loss: 0.3692 - dense_133_acc: 0.7236 - dense_134_acc: 0.6696 - dense_135_acc: 0.6433 - dense_136_acc: 0.6334 - dense_137_acc: 0.6420 - dense_138_acc: 0.6761 - dense_139_acc: 0.7223 - dense_140_acc: 0.7570 - dense_141_acc: 0.7809 - dense_142_acc: 0.7887 - val_loss: 1.8638 - val_dense_133_loss: 0.5281 - val_dense_134_loss: 0.5954 - val_dense_135_loss: 0.6232 - val_dense_136_loss: 0.6342 - val_dense_137_loss: 0.6319 - val_dense_138_loss: 0.5864 - val_dense_139_loss: 0.5068 - val_dense_140_loss: 0.4430 - val_dense_141_loss: 0.3876 - val_dense_142_loss: 0.3587 - val_dense_133_acc: 0.7480 - val_dense_134_acc: 0.6902 - val_dense_135_acc: 0.6514 - val_dense_136_acc: 0.6420 - val_dense_137_acc: 0.6526 - val_dense_138_acc: 0.6801 - val_dense_139_acc: 0.7223 - val_dense_140_acc: 0.7568 - val_dense_141_acc: 0.7871 - val_dense_142_acc: 0.8008\n",
      "2020-01-10 18:26:41.838888\t\tValid\tMAP:\t0.5556597783184706\tFPA:\t0.748044520451933\n",
      "=========================================================\n",
      "2020-01-10 18:26:42.240984\t\tStopping at epoch 2, best epoch was 2 with MAP 0.5556597783184706\n",
      "=========================================================\n",
      "2020-01-10 18:26:42.242428\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.9039 - dense_133_loss: 0.5534 - dense_134_loss: 0.6090 - dense_135_loss: 0.6302 - dense_136_loss: 0.6404 - dense_137_loss: 0.6359 - dense_138_loss: 0.5857 - dense_139_loss: 0.5016 - dense_140_loss: 0.4388 - dense_141_loss: 0.3902 - dense_142_loss: 0.3641 - dense_133_acc: 0.7259 - dense_134_acc: 0.6738 - dense_135_acc: 0.6475 - dense_136_acc: 0.6348 - dense_137_acc: 0.6453 - dense_138_acc: 0.6798 - dense_139_acc: 0.7257 - dense_140_acc: 0.7577 - dense_141_acc: 0.7826 - dense_142_acc: 0.7902 - val_loss: 1.8520 - val_dense_133_loss: 0.5196 - val_dense_134_loss: 0.5957 - val_dense_135_loss: 0.6212 - val_dense_136_loss: 0.6331 - val_dense_137_loss: 0.6283 - val_dense_138_loss: 0.5864 - val_dense_139_loss: 0.5043 - val_dense_140_loss: 0.4304 - val_dense_141_loss: 0.3816 - val_dense_142_loss: 0.3588 - val_dense_133_acc: 0.7494 - val_dense_134_acc: 0.6907 - val_dense_135_acc: 0.6599 - val_dense_136_acc: 0.6412 - val_dense_137_acc: 0.6520 - val_dense_138_acc: 0.6837 - val_dense_139_acc: 0.7270 - val_dense_140_acc: 0.7666 - val_dense_141_acc: 0.7895 - val_dense_142_acc: 0.7981\n",
      "2020-01-10 18:26:48.209241\t\tValid\tMAP:\t0.5578137600832207\tFPA:\t0.749390226807592\n",
      "=========================================================\n",
      "2020-01-10 18:26:48.668956\t\tStopping at epoch 3, best epoch was 3 with MAP 0.5578137600832207\n",
      "=========================================================\n",
      "2020-01-10 18:26:48.669046\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8981 - dense_133_loss: 0.5520 - dense_134_loss: 0.6079 - dense_135_loss: 0.6281 - dense_136_loss: 0.6379 - dense_137_loss: 0.6343 - dense_138_loss: 0.5833 - dense_139_loss: 0.5006 - dense_140_loss: 0.4375 - dense_141_loss: 0.3890 - dense_142_loss: 0.3603 - dense_133_acc: 0.7278 - dense_134_acc: 0.6745 - dense_135_acc: 0.6490 - dense_136_acc: 0.6373 - dense_137_acc: 0.6460 - dense_138_acc: 0.6818 - dense_139_acc: 0.7260 - dense_140_acc: 0.7596 - dense_141_acc: 0.7841 - dense_142_acc: 0.7934 - val_loss: 1.8378 - val_dense_133_loss: 0.5173 - val_dense_134_loss: 0.5862 - val_dense_135_loss: 0.6152 - val_dense_136_loss: 0.6343 - val_dense_137_loss: 0.6331 - val_dense_138_loss: 0.5748 - val_dense_139_loss: 0.4943 - val_dense_140_loss: 0.4300 - val_dense_141_loss: 0.3827 - val_dense_142_loss: 0.3535 - val_dense_133_acc: 0.7525 - val_dense_134_acc: 0.6962 - val_dense_135_acc: 0.6660 - val_dense_136_acc: 0.6409 - val_dense_137_acc: 0.6512 - val_dense_138_acc: 0.6880 - val_dense_139_acc: 0.7324 - val_dense_140_acc: 0.7652 - val_dense_141_acc: 0.7929 - val_dense_142_acc: 0.7949\n",
      "2020-01-10 18:26:54.497943\t\tValid\tMAP:\t0.5602428507522051\tFPA:\t0.7525302083041296\n",
      "=========================================================\n",
      "2020-01-10 18:26:54.877221\t\tStopping at epoch 4, best epoch was 4 with MAP 0.5602428507522051\n",
      "=========================================================\n",
      "2020-01-10 18:26:54.877577\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8872 - dense_133_loss: 0.5469 - dense_134_loss: 0.6043 - dense_135_loss: 0.6257 - dense_136_loss: 0.6356 - dense_137_loss: 0.6328 - dense_138_loss: 0.5803 - dense_139_loss: 0.4985 - dense_140_loss: 0.4351 - dense_141_loss: 0.3868 - dense_142_loss: 0.3603 - dense_133_acc: 0.7318 - dense_134_acc: 0.6774 - dense_135_acc: 0.6512 - dense_136_acc: 0.6401 - dense_137_acc: 0.6470 - dense_138_acc: 0.6840 - dense_139_acc: 0.7283 - dense_140_acc: 0.7615 - dense_141_acc: 0.7847 - dense_142_acc: 0.7930 - val_loss: 1.8292 - val_dense_133_loss: 0.5111 - val_dense_134_loss: 0.5858 - val_dense_135_loss: 0.6188 - val_dense_136_loss: 0.6286 - val_dense_137_loss: 0.6283 - val_dense_138_loss: 0.5794 - val_dense_139_loss: 0.4928 - val_dense_140_loss: 0.4293 - val_dense_141_loss: 0.3829 - val_dense_142_loss: 0.3500 - val_dense_133_acc: 0.7563 - val_dense_134_acc: 0.6953 - val_dense_135_acc: 0.6572 - val_dense_136_acc: 0.6503 - val_dense_137_acc: 0.6538 - val_dense_138_acc: 0.6888 - val_dense_139_acc: 0.7323 - val_dense_140_acc: 0.7673 - val_dense_141_acc: 0.7912 - val_dense_142_acc: 0.8053\n",
      "2020-01-10 18:27:00.842258\t\tValid\tMAP:\t0.562613946574195\tFPA:\t0.7562869718803443\n",
      "=========================================================\n",
      "2020-01-10 18:27:01.313383\t\tStopping at epoch 5, best epoch was 5 with MAP 0.562613946574195\n",
      "=========================================================\n",
      "2020-01-10 18:27:01.314619\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8814 - dense_133_loss: 0.5445 - dense_134_loss: 0.6031 - dense_135_loss: 0.6240 - dense_136_loss: 0.6343 - dense_137_loss: 0.6310 - dense_138_loss: 0.5787 - dense_139_loss: 0.4976 - dense_140_loss: 0.4343 - dense_141_loss: 0.3852 - dense_142_loss: 0.3551 - dense_133_acc: 0.7332 - dense_134_acc: 0.6788 - dense_135_acc: 0.6537 - dense_136_acc: 0.6422 - dense_137_acc: 0.6499 - dense_138_acc: 0.6842 - dense_139_acc: 0.7283 - dense_140_acc: 0.7614 - dense_141_acc: 0.7870 - dense_142_acc: 0.7967 - val_loss: 1.8194 - val_dense_133_loss: 0.5076 - val_dense_134_loss: 0.5827 - val_dense_135_loss: 0.6109 - val_dense_136_loss: 0.6247 - val_dense_137_loss: 0.6262 - val_dense_138_loss: 0.5742 - val_dense_139_loss: 0.4968 - val_dense_140_loss: 0.4309 - val_dense_141_loss: 0.3806 - val_dense_142_loss: 0.3466 - val_dense_133_acc: 0.7575 - val_dense_134_acc: 0.6966 - val_dense_135_acc: 0.6658 - val_dense_136_acc: 0.6520 - val_dense_137_acc: 0.6535 - val_dense_138_acc: 0.6907 - val_dense_139_acc: 0.7283 - val_dense_140_acc: 0.7626 - val_dense_141_acc: 0.7921 - val_dense_142_acc: 0.8039\n",
      "2020-01-10 18:27:07.300573\t\tValid\tMAP:\t0.5645006067914378\tFPA:\t0.7574644649415458\n",
      "=========================================================\n",
      "2020-01-10 18:27:07.691148\t\tStopping at epoch 6, best epoch was 6 with MAP 0.5645006067914378\n",
      "=========================================================\n",
      "2020-01-10 18:27:07.691255\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8749 - dense_133_loss: 0.5416 - dense_134_loss: 0.6017 - dense_135_loss: 0.6222 - dense_136_loss: 0.6325 - dense_137_loss: 0.6299 - dense_138_loss: 0.5776 - dense_139_loss: 0.4946 - dense_140_loss: 0.4331 - dense_141_loss: 0.3844 - dense_142_loss: 0.3531 - dense_133_acc: 0.7340 - dense_134_acc: 0.6788 - dense_135_acc: 0.6560 - dense_136_acc: 0.6437 - dense_137_acc: 0.6502 - dense_138_acc: 0.6843 - dense_139_acc: 0.7305 - dense_140_acc: 0.7619 - dense_141_acc: 0.7863 - dense_142_acc: 0.7984 - val_loss: 1.8265 - val_dense_133_loss: 0.5149 - val_dense_134_loss: 0.5865 - val_dense_135_loss: 0.6099 - val_dense_136_loss: 0.6243 - val_dense_137_loss: 0.6255 - val_dense_138_loss: 0.5760 - val_dense_139_loss: 0.4928 - val_dense_140_loss: 0.4248 - val_dense_141_loss: 0.3826 - val_dense_142_loss: 0.3472 - val_dense_133_acc: 0.7519 - val_dense_134_acc: 0.6931 - val_dense_135_acc: 0.6683 - val_dense_136_acc: 0.6522 - val_dense_137_acc: 0.6551 - val_dense_138_acc: 0.6875 - val_dense_139_acc: 0.7324 - val_dense_140_acc: 0.7673 - val_dense_141_acc: 0.7907 - val_dense_142_acc: 0.8060\n",
      "2020-01-10 18:27:13.560196\t\tValid\tMAP:\t0.564890580118009\tFPA:\t0.7519414617735288\n",
      "=========================================================\n",
      "2020-01-10 18:27:13.944724\t\tStopping at epoch 7, best epoch was 7 with MAP 0.564890580118009\n",
      "=========================================================\n",
      "2020-01-10 18:27:13.945016\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8736 - dense_133_loss: 0.5424 - dense_134_loss: 0.6002 - dense_135_loss: 0.6215 - dense_136_loss: 0.6317 - dense_137_loss: 0.6293 - dense_138_loss: 0.5764 - dense_139_loss: 0.4940 - dense_140_loss: 0.4320 - dense_141_loss: 0.3834 - dense_142_loss: 0.3533 - dense_133_acc: 0.7323 - dense_134_acc: 0.6793 - dense_135_acc: 0.6554 - dense_136_acc: 0.6447 - dense_137_acc: 0.6493 - dense_138_acc: 0.6843 - dense_139_acc: 0.7304 - dense_140_acc: 0.7616 - dense_141_acc: 0.7868 - dense_142_acc: 0.7969 - val_loss: 1.8187 - val_dense_133_loss: 0.5060 - val_dense_134_loss: 0.5856 - val_dense_135_loss: 0.6115 - val_dense_136_loss: 0.6230 - val_dense_137_loss: 0.6291 - val_dense_138_loss: 0.5720 - val_dense_139_loss: 0.4929 - val_dense_140_loss: 0.4244 - val_dense_141_loss: 0.3822 - val_dense_142_loss: 0.3433 - val_dense_133_acc: 0.7630 - val_dense_134_acc: 0.6907 - val_dense_135_acc: 0.6625 - val_dense_136_acc: 0.6535 - val_dense_137_acc: 0.6550 - val_dense_138_acc: 0.6878 - val_dense_139_acc: 0.7317 - val_dense_140_acc: 0.7675 - val_dense_141_acc: 0.7895 - val_dense_142_acc: 0.8058\n",
      "2020-01-10 18:27:19.826805\t\tValid\tMAP:\t0.5646907362486154\tFPA:\t0.762987468109563\n",
      "=========================================================\n",
      "2020-01-10 18:27:19.827011\t\tStopping at epoch 8, best epoch was 7 with MAP 0.564890580118009\n",
      "=========================================================\n",
      "2020-01-10 18:27:19.827679\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8723 - dense_133_loss: 0.5417 - dense_134_loss: 0.6004 - dense_135_loss: 0.6211 - dense_136_loss: 0.6321 - dense_137_loss: 0.6285 - dense_138_loss: 0.5766 - dense_139_loss: 0.4929 - dense_140_loss: 0.4304 - dense_141_loss: 0.3834 - dense_142_loss: 0.3517 - dense_133_acc: 0.7343 - dense_134_acc: 0.6809 - dense_135_acc: 0.6557 - dense_136_acc: 0.6440 - dense_137_acc: 0.6515 - dense_138_acc: 0.6858 - dense_139_acc: 0.7313 - dense_140_acc: 0.7641 - dense_141_acc: 0.7870 - dense_142_acc: 0.7990 - val_loss: 1.8179 - val_dense_133_loss: 0.5078 - val_dense_134_loss: 0.5859 - val_dense_135_loss: 0.6096 - val_dense_136_loss: 0.6236 - val_dense_137_loss: 0.6237 - val_dense_138_loss: 0.5736 - val_dense_139_loss: 0.4904 - val_dense_140_loss: 0.4258 - val_dense_141_loss: 0.3845 - val_dense_142_loss: 0.3451 - val_dense_133_acc: 0.7601 - val_dense_134_acc: 0.6949 - val_dense_135_acc: 0.6681 - val_dense_136_acc: 0.6543 - val_dense_137_acc: 0.6586 - val_dense_138_acc: 0.6887 - val_dense_139_acc: 0.7299 - val_dense_140_acc: 0.7690 - val_dense_141_acc: 0.7921 - val_dense_142_acc: 0.8067\n",
      "2020-01-10 18:27:25.693045\t\tValid\tMAP:\t0.566552299378209\tFPA:\t0.7600998065547113\n",
      "=========================================================\n",
      "2020-01-10 18:27:26.039263\t\tStopping at epoch 9, best epoch was 9 with MAP 0.566552299378209\n",
      "=========================================================\n",
      "2020-01-10 18:27:26.040284\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8704 - dense_133_loss: 0.5408 - dense_134_loss: 0.6001 - dense_135_loss: 0.6206 - dense_136_loss: 0.6311 - dense_137_loss: 0.6277 - dense_138_loss: 0.5767 - dense_139_loss: 0.4934 - dense_140_loss: 0.4298 - dense_141_loss: 0.3837 - dense_142_loss: 0.3514 - dense_133_acc: 0.7344 - dense_134_acc: 0.6797 - dense_135_acc: 0.6577 - dense_136_acc: 0.6450 - dense_137_acc: 0.6521 - dense_138_acc: 0.6858 - dense_139_acc: 0.7303 - dense_140_acc: 0.7627 - dense_141_acc: 0.7856 - dense_142_acc: 0.7992 - val_loss: 1.8172 - val_dense_133_loss: 0.5071 - val_dense_134_loss: 0.5857 - val_dense_135_loss: 0.6094 - val_dense_136_loss: 0.6248 - val_dense_137_loss: 0.6242 - val_dense_138_loss: 0.5726 - val_dense_139_loss: 0.4932 - val_dense_140_loss: 0.4229 - val_dense_141_loss: 0.3790 - val_dense_142_loss: 0.3444 - val_dense_133_acc: 0.7586 - val_dense_134_acc: 0.6910 - val_dense_135_acc: 0.6683 - val_dense_136_acc: 0.6534 - val_dense_137_acc: 0.6581 - val_dense_138_acc: 0.6870 - val_dense_139_acc: 0.7321 - val_dense_140_acc: 0.7698 - val_dense_141_acc: 0.7912 - val_dense_142_acc: 0.8058\n",
      "2020-01-10 18:27:31.842505\t\tValid\tMAP:\t0.5647719019047627\tFPA:\t0.758585886904595\n",
      "=========================================================\n",
      "2020-01-10 18:27:31.843019\t\tStopping at epoch 10, best epoch was 9 with MAP 0.566552299378209\n",
      "=========================================================\n",
      "2020-01-10 18:27:31.843158\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8687 - dense_133_loss: 0.5405 - dense_134_loss: 0.5992 - dense_135_loss: 0.6194 - dense_136_loss: 0.6311 - dense_137_loss: 0.6284 - dense_138_loss: 0.5745 - dense_139_loss: 0.4927 - dense_140_loss: 0.4296 - dense_141_loss: 0.3819 - dense_142_loss: 0.3509 - dense_133_acc: 0.7345 - dense_134_acc: 0.6802 - dense_135_acc: 0.6584 - dense_136_acc: 0.6444 - dense_137_acc: 0.6521 - dense_138_acc: 0.6870 - dense_139_acc: 0.7307 - dense_140_acc: 0.7634 - dense_141_acc: 0.7886 - dense_142_acc: 0.7997 - val_loss: 1.8357 - val_dense_133_loss: 0.5175 - val_dense_134_loss: 0.5910 - val_dense_135_loss: 0.6173 - val_dense_136_loss: 0.6265 - val_dense_137_loss: 0.6260 - val_dense_138_loss: 0.5704 - val_dense_139_loss: 0.4927 - val_dense_140_loss: 0.4239 - val_dense_141_loss: 0.3814 - val_dense_142_loss: 0.3454 - val_dense_133_acc: 0.7537 - val_dense_134_acc: 0.6957 - val_dense_135_acc: 0.6681 - val_dense_136_acc: 0.6540 - val_dense_137_acc: 0.6574 - val_dense_138_acc: 0.6888 - val_dense_139_acc: 0.7308 - val_dense_140_acc: 0.7695 - val_dense_141_acc: 0.7916 - val_dense_142_acc: 0.8047\n",
      "2020-01-10 18:27:37.682026\t\tValid\tMAP:\t0.5663569325834487\tFPA:\t0.7536516302671787\n",
      "=========================================================\n",
      "2020-01-10 18:27:37.682249\t\tStopping at epoch 11, best epoch was 9 with MAP 0.566552299378209\n",
      "=========================================================\n",
      "2020-01-10 18:27:37.682284\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8687 - dense_133_loss: 0.5405 - dense_134_loss: 0.5987 - dense_135_loss: 0.6194 - dense_136_loss: 0.6319 - dense_137_loss: 0.6275 - dense_138_loss: 0.5752 - dense_139_loss: 0.4931 - dense_140_loss: 0.4301 - dense_141_loss: 0.3836 - dense_142_loss: 0.3518 - dense_133_acc: 0.7352 - dense_134_acc: 0.6814 - dense_135_acc: 0.6582 - dense_136_acc: 0.6445 - dense_137_acc: 0.6514 - dense_138_acc: 0.6858 - dense_139_acc: 0.7302 - dense_140_acc: 0.7636 - dense_141_acc: 0.7884 - dense_142_acc: 0.7995 - val_loss: 1.8091 - val_dense_133_loss: 0.5052 - val_dense_134_loss: 0.5823 - val_dense_135_loss: 0.6077 - val_dense_136_loss: 0.6244 - val_dense_137_loss: 0.6224 - val_dense_138_loss: 0.5684 - val_dense_139_loss: 0.4895 - val_dense_140_loss: 0.4233 - val_dense_141_loss: 0.3789 - val_dense_142_loss: 0.3430 - val_dense_133_acc: 0.7628 - val_dense_134_acc: 0.6963 - val_dense_135_acc: 0.6701 - val_dense_136_acc: 0.6533 - val_dense_137_acc: 0.6600 - val_dense_138_acc: 0.6899 - val_dense_139_acc: 0.7329 - val_dense_140_acc: 0.7695 - val_dense_141_acc: 0.7909 - val_dense_142_acc: 0.8038\n",
      "2020-01-10 18:27:43.414726\t\tValid\tMAP:\t0.5683772497291804\tFPA:\t0.7627912192660293\n",
      "=========================================================\n",
      "2020-01-10 18:27:43.507865\t\tStopping at epoch 12, best epoch was 12 with MAP 0.5683772497291804\n",
      "=========================================================\n",
      "2020-01-10 18:27:43.507950\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8659 - dense_133_loss: 0.5387 - dense_134_loss: 0.5985 - dense_135_loss: 0.6197 - dense_136_loss: 0.6310 - dense_137_loss: 0.6267 - dense_138_loss: 0.5743 - dense_139_loss: 0.4927 - dense_140_loss: 0.4288 - dense_141_loss: 0.3820 - dense_142_loss: 0.3503 - dense_133_acc: 0.7352 - dense_134_acc: 0.6812 - dense_135_acc: 0.6575 - dense_136_acc: 0.6452 - dense_137_acc: 0.6524 - dense_138_acc: 0.6860 - dense_139_acc: 0.7316 - dense_140_acc: 0.7640 - dense_141_acc: 0.7872 - dense_142_acc: 0.7995 - val_loss: 1.8145 - val_dense_133_loss: 0.5071 - val_dense_134_loss: 0.5839 - val_dense_135_loss: 0.6102 - val_dense_136_loss: 0.6238 - val_dense_137_loss: 0.6263 - val_dense_138_loss: 0.5693 - val_dense_139_loss: 0.4903 - val_dense_140_loss: 0.4227 - val_dense_141_loss: 0.3784 - val_dense_142_loss: 0.3423 - val_dense_133_acc: 0.7593 - val_dense_134_acc: 0.6969 - val_dense_135_acc: 0.6682 - val_dense_136_acc: 0.6533 - val_dense_137_acc: 0.6580 - val_dense_138_acc: 0.6891 - val_dense_139_acc: 0.7328 - val_dense_140_acc: 0.7696 - val_dense_141_acc: 0.7921 - val_dense_142_acc: 0.8048\n",
      "2020-01-10 18:27:49.284328\t\tValid\tMAP:\t0.5675151047083109\tFPA:\t0.7592867756315007\n",
      "=========================================================\n",
      "2020-01-10 18:27:49.284799\t\tStopping at epoch 13, best epoch was 12 with MAP 0.5683772497291804\n",
      "=========================================================\n",
      "2020-01-10 18:27:49.284938\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8652 - dense_133_loss: 0.5387 - dense_134_loss: 0.5978 - dense_135_loss: 0.6197 - dense_136_loss: 0.6307 - dense_137_loss: 0.6268 - dense_138_loss: 0.5750 - dense_139_loss: 0.4926 - dense_140_loss: 0.4293 - dense_141_loss: 0.3814 - dense_142_loss: 0.3503 - dense_133_acc: 0.7353 - dense_134_acc: 0.6810 - dense_135_acc: 0.6571 - dense_136_acc: 0.6449 - dense_137_acc: 0.6527 - dense_138_acc: 0.6873 - dense_139_acc: 0.7311 - dense_140_acc: 0.7637 - dense_141_acc: 0.7889 - dense_142_acc: 0.7992 - val_loss: 1.8145 - val_dense_133_loss: 0.5069 - val_dense_134_loss: 0.5832 - val_dense_135_loss: 0.6095 - val_dense_136_loss: 0.6223 - val_dense_137_loss: 0.6233 - val_dense_138_loss: 0.5712 - val_dense_139_loss: 0.4894 - val_dense_140_loss: 0.4267 - val_dense_141_loss: 0.3769 - val_dense_142_loss: 0.3437 - val_dense_133_acc: 0.7575 - val_dense_134_acc: 0.6949 - val_dense_135_acc: 0.6692 - val_dense_136_acc: 0.6527 - val_dense_137_acc: 0.6571 - val_dense_138_acc: 0.6904 - val_dense_139_acc: 0.7317 - val_dense_140_acc: 0.7690 - val_dense_141_acc: 0.7918 - val_dense_142_acc: 0.8055\n",
      "2020-01-10 18:27:55.139012\t\tValid\tMAP:\t0.5674088865316013\tFPA:\t0.7574644649415458\n",
      "=========================================================\n",
      "2020-01-10 18:27:55.139229\t\tStopping at epoch 14, best epoch was 12 with MAP 0.5683772497291804\n",
      "=========================================================\n",
      "2020-01-10 18:27:55.139262\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8653 - dense_133_loss: 0.5391 - dense_134_loss: 0.5977 - dense_135_loss: 0.6194 - dense_136_loss: 0.6310 - dense_137_loss: 0.6273 - dense_138_loss: 0.5740 - dense_139_loss: 0.4916 - dense_140_loss: 0.4291 - dense_141_loss: 0.3815 - dense_142_loss: 0.3492 - dense_133_acc: 0.7352 - dense_134_acc: 0.6825 - dense_135_acc: 0.6578 - dense_136_acc: 0.6455 - dense_137_acc: 0.6509 - dense_138_acc: 0.6873 - dense_139_acc: 0.7315 - dense_140_acc: 0.7634 - dense_141_acc: 0.7892 - dense_142_acc: 0.8001 - val_loss: 1.8170 - val_dense_133_loss: 0.5092 - val_dense_134_loss: 0.5828 - val_dense_135_loss: 0.6076 - val_dense_136_loss: 0.6251 - val_dense_137_loss: 0.6269 - val_dense_138_loss: 0.5718 - val_dense_139_loss: 0.4913 - val_dense_140_loss: 0.4239 - val_dense_141_loss: 0.3806 - val_dense_142_loss: 0.3422 - val_dense_133_acc: 0.7572 - val_dense_134_acc: 0.6955 - val_dense_135_acc: 0.6688 - val_dense_136_acc: 0.6495 - val_dense_137_acc: 0.6503 - val_dense_138_acc: 0.6852 - val_dense_139_acc: 0.7299 - val_dense_140_acc: 0.7690 - val_dense_141_acc: 0.7913 - val_dense_142_acc: 0.8054\n",
      "2020-01-10 18:28:00.919087\t\tValid\tMAP:\t0.5644767834882575\tFPA:\t0.7572121449998598\n",
      "=========================================================\n",
      "2020-01-10 18:28:00.919537\t\tStopping at epoch 15, best epoch was 12 with MAP 0.5683772497291804\n",
      "=========================================================\n",
      "2020-01-10 18:28:00.919671\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8678 - dense_133_loss: 0.5404 - dense_134_loss: 0.5991 - dense_135_loss: 0.6196 - dense_136_loss: 0.6307 - dense_137_loss: 0.6274 - dense_138_loss: 0.5747 - dense_139_loss: 0.4915 - dense_140_loss: 0.4294 - dense_141_loss: 0.3819 - dense_142_loss: 0.3502 - dense_133_acc: 0.7354 - dense_134_acc: 0.6802 - dense_135_acc: 0.6567 - dense_136_acc: 0.6462 - dense_137_acc: 0.6518 - dense_138_acc: 0.6863 - dense_139_acc: 0.7330 - dense_140_acc: 0.7638 - dense_141_acc: 0.7888 - dense_142_acc: 0.8004 - val_loss: 1.8197 - val_dense_133_loss: 0.5062 - val_dense_134_loss: 0.5838 - val_dense_135_loss: 0.6143 - val_dense_136_loss: 0.6238 - val_dense_137_loss: 0.6260 - val_dense_138_loss: 0.5790 - val_dense_139_loss: 0.4940 - val_dense_140_loss: 0.4280 - val_dense_141_loss: 0.3805 - val_dense_142_loss: 0.3416 - val_dense_133_acc: 0.7624 - val_dense_134_acc: 0.6963 - val_dense_135_acc: 0.6608 - val_dense_136_acc: 0.6553 - val_dense_137_acc: 0.6542 - val_dense_138_acc: 0.6826 - val_dense_139_acc: 0.7312 - val_dense_140_acc: 0.7653 - val_dense_141_acc: 0.7892 - val_dense_142_acc: 0.8050\n",
      "2020-01-10 18:28:06.835783\t\tValid\tMAP:\t0.5640432143177859\tFPA:\t0.7623987215789622\n",
      "=========================================================\n",
      "2020-01-10 18:28:06.835993\t\tStopping at epoch 16, best epoch was 12 with MAP 0.5683772497291804\n",
      "=========================================================\n",
      "2020-01-10 18:28:06.836603\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8670 - dense_133_loss: 0.5400 - dense_134_loss: 0.5989 - dense_135_loss: 0.6190 - dense_136_loss: 0.6308 - dense_137_loss: 0.6267 - dense_138_loss: 0.5737 - dense_139_loss: 0.4927 - dense_140_loss: 0.4299 - dense_141_loss: 0.3821 - dense_142_loss: 0.3503 - dense_133_acc: 0.7349 - dense_134_acc: 0.6816 - dense_135_acc: 0.6581 - dense_136_acc: 0.6459 - dense_137_acc: 0.6512 - dense_138_acc: 0.6865 - dense_139_acc: 0.7316 - dense_140_acc: 0.7628 - dense_141_acc: 0.7883 - dense_142_acc: 0.7993 - val_loss: 1.8122 - val_dense_133_loss: 0.5073 - val_dense_134_loss: 0.5809 - val_dense_135_loss: 0.6078 - val_dense_136_loss: 0.6221 - val_dense_137_loss: 0.6232 - val_dense_138_loss: 0.5678 - val_dense_139_loss: 0.4897 - val_dense_140_loss: 0.4231 - val_dense_141_loss: 0.3801 - val_dense_142_loss: 0.3451 - val_dense_133_acc: 0.7571 - val_dense_134_acc: 0.6978 - val_dense_135_acc: 0.6682 - val_dense_136_acc: 0.6565 - val_dense_137_acc: 0.6582 - val_dense_138_acc: 0.6903 - val_dense_139_acc: 0.7330 - val_dense_140_acc: 0.7686 - val_dense_141_acc: 0.7902 - val_dense_142_acc: 0.7984\n",
      "2020-01-10 18:28:12.566344\t\tValid\tMAP:\t0.5662234156605342\tFPA:\t0.7571280383526311\n",
      "=========================================================\n",
      "2020-01-10 18:28:14.924400\t\tAverage best MAP over all folds:\t\t0.5615129514289329...\n",
      "=========================================================\n",
      "2020-01-10 18:28:14.931303\t----- FOLD 1 -----\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8657 - dense_133_loss: 0.5398 - dense_134_loss: 0.5976 - dense_135_loss: 0.6186 - dense_136_loss: 0.6301 - dense_137_loss: 0.6276 - dense_138_loss: 0.5734 - dense_139_loss: 0.4925 - dense_140_loss: 0.4291 - dense_141_loss: 0.3811 - dense_142_loss: 0.3502 - dense_133_acc: 0.7352 - dense_134_acc: 0.6826 - dense_135_acc: 0.6599 - dense_136_acc: 0.6459 - dense_137_acc: 0.6525 - dense_138_acc: 0.6878 - dense_139_acc: 0.7315 - dense_140_acc: 0.7638 - dense_141_acc: 0.7884 - dense_142_acc: 0.8000 - val_loss: 1.7856 - val_dense_133_loss: 0.4919 - val_dense_134_loss: 0.5766 - val_dense_135_loss: 0.6026 - val_dense_136_loss: 0.6163 - val_dense_137_loss: 0.6153 - val_dense_138_loss: 0.5676 - val_dense_139_loss: 0.4875 - val_dense_140_loss: 0.4221 - val_dense_141_loss: 0.3774 - val_dense_142_loss: 0.3412 - val_dense_133_acc: 0.7757 - val_dense_134_acc: 0.7029 - val_dense_135_acc: 0.6736 - val_dense_136_acc: 0.6607 - val_dense_137_acc: 0.6629 - val_dense_138_acc: 0.6897 - val_dense_139_acc: 0.7348 - val_dense_140_acc: 0.7683 - val_dense_141_acc: 0.7918 - val_dense_142_acc: 0.8064\n",
      "2020-01-10 18:28:21.503881\t\tValid\tMAP:\t0.5756778086147438\tFPA:\t0.7756875718410945\n",
      "=========================================================\n",
      "2020-01-10 18:28:21.998240\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5756778086147438\n",
      "=========================================================\n",
      "2020-01-10 18:28:21.998510\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8637 - dense_133_loss: 0.5399 - dense_134_loss: 0.5968 - dense_135_loss: 0.6182 - dense_136_loss: 0.6290 - dense_137_loss: 0.6271 - dense_138_loss: 0.5730 - dense_139_loss: 0.4915 - dense_140_loss: 0.4281 - dense_141_loss: 0.3802 - dense_142_loss: 0.3489 - dense_133_acc: 0.7349 - dense_134_acc: 0.6827 - dense_135_acc: 0.6587 - dense_136_acc: 0.6461 - dense_137_acc: 0.6535 - dense_138_acc: 0.6884 - dense_139_acc: 0.7319 - dense_140_acc: 0.7641 - dense_141_acc: 0.7877 - dense_142_acc: 0.8005 - val_loss: 1.7855 - val_dense_133_loss: 0.4875 - val_dense_134_loss: 0.5783 - val_dense_135_loss: 0.6040 - val_dense_136_loss: 0.6192 - val_dense_137_loss: 0.6179 - val_dense_138_loss: 0.5693 - val_dense_139_loss: 0.4853 - val_dense_140_loss: 0.4202 - val_dense_141_loss: 0.3754 - val_dense_142_loss: 0.3393 - val_dense_133_acc: 0.7702 - val_dense_134_acc: 0.7003 - val_dense_135_acc: 0.6730 - val_dense_136_acc: 0.6588 - val_dense_137_acc: 0.6624 - val_dense_138_acc: 0.6888 - val_dense_139_acc: 0.7376 - val_dense_140_acc: 0.7701 - val_dense_141_acc: 0.7910 - val_dense_142_acc: 0.8080\n",
      "2020-01-10 18:28:27.770770\t\tValid\tMAP:\t0.5743551619352624\tFPA:\t0.7701926042221537\n",
      "=========================================================\n",
      "2020-01-10 18:28:27.771232\t\tStopping at epoch 1, best epoch was 0 with MAP 0.5756778086147438\n",
      "=========================================================\n",
      "2020-01-10 18:28:27.771371\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8620 - dense_133_loss: 0.5384 - dense_134_loss: 0.5965 - dense_135_loss: 0.6178 - dense_136_loss: 0.6295 - dense_137_loss: 0.6264 - dense_138_loss: 0.5725 - dense_139_loss: 0.4921 - dense_140_loss: 0.4275 - dense_141_loss: 0.3802 - dense_142_loss: 0.3490 - dense_133_acc: 0.7357 - dense_134_acc: 0.6820 - dense_135_acc: 0.6582 - dense_136_acc: 0.6460 - dense_137_acc: 0.6525 - dense_138_acc: 0.6881 - dense_139_acc: 0.7319 - dense_140_acc: 0.7640 - dense_141_acc: 0.7884 - dense_142_acc: 0.7995 - val_loss: 1.7891 - val_dense_133_loss: 0.4944 - val_dense_134_loss: 0.5764 - val_dense_135_loss: 0.6034 - val_dense_136_loss: 0.6194 - val_dense_137_loss: 0.6157 - val_dense_138_loss: 0.5658 - val_dense_139_loss: 0.4821 - val_dense_140_loss: 0.4193 - val_dense_141_loss: 0.3761 - val_dense_142_loss: 0.3401 - val_dense_133_acc: 0.7664 - val_dense_134_acc: 0.6998 - val_dense_135_acc: 0.6744 - val_dense_136_acc: 0.6617 - val_dense_137_acc: 0.6639 - val_dense_138_acc: 0.6935 - val_dense_139_acc: 0.7392 - val_dense_140_acc: 0.7701 - val_dense_141_acc: 0.7933 - val_dense_142_acc: 0.8086\n",
      "2020-01-10 18:28:33.521478\t\tValid\tMAP:\t0.5750961432323516\tFPA:\t0.7663517339987104\n",
      "=========================================================\n",
      "2020-01-10 18:28:33.522024\t\tStopping at epoch 2, best epoch was 0 with MAP 0.5756778086147438\n",
      "=========================================================\n",
      "2020-01-10 18:28:33.522196\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8616 - dense_133_loss: 0.5387 - dense_134_loss: 0.5972 - dense_135_loss: 0.6173 - dense_136_loss: 0.6282 - dense_137_loss: 0.6256 - dense_138_loss: 0.5728 - dense_139_loss: 0.4917 - dense_140_loss: 0.4273 - dense_141_loss: 0.3800 - dense_142_loss: 0.3490 - dense_133_acc: 0.7363 - dense_134_acc: 0.6812 - dense_135_acc: 0.6592 - dense_136_acc: 0.6473 - dense_137_acc: 0.6534 - dense_138_acc: 0.6887 - dense_139_acc: 0.7315 - dense_140_acc: 0.7642 - dense_141_acc: 0.7883 - dense_142_acc: 0.8003 - val_loss: 1.7927 - val_dense_133_loss: 0.4968 - val_dense_134_loss: 0.5784 - val_dense_135_loss: 0.6041 - val_dense_136_loss: 0.6195 - val_dense_137_loss: 0.6169 - val_dense_138_loss: 0.5656 - val_dense_139_loss: 0.4837 - val_dense_140_loss: 0.4193 - val_dense_141_loss: 0.3748 - val_dense_142_loss: 0.3395 - val_dense_133_acc: 0.7668 - val_dense_134_acc: 0.6984 - val_dense_135_acc: 0.6732 - val_dense_136_acc: 0.6595 - val_dense_137_acc: 0.6631 - val_dense_138_acc: 0.6920 - val_dense_139_acc: 0.7372 - val_dense_140_acc: 0.7720 - val_dense_141_acc: 0.7912 - val_dense_142_acc: 0.8074\n",
      "2020-01-10 18:28:39.283520\t\tValid\tMAP:\t0.5737194017966986\tFPA:\t0.7668283383330062\n",
      "=========================================================\n",
      "2020-01-10 18:28:39.283979\t\tStopping at epoch 3, best epoch was 0 with MAP 0.5756778086147438\n",
      "=========================================================\n",
      "2020-01-10 18:28:39.284113\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8616 - dense_133_loss: 0.5386 - dense_134_loss: 0.5968 - dense_135_loss: 0.6179 - dense_136_loss: 0.6279 - dense_137_loss: 0.6263 - dense_138_loss: 0.5723 - dense_139_loss: 0.4911 - dense_140_loss: 0.4277 - dense_141_loss: 0.3802 - dense_142_loss: 0.3492 - dense_133_acc: 0.7364 - dense_134_acc: 0.6823 - dense_135_acc: 0.6590 - dense_136_acc: 0.6483 - dense_137_acc: 0.6530 - dense_138_acc: 0.6878 - dense_139_acc: 0.7317 - dense_140_acc: 0.7638 - dense_141_acc: 0.7887 - dense_142_acc: 0.8009 - val_loss: 1.7803 - val_dense_133_loss: 0.4874 - val_dense_134_loss: 0.5752 - val_dense_135_loss: 0.6023 - val_dense_136_loss: 0.6177 - val_dense_137_loss: 0.6163 - val_dense_138_loss: 0.5661 - val_dense_139_loss: 0.4843 - val_dense_140_loss: 0.4199 - val_dense_141_loss: 0.3740 - val_dense_142_loss: 0.3393 - val_dense_133_acc: 0.7743 - val_dense_134_acc: 0.7004 - val_dense_135_acc: 0.6756 - val_dense_136_acc: 0.6593 - val_dense_137_acc: 0.6639 - val_dense_138_acc: 0.6905 - val_dense_139_acc: 0.7376 - val_dense_140_acc: 0.7715 - val_dense_141_acc: 0.7937 - val_dense_142_acc: 0.8066\n",
      "2020-01-10 18:28:45.085219\t\tValid\tMAP:\t0.5761055617332123\tFPA:\t0.7742577588382068\n",
      "=========================================================\n",
      "2020-01-10 18:28:45.431873\t\tStopping at epoch 4, best epoch was 4 with MAP 0.5761055617332123\n",
      "=========================================================\n",
      "2020-01-10 18:28:45.431938\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8609 - dense_133_loss: 0.5378 - dense_134_loss: 0.5973 - dense_135_loss: 0.6173 - dense_136_loss: 0.6288 - dense_137_loss: 0.6262 - dense_138_loss: 0.5725 - dense_139_loss: 0.4910 - dense_140_loss: 0.4275 - dense_141_loss: 0.3791 - dense_142_loss: 0.3482 - dense_133_acc: 0.7356 - dense_134_acc: 0.6814 - dense_135_acc: 0.6597 - dense_136_acc: 0.6463 - dense_137_acc: 0.6521 - dense_138_acc: 0.6875 - dense_139_acc: 0.7317 - dense_140_acc: 0.7644 - dense_141_acc: 0.7884 - dense_142_acc: 0.8002 - val_loss: 1.7889 - val_dense_133_loss: 0.4925 - val_dense_134_loss: 0.5777 - val_dense_135_loss: 0.6057 - val_dense_136_loss: 0.6195 - val_dense_137_loss: 0.6165 - val_dense_138_loss: 0.5664 - val_dense_139_loss: 0.4851 - val_dense_140_loss: 0.4191 - val_dense_141_loss: 0.3769 - val_dense_142_loss: 0.3383 - val_dense_133_acc: 0.7666 - val_dense_134_acc: 0.7003 - val_dense_135_acc: 0.6727 - val_dense_136_acc: 0.6600 - val_dense_137_acc: 0.6621 - val_dense_138_acc: 0.6912 - val_dense_139_acc: 0.7377 - val_dense_140_acc: 0.7698 - val_dense_141_acc: 0.7908 - val_dense_142_acc: 0.8098\n",
      "2020-01-10 18:28:51.207376\t\tValid\tMAP:\t0.5746921302472351\tFPA:\t0.7666320894894727\n",
      "=========================================================\n",
      "2020-01-10 18:28:51.207536\t\tStopping at epoch 5, best epoch was 4 with MAP 0.5761055617332123\n",
      "=========================================================\n",
      "2020-01-10 18:28:51.207566\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8602 - dense_133_loss: 0.5386 - dense_134_loss: 0.5966 - dense_135_loss: 0.6163 - dense_136_loss: 0.6277 - dense_137_loss: 0.6262 - dense_138_loss: 0.5713 - dense_139_loss: 0.4909 - dense_140_loss: 0.4278 - dense_141_loss: 0.3799 - dense_142_loss: 0.3487 - dense_133_acc: 0.7348 - dense_134_acc: 0.6821 - dense_135_acc: 0.6613 - dense_136_acc: 0.6479 - dense_137_acc: 0.6532 - dense_138_acc: 0.6885 - dense_139_acc: 0.7320 - dense_140_acc: 0.7646 - dense_141_acc: 0.7899 - dense_142_acc: 0.7998 - val_loss: 1.7921 - val_dense_133_loss: 0.4963 - val_dense_134_loss: 0.5778 - val_dense_135_loss: 0.6047 - val_dense_136_loss: 0.6195 - val_dense_137_loss: 0.6171 - val_dense_138_loss: 0.5668 - val_dense_139_loss: 0.4856 - val_dense_140_loss: 0.4200 - val_dense_141_loss: 0.3748 - val_dense_142_loss: 0.3394 - val_dense_133_acc: 0.7656 - val_dense_134_acc: 0.6992 - val_dense_135_acc: 0.6738 - val_dense_136_acc: 0.6596 - val_dense_137_acc: 0.6636 - val_dense_138_acc: 0.6909 - val_dense_139_acc: 0.7386 - val_dense_140_acc: 0.7695 - val_dense_141_acc: 0.7919 - val_dense_142_acc: 0.8084\n",
      "2020-01-10 18:28:57.028563\t\tValid\tMAP:\t0.5741285325100716\tFPA:\t0.7655947741736522\n",
      "=========================================================\n",
      "2020-01-10 18:28:57.029022\t\tStopping at epoch 6, best epoch was 4 with MAP 0.5761055617332123\n",
      "=========================================================\n",
      "2020-01-10 18:28:57.029156\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8612 - dense_133_loss: 0.5389 - dense_134_loss: 0.5966 - dense_135_loss: 0.6168 - dense_136_loss: 0.6285 - dense_137_loss: 0.6257 - dense_138_loss: 0.5724 - dense_139_loss: 0.4914 - dense_140_loss: 0.4270 - dense_141_loss: 0.3800 - dense_142_loss: 0.3493 - dense_133_acc: 0.7349 - dense_134_acc: 0.6817 - dense_135_acc: 0.6607 - dense_136_acc: 0.6465 - dense_137_acc: 0.6524 - dense_138_acc: 0.6889 - dense_139_acc: 0.7307 - dense_140_acc: 0.7644 - dense_141_acc: 0.7894 - dense_142_acc: 0.8006 - val_loss: 1.7833 - val_dense_133_loss: 0.4912 - val_dense_134_loss: 0.5744 - val_dense_135_loss: 0.6012 - val_dense_136_loss: 0.6174 - val_dense_137_loss: 0.6135 - val_dense_138_loss: 0.5660 - val_dense_139_loss: 0.4852 - val_dense_140_loss: 0.4184 - val_dense_141_loss: 0.3762 - val_dense_142_loss: 0.3391 - val_dense_133_acc: 0.7711 - val_dense_134_acc: 0.7014 - val_dense_135_acc: 0.6743 - val_dense_136_acc: 0.6601 - val_dense_137_acc: 0.6642 - val_dense_138_acc: 0.6913 - val_dense_139_acc: 0.7383 - val_dense_140_acc: 0.7718 - val_dense_141_acc: 0.7909 - val_dense_142_acc: 0.8072\n",
      "2020-01-10 18:29:02.774781\t\tValid\tMAP:\t0.5761681801854366\tFPA:\t0.7711177773416692\n",
      "=========================================================\n",
      "2020-01-10 18:29:03.197797\t\tStopping at epoch 7, best epoch was 7 with MAP 0.5761681801854366\n",
      "=========================================================\n",
      "2020-01-10 18:29:03.198330\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8553 - dense_133_loss: 0.5360 - dense_134_loss: 0.5947 - dense_135_loss: 0.6159 - dense_136_loss: 0.6271 - dense_137_loss: 0.6248 - dense_138_loss: 0.5706 - dense_139_loss: 0.4890 - dense_140_loss: 0.4256 - dense_141_loss: 0.3781 - dense_142_loss: 0.3471 - dense_133_acc: 0.7378 - dense_134_acc: 0.6834 - dense_135_acc: 0.6603 - dense_136_acc: 0.6482 - dense_137_acc: 0.6541 - dense_138_acc: 0.6904 - dense_139_acc: 0.7342 - dense_140_acc: 0.7672 - dense_141_acc: 0.7912 - dense_142_acc: 0.8021 - val_loss: 1.8016 - val_dense_133_loss: 0.4985 - val_dense_134_loss: 0.5832 - val_dense_135_loss: 0.6097 - val_dense_136_loss: 0.6213 - val_dense_137_loss: 0.6184 - val_dense_138_loss: 0.5693 - val_dense_139_loss: 0.4834 - val_dense_140_loss: 0.4212 - val_dense_141_loss: 0.3749 - val_dense_142_loss: 0.3397 - val_dense_133_acc: 0.7625 - val_dense_134_acc: 0.6954 - val_dense_135_acc: 0.6719 - val_dense_136_acc: 0.6590 - val_dense_137_acc: 0.6623 - val_dense_138_acc: 0.6932 - val_dense_139_acc: 0.7378 - val_dense_140_acc: 0.7711 - val_dense_141_acc: 0.7905 - val_dense_142_acc: 0.8080\n",
      "2020-01-10 18:29:08.987482\t\tValid\tMAP:\t0.5725058450481859\tFPA:\t0.7625388993243433\n",
      "=========================================================\n",
      "2020-01-10 18:29:08.988133\t\tStopping at epoch 8, best epoch was 7 with MAP 0.5761681801854366\n",
      "=========================================================\n",
      "2020-01-10 18:29:08.988621\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8555 - dense_133_loss: 0.5359 - dense_134_loss: 0.5941 - dense_135_loss: 0.6163 - dense_136_loss: 0.6278 - dense_137_loss: 0.6256 - dense_138_loss: 0.5710 - dense_139_loss: 0.4902 - dense_140_loss: 0.4263 - dense_141_loss: 0.3787 - dense_142_loss: 0.3469 - dense_133_acc: 0.7371 - dense_134_acc: 0.6845 - dense_135_acc: 0.6596 - dense_136_acc: 0.6478 - dense_137_acc: 0.6545 - dense_138_acc: 0.6892 - dense_139_acc: 0.7324 - dense_140_acc: 0.7656 - dense_141_acc: 0.7901 - dense_142_acc: 0.8020 - val_loss: 1.7917 - val_dense_133_loss: 0.4949 - val_dense_134_loss: 0.5785 - val_dense_135_loss: 0.6053 - val_dense_136_loss: 0.6187 - val_dense_137_loss: 0.6171 - val_dense_138_loss: 0.5664 - val_dense_139_loss: 0.4853 - val_dense_140_loss: 0.4220 - val_dense_141_loss: 0.3763 - val_dense_142_loss: 0.3407 - val_dense_133_acc: 0.7681 - val_dense_134_acc: 0.6980 - val_dense_135_acc: 0.6704 - val_dense_136_acc: 0.6577 - val_dense_137_acc: 0.6634 - val_dense_138_acc: 0.6896 - val_dense_139_acc: 0.7350 - val_dense_140_acc: 0.7702 - val_dense_141_acc: 0.7923 - val_dense_142_acc: 0.8072\n",
      "2020-01-10 18:29:14.796664\t\tValid\tMAP:\t0.5718762352788395\tFPA:\t0.7681179735905128\n",
      "=========================================================\n",
      "2020-01-10 18:29:14.796860\t\tStopping at epoch 9, best epoch was 7 with MAP 0.5761681801854366\n",
      "=========================================================\n",
      "2020-01-10 18:29:14.796895\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8568 - dense_133_loss: 0.5368 - dense_134_loss: 0.5954 - dense_135_loss: 0.6163 - dense_136_loss: 0.6272 - dense_137_loss: 0.6250 - dense_138_loss: 0.5712 - dense_139_loss: 0.4902 - dense_140_loss: 0.4264 - dense_141_loss: 0.3782 - dense_142_loss: 0.3467 - dense_133_acc: 0.7359 - dense_134_acc: 0.6832 - dense_135_acc: 0.6609 - dense_136_acc: 0.6494 - dense_137_acc: 0.6543 - dense_138_acc: 0.6894 - dense_139_acc: 0.7329 - dense_140_acc: 0.7659 - dense_141_acc: 0.7906 - dense_142_acc: 0.8022 - val_loss: 1.7877 - val_dense_133_loss: 0.4926 - val_dense_134_loss: 0.5775 - val_dense_135_loss: 0.6046 - val_dense_136_loss: 0.6178 - val_dense_137_loss: 0.6157 - val_dense_138_loss: 0.5645 - val_dense_139_loss: 0.4842 - val_dense_140_loss: 0.4180 - val_dense_141_loss: 0.3744 - val_dense_142_loss: 0.3369 - val_dense_133_acc: 0.7713 - val_dense_134_acc: 0.6997 - val_dense_135_acc: 0.6713 - val_dense_136_acc: 0.6609 - val_dense_137_acc: 0.6611 - val_dense_138_acc: 0.6916 - val_dense_139_acc: 0.7384 - val_dense_140_acc: 0.7716 - val_dense_141_acc: 0.7908 - val_dense_142_acc: 0.8099\n",
      "2020-01-10 18:29:20.615365\t\tValid\tMAP:\t0.5747553690098223\tFPA:\t0.7712859906361266\n",
      "=========================================================\n",
      "2020-01-10 18:29:20.615568\t\tStopping at epoch 10, best epoch was 7 with MAP 0.5761681801854366\n",
      "=========================================================\n",
      "2020-01-10 18:29:20.615600\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8589 - dense_133_loss: 0.5380 - dense_134_loss: 0.5958 - dense_135_loss: 0.6163 - dense_136_loss: 0.6274 - dense_137_loss: 0.6255 - dense_138_loss: 0.5719 - dense_139_loss: 0.4905 - dense_140_loss: 0.4279 - dense_141_loss: 0.3795 - dense_142_loss: 0.3475 - dense_133_acc: 0.7371 - dense_134_acc: 0.6829 - dense_135_acc: 0.6607 - dense_136_acc: 0.6486 - dense_137_acc: 0.6541 - dense_138_acc: 0.6892 - dense_139_acc: 0.7323 - dense_140_acc: 0.7640 - dense_141_acc: 0.7895 - dense_142_acc: 0.8010 - val_loss: 1.7893 - val_dense_133_loss: 0.4936 - val_dense_134_loss: 0.5770 - val_dense_135_loss: 0.6044 - val_dense_136_loss: 0.6182 - val_dense_137_loss: 0.6163 - val_dense_138_loss: 0.5676 - val_dense_139_loss: 0.4841 - val_dense_140_loss: 0.4187 - val_dense_141_loss: 0.3750 - val_dense_142_loss: 0.3397 - val_dense_133_acc: 0.7685 - val_dense_134_acc: 0.6998 - val_dense_135_acc: 0.6721 - val_dense_136_acc: 0.6605 - val_dense_137_acc: 0.6619 - val_dense_138_acc: 0.6900 - val_dense_139_acc: 0.7362 - val_dense_140_acc: 0.7708 - val_dense_141_acc: 0.7927 - val_dense_142_acc: 0.8071\n",
      "2020-01-10 18:29:26.500669\t\tValid\tMAP:\t0.5735815515616794\tFPA:\t0.7684544001794276\n",
      "=========================================================\n",
      "2020-01-10 18:29:26.501343\t\tStopping at epoch 11, best epoch was 7 with MAP 0.5761681801854366\n",
      "=========================================================\n",
      "2020-01-10 18:29:26.501682\t\tPredicting for test set...\n",
      "Train on 142673 samples, validate on 35669 samples\n",
      "142673/142673 - 5s - loss: 1.8575 - dense_133_loss: 0.5381 - dense_134_loss: 0.5952 - dense_135_loss: 0.6160 - dense_136_loss: 0.6270 - dense_137_loss: 0.6244 - dense_138_loss: 0.5707 - dense_139_loss: 0.4896 - dense_140_loss: 0.4254 - dense_141_loss: 0.3778 - dense_142_loss: 0.3467 - dense_133_acc: 0.7360 - dense_134_acc: 0.6840 - dense_135_acc: 0.6613 - dense_136_acc: 0.6486 - dense_137_acc: 0.6538 - dense_138_acc: 0.6881 - dense_139_acc: 0.7331 - dense_140_acc: 0.7661 - dense_141_acc: 0.7910 - dense_142_acc: 0.8021 - val_loss: 1.8018 - val_dense_133_loss: 0.4979 - val_dense_134_loss: 0.5846 - val_dense_135_loss: 0.6086 - val_dense_136_loss: 0.6240 - val_dense_137_loss: 0.6183 - val_dense_138_loss: 0.5675 - val_dense_139_loss: 0.4849 - val_dense_140_loss: 0.4190 - val_dense_141_loss: 0.3749 - val_dense_142_loss: 0.3397 - val_dense_133_acc: 0.7647 - val_dense_134_acc: 0.6940 - val_dense_135_acc: 0.6722 - val_dense_136_acc: 0.6597 - val_dense_137_acc: 0.6640 - val_dense_138_acc: 0.6939 - val_dense_139_acc: 0.7386 - val_dense_140_acc: 0.7711 - val_dense_141_acc: 0.7904 - val_dense_142_acc: 0.8099\n",
      "2020-01-10 18:29:32.334037\t\tValid\tMAP:\t0.5731741860318905\tFPA:\t0.7647256721522891\n",
      "=========================================================\n",
      "2020-01-10 18:29:34.663833\t\tAverage best MAP over all folds:\t\t0.5675030687888847...\n",
      "=========================================================\n",
      "2020-01-10 18:29:34.673491\t----- FOLD 2 -----\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8582 - dense_133_loss: 0.5368 - dense_134_loss: 0.5959 - dense_135_loss: 0.6167 - dense_136_loss: 0.6286 - dense_137_loss: 0.6257 - dense_138_loss: 0.5719 - dense_139_loss: 0.4912 - dense_140_loss: 0.4257 - dense_141_loss: 0.3789 - dense_142_loss: 0.3466 - dense_133_acc: 0.7366 - dense_134_acc: 0.6836 - dense_135_acc: 0.6592 - dense_136_acc: 0.6471 - dense_137_acc: 0.6536 - dense_138_acc: 0.6876 - dense_139_acc: 0.7323 - dense_140_acc: 0.7658 - dense_141_acc: 0.7897 - dense_142_acc: 0.8023 - val_loss: 1.7738 - val_dense_133_loss: 0.4945 - val_dense_134_loss: 0.5691 - val_dense_135_loss: 0.5954 - val_dense_136_loss: 0.6108 - val_dense_137_loss: 0.6133 - val_dense_138_loss: 0.5603 - val_dense_139_loss: 0.4783 - val_dense_140_loss: 0.4196 - val_dense_141_loss: 0.3707 - val_dense_142_loss: 0.3402 - val_dense_133_acc: 0.7712 - val_dense_134_acc: 0.7073 - val_dense_135_acc: 0.6818 - val_dense_136_acc: 0.6650 - val_dense_137_acc: 0.6645 - val_dense_138_acc: 0.6986 - val_dense_139_acc: 0.7432 - val_dense_140_acc: 0.7737 - val_dense_141_acc: 0.7978 - val_dense_142_acc: 0.8091\n",
      "2020-01-10 18:29:41.287556\t\tValid\tMAP:\t0.5810597658286287\tFPA:\t0.7711954693282494\n",
      "=========================================================\n",
      "2020-01-10 18:29:41.768418\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5810597658286287\n",
      "=========================================================\n",
      "2020-01-10 18:29:41.768481\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8543 - dense_133_loss: 0.5352 - dense_134_loss: 0.5942 - dense_135_loss: 0.6161 - dense_136_loss: 0.6272 - dense_137_loss: 0.6248 - dense_138_loss: 0.5713 - dense_139_loss: 0.4892 - dense_140_loss: 0.4249 - dense_141_loss: 0.3779 - dense_142_loss: 0.3460 - dense_133_acc: 0.7379 - dense_134_acc: 0.6834 - dense_135_acc: 0.6608 - dense_136_acc: 0.6493 - dense_137_acc: 0.6555 - dense_138_acc: 0.6878 - dense_139_acc: 0.7333 - dense_140_acc: 0.7666 - dense_141_acc: 0.7901 - dense_142_acc: 0.8021 - val_loss: 1.7727 - val_dense_133_loss: 0.4916 - val_dense_134_loss: 0.5690 - val_dense_135_loss: 0.5967 - val_dense_136_loss: 0.6118 - val_dense_137_loss: 0.6144 - val_dense_138_loss: 0.5610 - val_dense_139_loss: 0.4804 - val_dense_140_loss: 0.4194 - val_dense_141_loss: 0.3738 - val_dense_142_loss: 0.3417 - val_dense_133_acc: 0.7714 - val_dense_134_acc: 0.7045 - val_dense_135_acc: 0.6804 - val_dense_136_acc: 0.6634 - val_dense_137_acc: 0.6652 - val_dense_138_acc: 0.6984 - val_dense_139_acc: 0.7442 - val_dense_140_acc: 0.7729 - val_dense_141_acc: 0.7953 - val_dense_142_acc: 0.8049\n",
      "2020-01-10 18:29:47.547920\t\tValid\tMAP:\t0.5798214789453393\tFPA:\t0.7713636873387911\n",
      "=========================================================\n",
      "2020-01-10 18:29:47.548426\t\tStopping at epoch 1, best epoch was 0 with MAP 0.5810597658286287\n",
      "=========================================================\n",
      "2020-01-10 18:29:47.548562\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8568 - dense_133_loss: 0.5364 - dense_134_loss: 0.5960 - dense_135_loss: 0.6170 - dense_136_loss: 0.6278 - dense_137_loss: 0.6248 - dense_138_loss: 0.5704 - dense_139_loss: 0.4889 - dense_140_loss: 0.4250 - dense_141_loss: 0.3788 - dense_142_loss: 0.3463 - dense_133_acc: 0.7366 - dense_134_acc: 0.6825 - dense_135_acc: 0.6585 - dense_136_acc: 0.6475 - dense_137_acc: 0.6544 - dense_138_acc: 0.6892 - dense_139_acc: 0.7332 - dense_140_acc: 0.7663 - dense_141_acc: 0.7893 - dense_142_acc: 0.8013 - val_loss: 1.7763 - val_dense_133_loss: 0.4914 - val_dense_134_loss: 0.5710 - val_dense_135_loss: 0.5972 - val_dense_136_loss: 0.6130 - val_dense_137_loss: 0.6146 - val_dense_138_loss: 0.5623 - val_dense_139_loss: 0.4808 - val_dense_140_loss: 0.4194 - val_dense_141_loss: 0.3693 - val_dense_142_loss: 0.3384 - val_dense_133_acc: 0.7720 - val_dense_134_acc: 0.7076 - val_dense_135_acc: 0.6822 - val_dense_136_acc: 0.6632 - val_dense_137_acc: 0.6645 - val_dense_138_acc: 0.6968 - val_dense_139_acc: 0.7425 - val_dense_140_acc: 0.7712 - val_dense_141_acc: 0.7990 - val_dense_142_acc: 0.8086\n",
      "2020-01-10 18:29:53.453433\t\tValid\tMAP:\t0.5808308080411337\tFPA:\t0.7719524503756869\n",
      "=========================================================\n",
      "2020-01-10 18:29:53.453623\t\tStopping at epoch 2, best epoch was 0 with MAP 0.5810597658286287\n",
      "=========================================================\n",
      "2020-01-10 18:29:53.453658\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8534 - dense_133_loss: 0.5345 - dense_134_loss: 0.5951 - dense_135_loss: 0.6155 - dense_136_loss: 0.6269 - dense_137_loss: 0.6240 - dense_138_loss: 0.5710 - dense_139_loss: 0.4900 - dense_140_loss: 0.4255 - dense_141_loss: 0.3782 - dense_142_loss: 0.3452 - dense_133_acc: 0.7382 - dense_134_acc: 0.6833 - dense_135_acc: 0.6617 - dense_136_acc: 0.6495 - dense_137_acc: 0.6557 - dense_138_acc: 0.6883 - dense_139_acc: 0.7319 - dense_140_acc: 0.7663 - dense_141_acc: 0.7899 - dense_142_acc: 0.8025 - val_loss: 1.7723 - val_dense_133_loss: 0.4884 - val_dense_134_loss: 0.5705 - val_dense_135_loss: 0.5976 - val_dense_136_loss: 0.6127 - val_dense_137_loss: 0.6135 - val_dense_138_loss: 0.5641 - val_dense_139_loss: 0.4792 - val_dense_140_loss: 0.4187 - val_dense_141_loss: 0.3699 - val_dense_142_loss: 0.3416 - val_dense_133_acc: 0.7741 - val_dense_134_acc: 0.7053 - val_dense_135_acc: 0.6807 - val_dense_136_acc: 0.6634 - val_dense_137_acc: 0.6632 - val_dense_138_acc: 0.6943 - val_dense_139_acc: 0.7443 - val_dense_140_acc: 0.7711 - val_dense_141_acc: 0.7968 - val_dense_142_acc: 0.8065\n",
      "2020-01-10 18:29:59.219099\t\tValid\tMAP:\t0.579605272891322\tFPA:\t0.7741112481776382\n",
      "=========================================================\n",
      "2020-01-10 18:29:59.219285\t\tStopping at epoch 3, best epoch was 0 with MAP 0.5810597658286287\n",
      "=========================================================\n",
      "2020-01-10 18:29:59.219316\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8561 - dense_133_loss: 0.5362 - dense_134_loss: 0.5955 - dense_135_loss: 0.6170 - dense_136_loss: 0.6272 - dense_137_loss: 0.6243 - dense_138_loss: 0.5717 - dense_139_loss: 0.4893 - dense_140_loss: 0.4247 - dense_141_loss: 0.3778 - dense_142_loss: 0.3449 - dense_133_acc: 0.7370 - dense_134_acc: 0.6829 - dense_135_acc: 0.6596 - dense_136_acc: 0.6493 - dense_137_acc: 0.6557 - dense_138_acc: 0.6893 - dense_139_acc: 0.7315 - dense_140_acc: 0.7665 - dense_141_acc: 0.7898 - dense_142_acc: 0.8036 - val_loss: 1.7723 - val_dense_133_loss: 0.4915 - val_dense_134_loss: 0.5682 - val_dense_135_loss: 0.5969 - val_dense_136_loss: 0.6113 - val_dense_137_loss: 0.6128 - val_dense_138_loss: 0.5612 - val_dense_139_loss: 0.4793 - val_dense_140_loss: 0.4232 - val_dense_141_loss: 0.3737 - val_dense_142_loss: 0.3434 - val_dense_133_acc: 0.7726 - val_dense_134_acc: 0.7062 - val_dense_135_acc: 0.6785 - val_dense_136_acc: 0.6643 - val_dense_137_acc: 0.6648 - val_dense_138_acc: 0.6979 - val_dense_139_acc: 0.7421 - val_dense_140_acc: 0.7706 - val_dense_141_acc: 0.7972 - val_dense_142_acc: 0.8059\n",
      "2020-01-10 18:30:05.046844\t\tValid\tMAP:\t0.580221037840681\tFPA:\t0.7725692497476729\n",
      "=========================================================\n",
      "2020-01-10 18:30:05.047037\t\tStopping at epoch 4, best epoch was 0 with MAP 0.5810597658286287\n",
      "=========================================================\n",
      "2020-01-10 18:30:05.047075\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8517 - dense_133_loss: 0.5336 - dense_134_loss: 0.5948 - dense_135_loss: 0.6158 - dense_136_loss: 0.6270 - dense_137_loss: 0.6237 - dense_138_loss: 0.5700 - dense_139_loss: 0.4891 - dense_140_loss: 0.4247 - dense_141_loss: 0.3778 - dense_142_loss: 0.3446 - dense_133_acc: 0.7387 - dense_134_acc: 0.6845 - dense_135_acc: 0.6618 - dense_136_acc: 0.6495 - dense_137_acc: 0.6564 - dense_138_acc: 0.6890 - dense_139_acc: 0.7336 - dense_140_acc: 0.7666 - dense_141_acc: 0.7901 - dense_142_acc: 0.8036 - val_loss: 1.7732 - val_dense_133_loss: 0.4888 - val_dense_134_loss: 0.5719 - val_dense_135_loss: 0.5979 - val_dense_136_loss: 0.6152 - val_dense_137_loss: 0.6134 - val_dense_138_loss: 0.5636 - val_dense_139_loss: 0.4808 - val_dense_140_loss: 0.4195 - val_dense_141_loss: 0.3707 - val_dense_142_loss: 0.3392 - val_dense_133_acc: 0.7752 - val_dense_134_acc: 0.7072 - val_dense_135_acc: 0.6832 - val_dense_136_acc: 0.6623 - val_dense_137_acc: 0.6666 - val_dense_138_acc: 0.6968 - val_dense_139_acc: 0.7427 - val_dense_140_acc: 0.7707 - val_dense_141_acc: 0.7971 - val_dense_142_acc: 0.8081\n",
      "2020-01-10 18:30:10.829400\t\tValid\tMAP:\t0.581104274181649\tFPA:\t0.7751766289110688\n",
      "=========================================================\n",
      "2020-01-10 18:30:11.201614\t\tStopping at epoch 5, best epoch was 5 with MAP 0.581104274181649\n",
      "=========================================================\n",
      "2020-01-10 18:30:11.202668\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8544 - dense_133_loss: 0.5359 - dense_134_loss: 0.5951 - dense_135_loss: 0.6155 - dense_136_loss: 0.6266 - dense_137_loss: 0.6238 - dense_138_loss: 0.5707 - dense_139_loss: 0.4892 - dense_140_loss: 0.4243 - dense_141_loss: 0.3785 - dense_142_loss: 0.3456 - dense_133_acc: 0.7363 - dense_134_acc: 0.6832 - dense_135_acc: 0.6607 - dense_136_acc: 0.6503 - dense_137_acc: 0.6557 - dense_138_acc: 0.6889 - dense_139_acc: 0.7328 - dense_140_acc: 0.7664 - dense_141_acc: 0.7897 - dense_142_acc: 0.8031 - val_loss: 1.7797 - val_dense_133_loss: 0.4951 - val_dense_134_loss: 0.5695 - val_dense_135_loss: 0.5973 - val_dense_136_loss: 0.6132 - val_dense_137_loss: 0.6161 - val_dense_138_loss: 0.5635 - val_dense_139_loss: 0.4813 - val_dense_140_loss: 0.4209 - val_dense_141_loss: 0.3717 - val_dense_142_loss: 0.3421 - val_dense_133_acc: 0.7693 - val_dense_134_acc: 0.7064 - val_dense_135_acc: 0.6817 - val_dense_136_acc: 0.6628 - val_dense_137_acc: 0.6618 - val_dense_138_acc: 0.6947 - val_dense_139_acc: 0.7410 - val_dense_140_acc: 0.7720 - val_dense_141_acc: 0.7969 - val_dense_142_acc: 0.8074\n",
      "2020-01-10 18:30:16.953401\t\tValid\tMAP:\t0.5793146910724168\tFPA:\t0.7693450712122911\n",
      "=========================================================\n",
      "2020-01-10 18:30:16.953590\t\tStopping at epoch 6, best epoch was 5 with MAP 0.581104274181649\n",
      "=========================================================\n",
      "2020-01-10 18:30:16.954144\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8517 - dense_133_loss: 0.5344 - dense_134_loss: 0.5940 - dense_135_loss: 0.6153 - dense_136_loss: 0.6265 - dense_137_loss: 0.6233 - dense_138_loss: 0.5702 - dense_139_loss: 0.4890 - dense_140_loss: 0.4245 - dense_141_loss: 0.3782 - dense_142_loss: 0.3450 - dense_133_acc: 0.7375 - dense_134_acc: 0.6853 - dense_135_acc: 0.6608 - dense_136_acc: 0.6498 - dense_137_acc: 0.6562 - dense_138_acc: 0.6888 - dense_139_acc: 0.7332 - dense_140_acc: 0.7671 - dense_141_acc: 0.7906 - dense_142_acc: 0.8030 - val_loss: 1.7745 - val_dense_133_loss: 0.4926 - val_dense_134_loss: 0.5679 - val_dense_135_loss: 0.5983 - val_dense_136_loss: 0.6116 - val_dense_137_loss: 0.6131 - val_dense_138_loss: 0.5625 - val_dense_139_loss: 0.4820 - val_dense_140_loss: 0.4200 - val_dense_141_loss: 0.3718 - val_dense_142_loss: 0.3400 - val_dense_133_acc: 0.7705 - val_dense_134_acc: 0.7061 - val_dense_135_acc: 0.6808 - val_dense_136_acc: 0.6652 - val_dense_137_acc: 0.6645 - val_dense_138_acc: 0.6969 - val_dense_139_acc: 0.7417 - val_dense_140_acc: 0.7705 - val_dense_141_acc: 0.7975 - val_dense_142_acc: 0.8072\n",
      "2020-01-10 18:30:22.707351\t\tValid\tMAP:\t0.5790671608861697\tFPA:\t0.7704945609509924\n",
      "=========================================================\n",
      "2020-01-10 18:30:22.707804\t\tStopping at epoch 7, best epoch was 5 with MAP 0.581104274181649\n",
      "=========================================================\n",
      "2020-01-10 18:30:22.707940\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8511 - dense_133_loss: 0.5347 - dense_134_loss: 0.5936 - dense_135_loss: 0.6148 - dense_136_loss: 0.6260 - dense_137_loss: 0.6234 - dense_138_loss: 0.5698 - dense_139_loss: 0.4884 - dense_140_loss: 0.4240 - dense_141_loss: 0.3780 - dense_142_loss: 0.3455 - dense_133_acc: 0.7369 - dense_134_acc: 0.6847 - dense_135_acc: 0.6611 - dense_136_acc: 0.6503 - dense_137_acc: 0.6566 - dense_138_acc: 0.6898 - dense_139_acc: 0.7335 - dense_140_acc: 0.7670 - dense_141_acc: 0.7898 - dense_142_acc: 0.8021 - val_loss: 1.7710 - val_dense_133_loss: 0.4907 - val_dense_134_loss: 0.5696 - val_dense_135_loss: 0.5956 - val_dense_136_loss: 0.6114 - val_dense_137_loss: 0.6130 - val_dense_138_loss: 0.5607 - val_dense_139_loss: 0.4806 - val_dense_140_loss: 0.4196 - val_dense_141_loss: 0.3718 - val_dense_142_loss: 0.3400 - val_dense_133_acc: 0.7706 - val_dense_134_acc: 0.7067 - val_dense_135_acc: 0.6828 - val_dense_136_acc: 0.6630 - val_dense_137_acc: 0.6669 - val_dense_138_acc: 0.6988 - val_dense_139_acc: 0.7432 - val_dense_140_acc: 0.7718 - val_dense_141_acc: 0.7974 - val_dense_142_acc: 0.8066\n",
      "2020-01-10 18:30:28.488937\t\tValid\tMAP:\t0.5809094166021339\tFPA:\t0.7705786699562633\n",
      "=========================================================\n",
      "2020-01-10 18:30:28.489144\t\tStopping at epoch 8, best epoch was 5 with MAP 0.581104274181649\n",
      "=========================================================\n",
      "2020-01-10 18:30:28.489248\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8505 - dense_133_loss: 0.5342 - dense_134_loss: 0.5936 - dense_135_loss: 0.6150 - dense_136_loss: 0.6261 - dense_137_loss: 0.6232 - dense_138_loss: 0.5690 - dense_139_loss: 0.4881 - dense_140_loss: 0.4239 - dense_141_loss: 0.3778 - dense_142_loss: 0.3454 - dense_133_acc: 0.7382 - dense_134_acc: 0.6841 - dense_135_acc: 0.6623 - dense_136_acc: 0.6499 - dense_137_acc: 0.6570 - dense_138_acc: 0.6911 - dense_139_acc: 0.7344 - dense_140_acc: 0.7670 - dense_141_acc: 0.7897 - dense_142_acc: 0.8023 - val_loss: 1.7795 - val_dense_133_loss: 0.4979 - val_dense_134_loss: 0.5684 - val_dense_135_loss: 0.5975 - val_dense_136_loss: 0.6140 - val_dense_137_loss: 0.6150 - val_dense_138_loss: 0.5642 - val_dense_139_loss: 0.4801 - val_dense_140_loss: 0.4190 - val_dense_141_loss: 0.3703 - val_dense_142_loss: 0.3385 - val_dense_133_acc: 0.7714 - val_dense_134_acc: 0.7060 - val_dense_135_acc: 0.6819 - val_dense_136_acc: 0.6622 - val_dense_137_acc: 0.6622 - val_dense_138_acc: 0.6956 - val_dense_139_acc: 0.7419 - val_dense_140_acc: 0.7723 - val_dense_141_acc: 0.7983 - val_dense_142_acc: 0.8073\n",
      "2020-01-10 18:30:34.316182\t\tValid\tMAP:\t0.5788638412242783\tFPA:\t0.7713636873387911\n",
      "=========================================================\n",
      "2020-01-10 18:30:34.316391\t\tStopping at epoch 9, best epoch was 5 with MAP 0.581104274181649\n",
      "=========================================================\n",
      "2020-01-10 18:30:34.316886\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8498 - dense_133_loss: 0.5333 - dense_134_loss: 0.5935 - dense_135_loss: 0.6148 - dense_136_loss: 0.6264 - dense_137_loss: 0.6230 - dense_138_loss: 0.5694 - dense_139_loss: 0.4884 - dense_140_loss: 0.4247 - dense_141_loss: 0.3780 - dense_142_loss: 0.3450 - dense_133_acc: 0.7381 - dense_134_acc: 0.6841 - dense_135_acc: 0.6614 - dense_136_acc: 0.6491 - dense_137_acc: 0.6563 - dense_138_acc: 0.6897 - dense_139_acc: 0.7337 - dense_140_acc: 0.7664 - dense_141_acc: 0.7892 - dense_142_acc: 0.8031 - val_loss: 1.7744 - val_dense_133_loss: 0.4894 - val_dense_134_loss: 0.5675 - val_dense_135_loss: 0.5966 - val_dense_136_loss: 0.6156 - val_dense_137_loss: 0.6180 - val_dense_138_loss: 0.5612 - val_dense_139_loss: 0.4803 - val_dense_140_loss: 0.4234 - val_dense_141_loss: 0.3755 - val_dense_142_loss: 0.3468 - val_dense_133_acc: 0.7723 - val_dense_134_acc: 0.7082 - val_dense_135_acc: 0.6815 - val_dense_136_acc: 0.6613 - val_dense_137_acc: 0.6633 - val_dense_138_acc: 0.6980 - val_dense_139_acc: 0.7438 - val_dense_140_acc: 0.7689 - val_dense_141_acc: 0.7933 - val_dense_142_acc: 0.7983\n",
      "2020-01-10 18:30:40.112388\t\tValid\tMAP:\t0.5799002065539217\tFPA:\t0.7723449590669508\n",
      "=========================================================\n",
      "2020-01-10 18:30:42.464120\t\tAverage best MAP over all folds:\t\t0.5709848511520266...\n",
      "=========================================================\n",
      "2020-01-10 18:30:42.499288\t----- FOLD 3 -----\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8499 - dense_133_loss: 0.5341 - dense_134_loss: 0.5936 - dense_135_loss: 0.6143 - dense_136_loss: 0.6254 - dense_137_loss: 0.6240 - dense_138_loss: 0.5693 - dense_139_loss: 0.4872 - dense_140_loss: 0.4243 - dense_141_loss: 0.3766 - dense_142_loss: 0.3458 - dense_133_acc: 0.7378 - dense_134_acc: 0.6851 - dense_135_acc: 0.6622 - dense_136_acc: 0.6507 - dense_137_acc: 0.6552 - dense_138_acc: 0.6892 - dense_139_acc: 0.7345 - dense_140_acc: 0.7673 - dense_141_acc: 0.7903 - dense_142_acc: 0.8014 - val_loss: 1.7622 - val_dense_133_loss: 0.4806 - val_dense_134_loss: 0.5688 - val_dense_135_loss: 0.5986 - val_dense_136_loss: 0.6140 - val_dense_137_loss: 0.6127 - val_dense_138_loss: 0.5573 - val_dense_139_loss: 0.4783 - val_dense_140_loss: 0.4192 - val_dense_141_loss: 0.3694 - val_dense_142_loss: 0.3335 - val_dense_133_acc: 0.7764 - val_dense_134_acc: 0.7073 - val_dense_135_acc: 0.6782 - val_dense_136_acc: 0.6605 - val_dense_137_acc: 0.6653 - val_dense_138_acc: 0.7002 - val_dense_139_acc: 0.7448 - val_dense_140_acc: 0.7708 - val_dense_141_acc: 0.7977 - val_dense_142_acc: 0.8147\n",
      "2020-01-10 18:30:49.114807\t\tValid\tMAP:\t0.5821014172992367\tFPA:\t0.776410227655041\n",
      "=========================================================\n",
      "2020-01-10 18:30:49.653029\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5821014172992367\n",
      "=========================================================\n",
      "2020-01-10 18:30:49.653513\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8481 - dense_133_loss: 0.5333 - dense_134_loss: 0.5926 - dense_135_loss: 0.6133 - dense_136_loss: 0.6250 - dense_137_loss: 0.6231 - dense_138_loss: 0.5698 - dense_139_loss: 0.4881 - dense_140_loss: 0.4250 - dense_141_loss: 0.3774 - dense_142_loss: 0.3455 - dense_133_acc: 0.7386 - dense_134_acc: 0.6855 - dense_135_acc: 0.6642 - dense_136_acc: 0.6511 - dense_137_acc: 0.6568 - dense_138_acc: 0.6896 - dense_139_acc: 0.7337 - dense_140_acc: 0.7660 - dense_141_acc: 0.7909 - dense_142_acc: 0.8020 - val_loss: 1.7738 - val_dense_133_loss: 0.4861 - val_dense_134_loss: 0.5711 - val_dense_135_loss: 0.6044 - val_dense_136_loss: 0.6173 - val_dense_137_loss: 0.6166 - val_dense_138_loss: 0.5619 - val_dense_139_loss: 0.4808 - val_dense_140_loss: 0.4179 - val_dense_141_loss: 0.3695 - val_dense_142_loss: 0.3339 - val_dense_133_acc: 0.7772 - val_dense_134_acc: 0.7049 - val_dense_135_acc: 0.6759 - val_dense_136_acc: 0.6610 - val_dense_137_acc: 0.6617 - val_dense_138_acc: 0.6995 - val_dense_139_acc: 0.7428 - val_dense_140_acc: 0.7697 - val_dense_141_acc: 0.7962 - val_dense_142_acc: 0.8126\n",
      "2020-01-10 18:30:55.588119\t\tValid\tMAP:\t0.5802508044649702\tFPA:\t0.777223281372659\n",
      "=========================================================\n",
      "2020-01-10 18:30:55.588648\t\tStopping at epoch 1, best epoch was 0 with MAP 0.5821014172992367\n",
      "=========================================================\n",
      "2020-01-10 18:30:55.588825\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8456 - dense_133_loss: 0.5332 - dense_134_loss: 0.5913 - dense_135_loss: 0.6122 - dense_136_loss: 0.6245 - dense_137_loss: 0.6222 - dense_138_loss: 0.5683 - dense_139_loss: 0.4872 - dense_140_loss: 0.4240 - dense_141_loss: 0.3765 - dense_142_loss: 0.3448 - dense_133_acc: 0.7381 - dense_134_acc: 0.6880 - dense_135_acc: 0.6657 - dense_136_acc: 0.6513 - dense_137_acc: 0.6570 - dense_138_acc: 0.6902 - dense_139_acc: 0.7345 - dense_140_acc: 0.7669 - dense_141_acc: 0.7911 - dense_142_acc: 0.8028 - val_loss: 1.7735 - val_dense_133_loss: 0.4890 - val_dense_134_loss: 0.5720 - val_dense_135_loss: 0.6012 - val_dense_136_loss: 0.6158 - val_dense_137_loss: 0.6145 - val_dense_138_loss: 0.5590 - val_dense_139_loss: 0.4787 - val_dense_140_loss: 0.4180 - val_dense_141_loss: 0.3682 - val_dense_142_loss: 0.3333 - val_dense_133_acc: 0.7758 - val_dense_134_acc: 0.7023 - val_dense_135_acc: 0.6753 - val_dense_136_acc: 0.6573 - val_dense_137_acc: 0.6630 - val_dense_138_acc: 0.7003 - val_dense_139_acc: 0.7431 - val_dense_140_acc: 0.7703 - val_dense_141_acc: 0.7977 - val_dense_142_acc: 0.8125\n",
      "2020-01-10 18:31:01.387717\t\tValid\tMAP:\t0.5789656735075569\tFPA:\t0.7757934282830549\n",
      "=========================================================\n",
      "2020-01-10 18:31:01.387890\t\tStopping at epoch 2, best epoch was 0 with MAP 0.5821014172992367\n",
      "=========================================================\n",
      "2020-01-10 18:31:01.388401\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8498 - dense_133_loss: 0.5359 - dense_134_loss: 0.5930 - dense_135_loss: 0.6125 - dense_136_loss: 0.6251 - dense_137_loss: 0.6226 - dense_138_loss: 0.5693 - dense_139_loss: 0.4878 - dense_140_loss: 0.4236 - dense_141_loss: 0.3766 - dense_142_loss: 0.3448 - dense_133_acc: 0.7359 - dense_134_acc: 0.6854 - dense_135_acc: 0.6642 - dense_136_acc: 0.6507 - dense_137_acc: 0.6565 - dense_138_acc: 0.6892 - dense_139_acc: 0.7331 - dense_140_acc: 0.7679 - dense_141_acc: 0.7908 - dense_142_acc: 0.8026 - val_loss: 1.7746 - val_dense_133_loss: 0.4876 - val_dense_134_loss: 0.5726 - val_dense_135_loss: 0.6021 - val_dense_136_loss: 0.6160 - val_dense_137_loss: 0.6152 - val_dense_138_loss: 0.5586 - val_dense_139_loss: 0.4792 - val_dense_140_loss: 0.4190 - val_dense_141_loss: 0.3698 - val_dense_142_loss: 0.3340 - val_dense_133_acc: 0.7735 - val_dense_134_acc: 0.7040 - val_dense_135_acc: 0.6750 - val_dense_136_acc: 0.6609 - val_dense_137_acc: 0.6626 - val_dense_138_acc: 0.6999 - val_dense_139_acc: 0.7439 - val_dense_140_acc: 0.7700 - val_dense_141_acc: 0.7952 - val_dense_142_acc: 0.8134\n",
      "2020-01-10 18:31:07.262984\t\tValid\tMAP:\t0.5791221593952783\tFPA:\t0.7735224851407424\n",
      "=========================================================\n",
      "2020-01-10 18:31:07.263167\t\tStopping at epoch 3, best epoch was 0 with MAP 0.5821014172992367\n",
      "=========================================================\n",
      "2020-01-10 18:31:07.263632\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8466 - dense_133_loss: 0.5339 - dense_134_loss: 0.5917 - dense_135_loss: 0.6123 - dense_136_loss: 0.6241 - dense_137_loss: 0.6220 - dense_138_loss: 0.5689 - dense_139_loss: 0.4874 - dense_140_loss: 0.4235 - dense_141_loss: 0.3769 - dense_142_loss: 0.3453 - dense_133_acc: 0.7369 - dense_134_acc: 0.6862 - dense_135_acc: 0.6636 - dense_136_acc: 0.6513 - dense_137_acc: 0.6572 - dense_138_acc: 0.6900 - dense_139_acc: 0.7349 - dense_140_acc: 0.7679 - dense_141_acc: 0.7910 - dense_142_acc: 0.8014 - val_loss: 1.7752 - val_dense_133_loss: 0.4876 - val_dense_134_loss: 0.5719 - val_dense_135_loss: 0.6027 - val_dense_136_loss: 0.6167 - val_dense_137_loss: 0.6153 - val_dense_138_loss: 0.5612 - val_dense_139_loss: 0.4814 - val_dense_140_loss: 0.4184 - val_dense_141_loss: 0.3689 - val_dense_142_loss: 0.3329 - val_dense_133_acc: 0.7734 - val_dense_134_acc: 0.7063 - val_dense_135_acc: 0.6762 - val_dense_136_acc: 0.6607 - val_dense_137_acc: 0.6640 - val_dense_138_acc: 0.6987 - val_dense_139_acc: 0.7441 - val_dense_140_acc: 0.7707 - val_dense_141_acc: 0.7968 - val_dense_142_acc: 0.8136\n",
      "2020-01-10 18:31:13.086087\t\tValid\tMAP:\t0.5790843136099283\tFPA:\t0.7733542671302007\n",
      "=========================================================\n",
      "2020-01-10 18:31:13.086633\t\tStopping at epoch 4, best epoch was 0 with MAP 0.5821014172992367\n",
      "=========================================================\n",
      "2020-01-10 18:31:13.086683\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8479 - dense_133_loss: 0.5337 - dense_134_loss: 0.5928 - dense_135_loss: 0.6128 - dense_136_loss: 0.6251 - dense_137_loss: 0.6233 - dense_138_loss: 0.5695 - dense_139_loss: 0.4867 - dense_140_loss: 0.4235 - dense_141_loss: 0.3771 - dense_142_loss: 0.3451 - dense_133_acc: 0.7375 - dense_134_acc: 0.6853 - dense_135_acc: 0.6644 - dense_136_acc: 0.6505 - dense_137_acc: 0.6556 - dense_138_acc: 0.6889 - dense_139_acc: 0.7331 - dense_140_acc: 0.7674 - dense_141_acc: 0.7904 - dense_142_acc: 0.8019 - val_loss: 1.7651 - val_dense_133_loss: 0.4822 - val_dense_134_loss: 0.5694 - val_dense_135_loss: 0.6003 - val_dense_136_loss: 0.6140 - val_dense_137_loss: 0.6133 - val_dense_138_loss: 0.5623 - val_dense_139_loss: 0.4825 - val_dense_140_loss: 0.4197 - val_dense_141_loss: 0.3732 - val_dense_142_loss: 0.3374 - val_dense_133_acc: 0.7814 - val_dense_134_acc: 0.7086 - val_dense_135_acc: 0.6787 - val_dense_136_acc: 0.6620 - val_dense_137_acc: 0.6636 - val_dense_138_acc: 0.6976 - val_dense_139_acc: 0.7401 - val_dense_140_acc: 0.7702 - val_dense_141_acc: 0.7956 - val_dense_142_acc: 0.8120\n",
      "2020-01-10 18:31:18.972319\t\tValid\tMAP:\t0.5813509764253957\tFPA:\t0.7814287316362005\n",
      "=========================================================\n",
      "2020-01-10 18:31:21.269027\t\tAverage best MAP over all folds:\t\t0.5722480973051186...\n",
      "=========================================================\n",
      "2020-01-10 18:31:21.307294\t----- FOLD 4 -----\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8453 - dense_133_loss: 0.5326 - dense_134_loss: 0.5918 - dense_135_loss: 0.6124 - dense_136_loss: 0.6250 - dense_137_loss: 0.6218 - dense_138_loss: 0.5682 - dense_139_loss: 0.4872 - dense_140_loss: 0.4232 - dense_141_loss: 0.3764 - dense_142_loss: 0.3440 - dense_133_acc: 0.7385 - dense_134_acc: 0.6863 - dense_135_acc: 0.6635 - dense_136_acc: 0.6509 - dense_137_acc: 0.6573 - dense_138_acc: 0.6910 - dense_139_acc: 0.7360 - dense_140_acc: 0.7682 - dense_141_acc: 0.7917 - dense_142_acc: 0.8036 - val_loss: 1.7742 - val_dense_133_loss: 0.4910 - val_dense_134_loss: 0.5748 - val_dense_135_loss: 0.6000 - val_dense_136_loss: 0.6115 - val_dense_137_loss: 0.6102 - val_dense_138_loss: 0.5593 - val_dense_139_loss: 0.4786 - val_dense_140_loss: 0.4179 - val_dense_141_loss: 0.3684 - val_dense_142_loss: 0.3379 - val_dense_133_acc: 0.7734 - val_dense_134_acc: 0.7021 - val_dense_135_acc: 0.6777 - val_dense_136_acc: 0.6649 - val_dense_137_acc: 0.6688 - val_dense_138_acc: 0.6991 - val_dense_139_acc: 0.7406 - val_dense_140_acc: 0.7730 - val_dense_141_acc: 0.7987 - val_dense_142_acc: 0.8077\n",
      "2020-01-10 18:31:27.870432\t\tValid\tMAP:\t0.5803411281911839\tFPA:\t0.7734383761354716\n",
      "=========================================================\n",
      "2020-01-10 18:31:28.304632\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5803411281911839\n",
      "=========================================================\n",
      "2020-01-10 18:31:28.304786\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8401 - dense_133_loss: 0.5300 - dense_134_loss: 0.5904 - dense_135_loss: 0.6111 - dense_136_loss: 0.6230 - dense_137_loss: 0.6212 - dense_138_loss: 0.5680 - dense_139_loss: 0.4870 - dense_140_loss: 0.4227 - dense_141_loss: 0.3758 - dense_142_loss: 0.3433 - dense_133_acc: 0.7403 - dense_134_acc: 0.6864 - dense_135_acc: 0.6660 - dense_136_acc: 0.6527 - dense_137_acc: 0.6575 - dense_138_acc: 0.6903 - dense_139_acc: 0.7357 - dense_140_acc: 0.7682 - dense_141_acc: 0.7918 - dense_142_acc: 0.8048 - val_loss: 1.7833 - val_dense_133_loss: 0.4895 - val_dense_134_loss: 0.5763 - val_dense_135_loss: 0.6053 - val_dense_136_loss: 0.6183 - val_dense_137_loss: 0.6165 - val_dense_138_loss: 0.5621 - val_dense_139_loss: 0.4819 - val_dense_140_loss: 0.4181 - val_dense_141_loss: 0.3707 - val_dense_142_loss: 0.3364 - val_dense_133_acc: 0.7736 - val_dense_134_acc: 0.6986 - val_dense_135_acc: 0.6688 - val_dense_136_acc: 0.6545 - val_dense_137_acc: 0.6636 - val_dense_138_acc: 0.6978 - val_dense_139_acc: 0.7386 - val_dense_140_acc: 0.7725 - val_dense_141_acc: 0.7975 - val_dense_142_acc: 0.8084\n",
      "2020-01-10 18:31:34.265291\t\tValid\tMAP:\t0.5748451259366374\tFPA:\t0.773578557810923\n",
      "=========================================================\n",
      "2020-01-10 18:31:34.265465\t\tStopping at epoch 1, best epoch was 0 with MAP 0.5803411281911839\n",
      "=========================================================\n",
      "2020-01-10 18:31:34.265500\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8405 - dense_133_loss: 0.5301 - dense_134_loss: 0.5902 - dense_135_loss: 0.6113 - dense_136_loss: 0.6238 - dense_137_loss: 0.6216 - dense_138_loss: 0.5680 - dense_139_loss: 0.4863 - dense_140_loss: 0.4230 - dense_141_loss: 0.3757 - dense_142_loss: 0.3434 - dense_133_acc: 0.7407 - dense_134_acc: 0.6872 - dense_135_acc: 0.6652 - dense_136_acc: 0.6520 - dense_137_acc: 0.6570 - dense_138_acc: 0.6912 - dense_139_acc: 0.7352 - dense_140_acc: 0.7676 - dense_141_acc: 0.7916 - dense_142_acc: 0.8042 - val_loss: 1.7686 - val_dense_133_loss: 0.4850 - val_dense_134_loss: 0.5705 - val_dense_135_loss: 0.5993 - val_dense_136_loss: 0.6145 - val_dense_137_loss: 0.6130 - val_dense_138_loss: 0.5605 - val_dense_139_loss: 0.4799 - val_dense_140_loss: 0.4197 - val_dense_141_loss: 0.3702 - val_dense_142_loss: 0.3404 - val_dense_133_acc: 0.7767 - val_dense_134_acc: 0.7048 - val_dense_135_acc: 0.6774 - val_dense_136_acc: 0.6648 - val_dense_137_acc: 0.6668 - val_dense_138_acc: 0.6989 - val_dense_139_acc: 0.7395 - val_dense_140_acc: 0.7721 - val_dense_141_acc: 0.7962 - val_dense_142_acc: 0.8097\n",
      "2020-01-10 18:31:40.175554\t\tValid\tMAP:\t0.5807425430633052\tFPA:\t0.776718627341034\n",
      "=========================================================\n",
      "2020-01-10 18:31:40.524995\t\tStopping at epoch 2, best epoch was 2 with MAP 0.5807425430633052\n",
      "=========================================================\n",
      "2020-01-10 18:31:40.525151\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8387 - dense_133_loss: 0.5300 - dense_134_loss: 0.5889 - dense_135_loss: 0.6101 - dense_136_loss: 0.6227 - dense_137_loss: 0.6209 - dense_138_loss: 0.5676 - dense_139_loss: 0.4859 - dense_140_loss: 0.4229 - dense_141_loss: 0.3755 - dense_142_loss: 0.3428 - dense_133_acc: 0.7412 - dense_134_acc: 0.6885 - dense_135_acc: 0.6673 - dense_136_acc: 0.6523 - dense_137_acc: 0.6574 - dense_138_acc: 0.6901 - dense_139_acc: 0.7365 - dense_140_acc: 0.7669 - dense_141_acc: 0.7917 - dense_142_acc: 0.8053 - val_loss: 1.7788 - val_dense_133_loss: 0.4909 - val_dense_134_loss: 0.5747 - val_dense_135_loss: 0.6042 - val_dense_136_loss: 0.6144 - val_dense_137_loss: 0.6132 - val_dense_138_loss: 0.5607 - val_dense_139_loss: 0.4795 - val_dense_140_loss: 0.4167 - val_dense_141_loss: 0.3687 - val_dense_142_loss: 0.3373 - val_dense_133_acc: 0.7697 - val_dense_134_acc: 0.7010 - val_dense_135_acc: 0.6725 - val_dense_136_acc: 0.6626 - val_dense_137_acc: 0.6663 - val_dense_138_acc: 0.6980 - val_dense_139_acc: 0.7390 - val_dense_140_acc: 0.7714 - val_dense_141_acc: 0.7965 - val_dense_142_acc: 0.8101\n",
      "2020-01-10 18:31:46.335982\t\tValid\tMAP:\t0.5774193980928216\tFPA:\t0.7697095435684648\n",
      "=========================================================\n",
      "2020-01-10 18:31:46.336186\t\tStopping at epoch 3, best epoch was 2 with MAP 0.5807425430633052\n",
      "=========================================================\n",
      "2020-01-10 18:31:46.336878\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8392 - dense_133_loss: 0.5302 - dense_134_loss: 0.5897 - dense_135_loss: 0.6108 - dense_136_loss: 0.6235 - dense_137_loss: 0.6210 - dense_138_loss: 0.5667 - dense_139_loss: 0.4851 - dense_140_loss: 0.4220 - dense_141_loss: 0.3751 - dense_142_loss: 0.3422 - dense_133_acc: 0.7407 - dense_134_acc: 0.6873 - dense_135_acc: 0.6654 - dense_136_acc: 0.6518 - dense_137_acc: 0.6582 - dense_138_acc: 0.6922 - dense_139_acc: 0.7361 - dense_140_acc: 0.7688 - dense_141_acc: 0.7919 - dense_142_acc: 0.8064 - val_loss: 1.7795 - val_dense_133_loss: 0.4942 - val_dense_134_loss: 0.5747 - val_dense_135_loss: 0.6006 - val_dense_136_loss: 0.6146 - val_dense_137_loss: 0.6112 - val_dense_138_loss: 0.5592 - val_dense_139_loss: 0.4787 - val_dense_140_loss: 0.4173 - val_dense_141_loss: 0.3675 - val_dense_142_loss: 0.3365 - val_dense_133_acc: 0.7716 - val_dense_134_acc: 0.7029 - val_dense_135_acc: 0.6755 - val_dense_136_acc: 0.6651 - val_dense_137_acc: 0.6683 - val_dense_138_acc: 0.6983 - val_dense_139_acc: 0.7403 - val_dense_140_acc: 0.7720 - val_dense_141_acc: 0.7971 - val_dense_142_acc: 0.8096\n",
      "2020-01-10 18:31:52.232853\t\tValid\tMAP:\t0.5787236627584577\tFPA:\t0.771559941684423\n",
      "=========================================================\n",
      "2020-01-10 18:31:52.233505\t\tStopping at epoch 4, best epoch was 2 with MAP 0.5807425430633052\n",
      "=========================================================\n",
      "2020-01-10 18:31:52.233595\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8392 - dense_133_loss: 0.5298 - dense_134_loss: 0.5895 - dense_135_loss: 0.6108 - dense_136_loss: 0.6236 - dense_137_loss: 0.6212 - dense_138_loss: 0.5674 - dense_139_loss: 0.4862 - dense_140_loss: 0.4225 - dense_141_loss: 0.3752 - dense_142_loss: 0.3429 - dense_133_acc: 0.7404 - dense_134_acc: 0.6881 - dense_135_acc: 0.6658 - dense_136_acc: 0.6518 - dense_137_acc: 0.6584 - dense_138_acc: 0.6906 - dense_139_acc: 0.7355 - dense_140_acc: 0.7689 - dense_141_acc: 0.7922 - dense_142_acc: 0.8047 - val_loss: 1.7742 - val_dense_133_loss: 0.4890 - val_dense_134_loss: 0.5731 - val_dense_135_loss: 0.5999 - val_dense_136_loss: 0.6157 - val_dense_137_loss: 0.6119 - val_dense_138_loss: 0.5597 - val_dense_139_loss: 0.4769 - val_dense_140_loss: 0.4161 - val_dense_141_loss: 0.3698 - val_dense_142_loss: 0.3384 - val_dense_133_acc: 0.7750 - val_dense_134_acc: 0.7040 - val_dense_135_acc: 0.6770 - val_dense_136_acc: 0.6619 - val_dense_137_acc: 0.6672 - val_dense_138_acc: 0.6989 - val_dense_139_acc: 0.7416 - val_dense_140_acc: 0.7743 - val_dense_141_acc: 0.7965 - val_dense_142_acc: 0.8106\n",
      "2020-01-10 18:31:58.123990\t\tValid\tMAP:\t0.5799330820309149\tFPA:\t0.7749803745654368\n",
      "=========================================================\n",
      "2020-01-10 18:31:58.124567\t\tStopping at epoch 5, best epoch was 2 with MAP 0.5807425430633052\n",
      "=========================================================\n",
      "2020-01-10 18:31:58.124909\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8383 - dense_133_loss: 0.5298 - dense_134_loss: 0.5888 - dense_135_loss: 0.6106 - dense_136_loss: 0.6227 - dense_137_loss: 0.6205 - dense_138_loss: 0.5680 - dense_139_loss: 0.4868 - dense_140_loss: 0.4226 - dense_141_loss: 0.3757 - dense_142_loss: 0.3432 - dense_133_acc: 0.7412 - dense_134_acc: 0.6888 - dense_135_acc: 0.6664 - dense_136_acc: 0.6537 - dense_137_acc: 0.6584 - dense_138_acc: 0.6924 - dense_139_acc: 0.7362 - dense_140_acc: 0.7687 - dense_141_acc: 0.7917 - dense_142_acc: 0.8052 - val_loss: 1.7748 - val_dense_133_loss: 0.4906 - val_dense_134_loss: 0.5726 - val_dense_135_loss: 0.5995 - val_dense_136_loss: 0.6138 - val_dense_137_loss: 0.6116 - val_dense_138_loss: 0.5586 - val_dense_139_loss: 0.4786 - val_dense_140_loss: 0.4168 - val_dense_141_loss: 0.3700 - val_dense_142_loss: 0.3377 - val_dense_133_acc: 0.7757 - val_dense_134_acc: 0.7032 - val_dense_135_acc: 0.6754 - val_dense_136_acc: 0.6624 - val_dense_137_acc: 0.6658 - val_dense_138_acc: 0.6984 - val_dense_139_acc: 0.7401 - val_dense_140_acc: 0.7733 - val_dense_141_acc: 0.7968 - val_dense_142_acc: 0.8086\n",
      "2020-01-10 18:32:04.019748\t\tValid\tMAP:\t0.5797008887980755\tFPA:\t0.7757373556128743\n",
      "=========================================================\n",
      "2020-01-10 18:32:04.019911\t\tStopping at epoch 6, best epoch was 2 with MAP 0.5807425430633052\n",
      "=========================================================\n",
      "2020-01-10 18:32:04.019943\t\tPredicting for test set...\n",
      "Train on 142674 samples, validate on 35668 samples\n",
      "142674/142674 - 5s - loss: 1.8390 - dense_133_loss: 0.5297 - dense_134_loss: 0.5900 - dense_135_loss: 0.6103 - dense_136_loss: 0.6229 - dense_137_loss: 0.6208 - dense_138_loss: 0.5681 - dense_139_loss: 0.4855 - dense_140_loss: 0.4223 - dense_141_loss: 0.3756 - dense_142_loss: 0.3428 - dense_133_acc: 0.7403 - dense_134_acc: 0.6873 - dense_135_acc: 0.6674 - dense_136_acc: 0.6527 - dense_137_acc: 0.6592 - dense_138_acc: 0.6911 - dense_139_acc: 0.7362 - dense_140_acc: 0.7683 - dense_141_acc: 0.7921 - dense_142_acc: 0.8058 - val_loss: 1.7844 - val_dense_133_loss: 0.4955 - val_dense_134_loss: 0.5744 - val_dense_135_loss: 0.6015 - val_dense_136_loss: 0.6154 - val_dense_137_loss: 0.6150 - val_dense_138_loss: 0.5627 - val_dense_139_loss: 0.4818 - val_dense_140_loss: 0.4203 - val_dense_141_loss: 0.3707 - val_dense_142_loss: 0.3378 - val_dense_133_acc: 0.7736 - val_dense_134_acc: 0.7018 - val_dense_135_acc: 0.6750 - val_dense_136_acc: 0.6665 - val_dense_137_acc: 0.6661 - val_dense_138_acc: 0.6993 - val_dense_139_acc: 0.7401 - val_dense_140_acc: 0.7722 - val_dense_141_acc: 0.7967 - val_dense_142_acc: 0.8099\n",
      "2020-01-10 18:32:09.957914\t\tValid\tMAP:\t0.5797809621542753\tFPA:\t0.7735505214758327\n",
      "=========================================================\n",
      "2020-01-10 18:32:12.334908\t\tAverage best MAP over all folds:\t\t0.5733982598651788...\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, utils, Model, Input\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "\n",
    "# k-fold Cross-validation grouped on sessions\n",
    "k = 5\n",
    "n_epochs = 50\n",
    "test_predictions = []\n",
    "all_maps = []\n",
    "\n",
    "# Generate model\n",
    "model = generate_model(train_history, train_future)\n",
    "plot_model(model)\n",
    "for fold_id, (train_idx, valid_idx) in enumerate(KFold(n_splits = k).split(train_history)):\n",
    "    print('{0}\\t----- FOLD {1} -----'.format(datetime.datetime.now(),fold_id))\n",
    "    # Filter out training and testing data\n",
    "    h_train = train_history[train_idx]\n",
    "    h_valid = train_history[valid_idx]\n",
    "    f_train = train_future[train_idx]\n",
    "    f_valid = train_future[valid_idx]\n",
    "    l_train = train_labels[train_idx]\n",
    "    l_valid = train_labels[valid_idx]\n",
    "    s_train = train_session_len[train_idx]\n",
    "    s_valid = train_session_len[valid_idx]\n",
    "\n",
    "    # Loss weights\n",
    "    weights = np.asarray([(1 / val + sum((1 / (2*n)) for n in range(val + 1,11))) for val in range(1,11)])\n",
    "    weights /= weights.max()\n",
    "    \n",
    "    # Early stopping\n",
    "    best_map = .0\n",
    "    best_weights = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    # For every epoch\n",
    "    for epoch_id in range(n_epochs):\n",
    "        model.fit([h_train, f_train], [l_train[:,i] for i in range(10)],\n",
    "                  validation_data = ([h_valid, f_valid], [l_valid[:,i] for i in range(10)]),\n",
    "                  batch_size = 2048, epochs = 1, verbose = 2)\n",
    "        p_valid = model.predict([h_valid, f_valid], batch_size = 4096)\n",
    "        MAP, FPA = evaluation_MAP_FPA(s_valid, l_valid, np.swapaxes(np.round(p_valid),0,1))\n",
    "        print('{0}\\t\\tValid\\tMAP:\\t{1}\\tFPA:\\t{2}'.format(datetime.datetime.now(),MAP, FPA))\n",
    "        if MAP > best_map:\n",
    "            best_map = MAP\n",
    "            best_epoch = epoch_id\n",
    "            best_weights = model.get_weights()\n",
    "            model.save_weights(modelpath + '/model_weights_epoch_{}.h5'.format(epoch_id+1))\n",
    "        elif epoch_id - best_epoch >= 5:\n",
    "            break\n",
    "\n",
    "        print('=========================================================')\n",
    "        print('{0}\\t\\tStopping at epoch {1}, best epoch was {2} with MAP {3}'.format(datetime.datetime.now(),epoch_id, best_epoch, best_map))\n",
    "        print('=========================================================')\n",
    "        all_maps.append(best_map)\n",
    "\n",
    "        print('{0}\\t\\tPredicting for test set...'.format(datetime.datetime.now()))\n",
    "        # Reload best weights\n",
    "        model.set_weights(best_weights)\n",
    "\n",
    "    # Predict for test set\n",
    "    p_test = model.predict([test_history, test_future], batch_size = 4096)\n",
    "    test_predictions.append(np.swapaxes(p_test,0,1))\n",
    "\n",
    "    print('=========================================================')\n",
    "    print('{0}\\t\\tAverage best MAP over all folds:\\t\\t{1}...'.format(datetime.datetime.now(), np.mean(all_maps)))\n",
    "    print('=========================================================')\n",
    "    #print('{0}\\t\\tGenerating submission...'.format(datetime.datetime.now()))\n",
    "    # Geometric mean of predictions over folds\n",
    "    p_test = np.prod(test_predictions, axis = 0) ** (1.0 / len(test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srPpO1_EjWFp"
   },
   "source": [
    "# Test set Evaluation\n",
    "\n",
    "According to the 5-fold cross validation, we have found that the best epochs are 14.\n",
    "\n",
    "We go to load the model saved at these epochs and perform the evaluation on the test set_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "urujZthnSAxe"
   },
   "outputs": [],
   "source": [
    "#Import the model with the highest performances\n",
    "best_epochs = 14\n",
    "\n",
    "weights = np.asarray([(1 / val + sum((1 / (2*n)) for n in range(val + 1,11))) for val in range(1,11)])\n",
    "weights /= weights.max()\n",
    "\n",
    "new_model = generate_model(test_history, test_future)\n",
    "new_model.load_weights(modelpath + '/model_weights_epoch_{}.h5'.format(best_epochs))\n",
    "new_model.compile(loss='binary_crossentropy',\n",
    "                                   optimizer = tf.keras.optimizers.Adam(lr = 0.002, amsgrad = True),\n",
    "                                   metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iezMnfAHSAxi"
   },
   "outputs": [],
   "source": [
    "prediction = new_model.predict([test_history, test_future], batch_size = 4096)\n",
    "MAP, FPA = evaluation_MAP_FPA(train_session_len, train_labels, np.swapaxes(np.round(prediction),0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-Y2lIsocSAxt",
    "outputId": "1bb3edd7-9921-4d3c-ba5b-8d158585b0e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performances--> MAP: 0.5793433557729396 | FPA: 0.7734857745231073\n"
     ]
    }
   ],
   "source": [
    "print(\"Performances--> MAP: {} | FPA: {}\".format(MAP,FPA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5zH-UFaASb_t"
   },
   "source": [
    "### Not bad!\n",
    "\n",
    "With only few samples from the dataset and a good model we are able to perform a score similar to **~10th position** of the rank in the global Leaderboard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_yPCLBcESqGe"
   },
   "source": [
    "# 6- Feature engineered dataset: New training & predictions\n",
    "\n",
    "# First see Notebook 4\n",
    "\n",
    "We hereby show the results over a feature-engineered dataset (Notebook of the preprocessing over Features)\n",
    "\n",
    "We can observe that the dataset we employed is quite small (applying the feature engineering function required too much time, so we used a subset of the training samples), hence different results are expected and not comparable over the ones obtained before. \n",
    "\n",
    "Recurrent networks have lots of parameters due to the LSTM gates and layers created by the sequences, that is expecte that the performance of the model won't be as good as the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FKiDHZbKStix"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# save np.load\n",
    "np_load_old = np.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LT0WlCyMaisW"
   },
   "outputs": [],
   "source": [
    "np.load = np_load_old\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "\n",
    "restored = np.load(processedpath + \"/trainfeatures.npz\")\n",
    "train_history = restored['history_train']\n",
    "train_future = restored['future_train']\n",
    "train_labels = restored['labels_train']\n",
    "train_session_len = restored['session_len_train']\n",
    "\n",
    "del restored\n",
    "\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LgsNBhenakEa"
   },
   "outputs": [],
   "source": [
    "np.load = np_load_old\n",
    "import numpy as np\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "restored2 = np.load(processedpath + \"/testfeatures.npz\")\n",
    "test_history = restored2['history_test']\n",
    "test_future = restored2['future_test']\n",
    "test_session_len = restored2['session_len_test']\n",
    "\n",
    "del restored2\n",
    "\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "kSIRrwJwal0N",
    "outputId": "d58cf0cc-bb7d-4c14-b989-4f72e1037122"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-12 15:41:18.758594\t----- FOLD 0 -----\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 19s - loss: 2.9475 - dense_315_loss: 0.9729 - dense_316_loss: 0.7953 - dense_317_loss: 1.0846 - dense_318_loss: 0.8277 - dense_319_loss: 0.8086 - dense_320_loss: 0.7448 - dense_321_loss: 0.8592 - dense_322_loss: 0.6929 - dense_323_loss: 0.6997 - dense_324_loss: 0.6497 - dense_315_accuracy: 0.5054 - dense_316_accuracy: 0.5122 - dense_317_accuracy: 0.5136 - dense_318_accuracy: 0.5299 - dense_319_accuracy: 0.5426 - dense_320_accuracy: 0.5857 - dense_321_accuracy: 0.6088 - dense_322_accuracy: 0.6288 - dense_323_accuracy: 0.6078 - dense_324_accuracy: 0.6348 - val_loss: 2.5052 - val_dense_315_loss: 0.7437 - val_dense_316_loss: 0.6971 - val_dense_317_loss: 0.7591 - val_dense_318_loss: 0.7640 - val_dense_319_loss: 0.7225 - val_dense_320_loss: 0.7183 - val_dense_321_loss: 0.9924 - val_dense_322_loss: 1.0186 - val_dense_323_loss: 0.8600 - val_dense_324_loss: 0.7943 - val_dense_315_accuracy: 0.5462 - val_dense_316_accuracy: 0.5529 - val_dense_317_accuracy: 0.4919 - val_dense_318_accuracy: 0.4751 - val_dense_319_accuracy: 0.5076 - val_dense_320_accuracy: 0.5255 - val_dense_321_accuracy: 0.4605 - val_dense_322_accuracy: 0.4180 - val_dense_323_accuracy: 0.4578 - val_dense_324_accuracy: 0.5277\n",
      "2020-01-12 15:41:38.426120\t\tValid\tMAP:\t0.3533971213565736\tFPA:\t0.5461667599328484\n",
      "=========================================================\n",
      "2020-01-12 15:41:38.826394\t\tStopping at epoch 0, best epoch was 0 with MAP 0.3533971213565736\n",
      "=========================================================\n",
      "2020-01-12 15:41:38.827158\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 2.3869 - dense_315_loss: 0.7728 - dense_316_loss: 0.7051 - dense_317_loss: 0.7850 - dense_318_loss: 0.7176 - dense_319_loss: 0.7234 - dense_320_loss: 0.6873 - dense_321_loss: 0.6579 - dense_322_loss: 0.5585 - dense_323_loss: 0.5403 - dense_324_loss: 0.5128 - dense_315_accuracy: 0.5352 - dense_316_accuracy: 0.5738 - dense_317_accuracy: 0.5344 - dense_318_accuracy: 0.5544 - dense_319_accuracy: 0.5362 - dense_320_accuracy: 0.5935 - dense_321_accuracy: 0.6452 - dense_322_accuracy: 0.7031 - dense_323_accuracy: 0.7158 - dense_324_accuracy: 0.7252 - val_loss: 2.2179 - val_dense_315_loss: 0.6711 - val_dense_316_loss: 0.6610 - val_dense_317_loss: 0.7415 - val_dense_318_loss: 0.6701 - val_dense_319_loss: 0.6843 - val_dense_320_loss: 0.6795 - val_dense_321_loss: 0.6208 - val_dense_322_loss: 0.6382 - val_dense_323_loss: 0.6184 - val_dense_324_loss: 0.5818 - val_dense_315_accuracy: 0.5797 - val_dense_316_accuracy: 0.6072 - val_dense_317_accuracy: 0.5372 - val_dense_318_accuracy: 0.5960 - val_dense_319_accuracy: 0.5624 - val_dense_320_accuracy: 0.5954 - val_dense_321_accuracy: 0.6318 - val_dense_322_accuracy: 0.6682 - val_dense_323_accuracy: 0.7073 - val_dense_324_accuracy: 0.7124\n",
      "2020-01-12 15:41:39.273359\t\tValid\tMAP:\t0.41254295602051827\tFPA:\t0.5797425853385563\n",
      "=========================================================\n",
      "2020-01-12 15:41:39.643320\t\tStopping at epoch 1, best epoch was 1 with MAP 0.41254295602051827\n",
      "=========================================================\n",
      "2020-01-12 15:41:39.644075\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 2.1843 - dense_315_loss: 0.6766 - dense_316_loss: 0.6745 - dense_317_loss: 0.7097 - dense_318_loss: 0.6706 - dense_319_loss: 0.6843 - dense_320_loss: 0.6464 - dense_321_loss: 0.6082 - dense_322_loss: 0.5248 - dense_323_loss: 0.4845 - dense_324_loss: 0.4673 - dense_315_accuracy: 0.6278 - dense_316_accuracy: 0.6182 - dense_317_accuracy: 0.5805 - dense_318_accuracy: 0.6088 - dense_319_accuracy: 0.6065 - dense_320_accuracy: 0.6306 - dense_321_accuracy: 0.6509 - dense_322_accuracy: 0.7112 - dense_323_accuracy: 0.7339 - dense_324_accuracy: 0.7452 - val_loss: 2.2124 - val_dense_315_loss: 0.6983 - val_dense_316_loss: 0.6527 - val_dense_317_loss: 0.6690 - val_dense_318_loss: 0.6754 - val_dense_319_loss: 0.6677 - val_dense_320_loss: 0.6864 - val_dense_321_loss: 0.7077 - val_dense_322_loss: 0.5924 - val_dense_323_loss: 0.5652 - val_dense_324_loss: 0.6224 - val_dense_315_accuracy: 0.5350 - val_dense_316_accuracy: 0.6223 - val_dense_317_accuracy: 0.5965 - val_dense_318_accuracy: 0.5809 - val_dense_319_accuracy: 0.5932 - val_dense_320_accuracy: 0.5870 - val_dense_321_accuracy: 0.5988 - val_dense_322_accuracy: 0.6967 - val_dense_323_accuracy: 0.7252 - val_dense_324_accuracy: 0.6922\n",
      "2020-01-12 15:41:40.090059\t\tValid\tMAP:\t0.4212391295198671\tFPA:\t0.5349748181309457\n",
      "=========================================================\n",
      "2020-01-12 15:41:40.688959\t\tStopping at epoch 2, best epoch was 2 with MAP 0.4212391295198671\n",
      "=========================================================\n",
      "2020-01-12 15:41:40.689237\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 2.0814 - dense_315_loss: 0.6231 - dense_316_loss: 0.6506 - dense_317_loss: 0.6789 - dense_318_loss: 0.6592 - dense_319_loss: 0.6681 - dense_320_loss: 0.6262 - dense_321_loss: 0.5726 - dense_322_loss: 0.4997 - dense_323_loss: 0.4611 - dense_324_loss: 0.4439 - dense_315_accuracy: 0.6559 - dense_316_accuracy: 0.6278 - dense_317_accuracy: 0.5933 - dense_318_accuracy: 0.6123 - dense_319_accuracy: 0.6054 - dense_320_accuracy: 0.6568 - dense_321_accuracy: 0.6796 - dense_322_accuracy: 0.7262 - dense_323_accuracy: 0.7473 - dense_324_accuracy: 0.7574 - val_loss: 2.1011 - val_dense_315_loss: 0.6271 - val_dense_316_loss: 0.6479 - val_dense_317_loss: 0.6458 - val_dense_318_loss: 0.6741 - val_dense_319_loss: 0.6765 - val_dense_320_loss: 0.6318 - val_dense_321_loss: 0.6841 - val_dense_322_loss: 0.5725 - val_dense_323_loss: 0.4848 - val_dense_324_loss: 0.5325 - val_dense_315_accuracy: 0.6620 - val_dense_316_accuracy: 0.6340 - val_dense_317_accuracy: 0.6312 - val_dense_318_accuracy: 0.5921 - val_dense_319_accuracy: 0.5747 - val_dense_320_accuracy: 0.6508 - val_dense_321_accuracy: 0.6284 - val_dense_322_accuracy: 0.7096 - val_dense_323_accuracy: 0.7644 - val_dense_324_accuracy: 0.7448\n",
      "2020-01-12 15:41:41.131013\t\tValid\tMAP:\t0.4767018984426453\tFPA:\t0.6620033575825406\n",
      "=========================================================\n",
      "2020-01-12 15:41:41.569089\t\tStopping at epoch 3, best epoch was 3 with MAP 0.4767018984426453\n",
      "=========================================================\n",
      "2020-01-12 15:41:41.569887\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 2.0139 - dense_315_loss: 0.5822 - dense_316_loss: 0.6366 - dense_317_loss: 0.6669 - dense_318_loss: 0.6554 - dense_319_loss: 0.6613 - dense_320_loss: 0.6183 - dense_321_loss: 0.5605 - dense_322_loss: 0.4780 - dense_323_loss: 0.4401 - dense_324_loss: 0.4242 - dense_315_accuracy: 0.6955 - dense_316_accuracy: 0.6466 - dense_317_accuracy: 0.6235 - dense_318_accuracy: 0.6250 - dense_319_accuracy: 0.6148 - dense_320_accuracy: 0.6548 - dense_321_accuracy: 0.6996 - dense_322_accuracy: 0.7456 - dense_323_accuracy: 0.7656 - dense_324_accuracy: 0.7676 - val_loss: 2.1122 - val_dense_315_loss: 0.6475 - val_dense_316_loss: 0.6796 - val_dense_317_loss: 0.6659 - val_dense_318_loss: 0.6585 - val_dense_319_loss: 0.6579 - val_dense_320_loss: 0.6292 - val_dense_321_loss: 0.5789 - val_dense_322_loss: 0.5347 - val_dense_323_loss: 0.4527 - val_dense_324_loss: 0.4641 - val_dense_315_accuracy: 0.6144 - val_dense_316_accuracy: 0.5691 - val_dense_317_accuracy: 0.6072 - val_dense_318_accuracy: 0.6240 - val_dense_319_accuracy: 0.6094 - val_dense_320_accuracy: 0.6570 - val_dense_321_accuracy: 0.6905 - val_dense_322_accuracy: 0.7152 - val_dense_323_accuracy: 0.7790 - val_dense_324_accuracy: 0.7689\n",
      "2020-01-12 15:41:42.010078\t\tValid\tMAP:\t0.46700526626120137\tFPA:\t0.6144376049244544\n",
      "=========================================================\n",
      "2020-01-12 15:41:42.010775\t\tStopping at epoch 4, best epoch was 3 with MAP 0.4767018984426453\n",
      "=========================================================\n",
      "2020-01-12 15:41:42.011035\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.9857 - dense_315_loss: 0.5832 - dense_316_loss: 0.6213 - dense_317_loss: 0.6467 - dense_318_loss: 0.6511 - dense_319_loss: 0.6568 - dense_320_loss: 0.6136 - dense_321_loss: 0.5531 - dense_322_loss: 0.4837 - dense_323_loss: 0.4454 - dense_324_loss: 0.4368 - dense_315_accuracy: 0.6951 - dense_316_accuracy: 0.6629 - dense_317_accuracy: 0.6369 - dense_318_accuracy: 0.6280 - dense_319_accuracy: 0.6228 - dense_320_accuracy: 0.6722 - dense_321_accuracy: 0.6972 - dense_322_accuracy: 0.7449 - dense_323_accuracy: 0.7570 - dense_324_accuracy: 0.7616 - val_loss: 2.0617 - val_dense_315_loss: 0.6028 - val_dense_316_loss: 0.6385 - val_dense_317_loss: 0.6581 - val_dense_318_loss: 0.6585 - val_dense_319_loss: 0.6718 - val_dense_320_loss: 0.6716 - val_dense_321_loss: 0.5866 - val_dense_322_loss: 0.5581 - val_dense_323_loss: 0.5059 - val_dense_324_loss: 0.5088 - val_dense_315_accuracy: 0.6933 - val_dense_316_accuracy: 0.6508 - val_dense_317_accuracy: 0.6189 - val_dense_318_accuracy: 0.6374 - val_dense_319_accuracy: 0.5932 - val_dense_320_accuracy: 0.5865 - val_dense_321_accuracy: 0.6861 - val_dense_322_accuracy: 0.7084 - val_dense_323_accuracy: 0.7409 - val_dense_324_accuracy: 0.7443\n",
      "2020-01-12 15:41:42.451710\t\tValid\tMAP:\t0.49130205617035055\tFPA:\t0.6933407946278679\n",
      "=========================================================\n",
      "2020-01-12 15:41:42.874349\t\tStopping at epoch 5, best epoch was 5 with MAP 0.49130205617035055\n",
      "=========================================================\n",
      "2020-01-12 15:41:42.875783\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.9357 - dense_315_loss: 0.5567 - dense_316_loss: 0.6067 - dense_317_loss: 0.6352 - dense_318_loss: 0.6466 - dense_319_loss: 0.6470 - dense_320_loss: 0.6049 - dense_321_loss: 0.5371 - dense_322_loss: 0.4692 - dense_323_loss: 0.4344 - dense_324_loss: 0.4236 - dense_315_accuracy: 0.7202 - dense_316_accuracy: 0.6750 - dense_317_accuracy: 0.6421 - dense_318_accuracy: 0.6331 - dense_319_accuracy: 0.6418 - dense_320_accuracy: 0.6656 - dense_321_accuracy: 0.7023 - dense_322_accuracy: 0.7413 - dense_323_accuracy: 0.7623 - dense_324_accuracy: 0.7593 - val_loss: 1.9839 - val_dense_315_loss: 0.5610 - val_dense_316_loss: 0.6125 - val_dense_317_loss: 0.6333 - val_dense_318_loss: 0.6550 - val_dense_319_loss: 0.6638 - val_dense_320_loss: 0.6902 - val_dense_321_loss: 0.5632 - val_dense_322_loss: 0.5414 - val_dense_323_loss: 0.4859 - val_dense_324_loss: 0.4702 - val_dense_315_accuracy: 0.7275 - val_dense_316_accuracy: 0.6805 - val_dense_317_accuracy: 0.6581 - val_dense_318_accuracy: 0.6329 - val_dense_319_accuracy: 0.6049 - val_dense_320_accuracy: 0.5708 - val_dense_321_accuracy: 0.7073 - val_dense_322_accuracy: 0.7185 - val_dense_323_accuracy: 0.7482 - val_dense_324_accuracy: 0.7661\n",
      "2020-01-12 15:41:43.308942\t\tValid\tMAP:\t0.5164917266622704\tFPA:\t0.7274762171236709\n",
      "=========================================================\n",
      "2020-01-12 15:41:43.904459\t\tStopping at epoch 6, best epoch was 6 with MAP 0.5164917266622704\n",
      "=========================================================\n",
      "2020-01-12 15:41:43.904918\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.9174 - dense_315_loss: 0.5483 - dense_316_loss: 0.6026 - dense_317_loss: 0.6250 - dense_318_loss: 0.6421 - dense_319_loss: 0.6508 - dense_320_loss: 0.6031 - dense_321_loss: 0.5431 - dense_322_loss: 0.4700 - dense_323_loss: 0.4314 - dense_324_loss: 0.4145 - dense_315_accuracy: 0.7266 - dense_316_accuracy: 0.6808 - dense_317_accuracy: 0.6575 - dense_318_accuracy: 0.6352 - dense_319_accuracy: 0.6212 - dense_320_accuracy: 0.6660 - dense_321_accuracy: 0.6968 - dense_322_accuracy: 0.7374 - dense_323_accuracy: 0.7586 - dense_324_accuracy: 0.7623 - val_loss: 1.9808 - val_dense_315_loss: 0.5582 - val_dense_316_loss: 0.6134 - val_dense_317_loss: 0.6350 - val_dense_318_loss: 0.6622 - val_dense_319_loss: 0.6630 - val_dense_320_loss: 0.6597 - val_dense_321_loss: 0.5779 - val_dense_322_loss: 0.5449 - val_dense_323_loss: 0.4871 - val_dense_324_loss: 0.4608 - val_dense_315_accuracy: 0.7331 - val_dense_316_accuracy: 0.6821 - val_dense_317_accuracy: 0.6598 - val_dense_318_accuracy: 0.6195 - val_dense_319_accuracy: 0.6156 - val_dense_320_accuracy: 0.6016 - val_dense_321_accuracy: 0.6950 - val_dense_322_accuracy: 0.7230 - val_dense_323_accuracy: 0.7471 - val_dense_324_accuracy: 0.7711\n",
      "2020-01-12 15:41:44.345029\t\tValid\tMAP:\t0.519530083930398\tFPA:\t0.7330721880246223\n",
      "=========================================================\n",
      "2020-01-12 15:41:44.868104\t\tStopping at epoch 7, best epoch was 7 with MAP 0.519530083930398\n",
      "=========================================================\n",
      "2020-01-12 15:41:44.868671\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.8759 - dense_315_loss: 0.5291 - dense_316_loss: 0.5964 - dense_317_loss: 0.6180 - dense_318_loss: 0.6285 - dense_319_loss: 0.6362 - dense_320_loss: 0.5904 - dense_321_loss: 0.5235 - dense_322_loss: 0.4682 - dense_323_loss: 0.4292 - dense_324_loss: 0.4145 - dense_315_accuracy: 0.7350 - dense_316_accuracy: 0.6852 - dense_317_accuracy: 0.6618 - dense_318_accuracy: 0.6480 - dense_319_accuracy: 0.6489 - dense_320_accuracy: 0.6794 - dense_321_accuracy: 0.7218 - dense_322_accuracy: 0.7473 - dense_323_accuracy: 0.7665 - dense_324_accuracy: 0.7707 - val_loss: 1.9821 - val_dense_315_loss: 0.5542 - val_dense_316_loss: 0.6165 - val_dense_317_loss: 0.6336 - val_dense_318_loss: 0.6567 - val_dense_319_loss: 0.6614 - val_dense_320_loss: 0.6486 - val_dense_321_loss: 0.6154 - val_dense_322_loss: 0.5603 - val_dense_323_loss: 0.4955 - val_dense_324_loss: 0.4678 - val_dense_315_accuracy: 0.7275 - val_dense_316_accuracy: 0.6693 - val_dense_317_accuracy: 0.6581 - val_dense_318_accuracy: 0.6262 - val_dense_319_accuracy: 0.6133 - val_dense_320_accuracy: 0.6184 - val_dense_321_accuracy: 0.6570 - val_dense_322_accuracy: 0.7101 - val_dense_323_accuracy: 0.7532 - val_dense_324_accuracy: 0.7722\n",
      "2020-01-12 15:41:45.345097\t\tValid\tMAP:\t0.5155911408688739\tFPA:\t0.7274762171236709\n",
      "=========================================================\n",
      "2020-01-12 15:41:45.345336\t\tStopping at epoch 8, best epoch was 7 with MAP 0.519530083930398\n",
      "=========================================================\n",
      "2020-01-12 15:41:45.345412\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.8681 - dense_315_loss: 0.5294 - dense_316_loss: 0.5869 - dense_317_loss: 0.6174 - dense_318_loss: 0.6324 - dense_319_loss: 0.6363 - dense_320_loss: 0.5955 - dense_321_loss: 0.5271 - dense_322_loss: 0.4630 - dense_323_loss: 0.4290 - dense_324_loss: 0.4127 - dense_315_accuracy: 0.7521 - dense_316_accuracy: 0.6958 - dense_317_accuracy: 0.6627 - dense_318_accuracy: 0.6459 - dense_319_accuracy: 0.6445 - dense_320_accuracy: 0.6701 - dense_321_accuracy: 0.7111 - dense_322_accuracy: 0.7474 - dense_323_accuracy: 0.7648 - dense_324_accuracy: 0.7714 - val_loss: 1.9735 - val_dense_315_loss: 0.5530 - val_dense_316_loss: 0.6132 - val_dense_317_loss: 0.6327 - val_dense_318_loss: 0.6506 - val_dense_319_loss: 0.6585 - val_dense_320_loss: 0.6534 - val_dense_321_loss: 0.5944 - val_dense_322_loss: 0.5582 - val_dense_323_loss: 0.4974 - val_dense_324_loss: 0.4721 - val_dense_315_accuracy: 0.7308 - val_dense_316_accuracy: 0.6766 - val_dense_317_accuracy: 0.6575 - val_dense_318_accuracy: 0.6335 - val_dense_319_accuracy: 0.6240 - val_dense_320_accuracy: 0.6116 - val_dense_321_accuracy: 0.6777 - val_dense_322_accuracy: 0.7068 - val_dense_323_accuracy: 0.7493 - val_dense_324_accuracy: 0.7683\n",
      "2020-01-12 15:41:45.788166\t\tValid\tMAP:\t0.52223544770869\tFPA:\t0.7308337996642418\n",
      "=========================================================\n",
      "2020-01-12 15:41:46.370945\t\tStopping at epoch 9, best epoch was 9 with MAP 0.52223544770869\n",
      "=========================================================\n",
      "2020-01-12 15:41:46.371455\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.8554 - dense_315_loss: 0.5286 - dense_316_loss: 0.5860 - dense_317_loss: 0.6123 - dense_318_loss: 0.6273 - dense_319_loss: 0.6333 - dense_320_loss: 0.5888 - dense_321_loss: 0.5255 - dense_322_loss: 0.4576 - dense_323_loss: 0.4111 - dense_324_loss: 0.4023 - dense_315_accuracy: 0.7463 - dense_316_accuracy: 0.6936 - dense_317_accuracy: 0.6688 - dense_318_accuracy: 0.6544 - dense_319_accuracy: 0.6480 - dense_320_accuracy: 0.6828 - dense_321_accuracy: 0.7112 - dense_322_accuracy: 0.7563 - dense_323_accuracy: 0.7708 - dense_324_accuracy: 0.7743 - val_loss: 1.9734 - val_dense_315_loss: 0.5657 - val_dense_316_loss: 0.6072 - val_dense_317_loss: 0.6330 - val_dense_318_loss: 0.6570 - val_dense_319_loss: 0.6569 - val_dense_320_loss: 0.6282 - val_dense_321_loss: 0.5977 - val_dense_322_loss: 0.5344 - val_dense_323_loss: 0.4682 - val_dense_324_loss: 0.4522 - val_dense_315_accuracy: 0.7353 - val_dense_316_accuracy: 0.6883 - val_dense_317_accuracy: 0.6581 - val_dense_318_accuracy: 0.6391 - val_dense_319_accuracy: 0.6223 - val_dense_320_accuracy: 0.6547 - val_dense_321_accuracy: 0.6631 - val_dense_322_accuracy: 0.7180 - val_dense_323_accuracy: 0.7622 - val_dense_324_accuracy: 0.7650\n",
      "2020-01-12 15:41:46.811458\t\tValid\tMAP:\t0.5289268407522599\tFPA:\t0.7353105763850027\n",
      "=========================================================\n",
      "2020-01-12 15:41:46.910412\t\tStopping at epoch 10, best epoch was 10 with MAP 0.5289268407522599\n",
      "=========================================================\n",
      "2020-01-12 15:41:46.910765\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.8263 - dense_315_loss: 0.5131 - dense_316_loss: 0.5778 - dense_317_loss: 0.6018 - dense_318_loss: 0.6194 - dense_319_loss: 0.6218 - dense_320_loss: 0.5849 - dense_321_loss: 0.5173 - dense_322_loss: 0.4537 - dense_323_loss: 0.4142 - dense_324_loss: 0.3964 - dense_315_accuracy: 0.7561 - dense_316_accuracy: 0.7020 - dense_317_accuracy: 0.6766 - dense_318_accuracy: 0.6576 - dense_319_accuracy: 0.6540 - dense_320_accuracy: 0.6894 - dense_321_accuracy: 0.7218 - dense_322_accuracy: 0.7508 - dense_323_accuracy: 0.7728 - dense_324_accuracy: 0.7788 - val_loss: 1.9497 - val_dense_315_loss: 0.5514 - val_dense_316_loss: 0.6080 - val_dense_317_loss: 0.6259 - val_dense_318_loss: 0.6475 - val_dense_319_loss: 0.6550 - val_dense_320_loss: 0.6247 - val_dense_321_loss: 0.5902 - val_dense_322_loss: 0.5277 - val_dense_323_loss: 0.4668 - val_dense_324_loss: 0.4483 - val_dense_315_accuracy: 0.7387 - val_dense_316_accuracy: 0.6861 - val_dense_317_accuracy: 0.6654 - val_dense_318_accuracy: 0.6491 - val_dense_319_accuracy: 0.6267 - val_dense_320_accuracy: 0.6642 - val_dense_321_accuracy: 0.6665 - val_dense_322_accuracy: 0.7236 - val_dense_323_accuracy: 0.7566 - val_dense_324_accuracy: 0.7706\n",
      "2020-01-12 15:41:47.365800\t\tValid\tMAP:\t0.5344134018039636\tFPA:\t0.7386681589255736\n",
      "=========================================================\n",
      "2020-01-12 15:41:47.482198\t\tStopping at epoch 11, best epoch was 11 with MAP 0.5344134018039636\n",
      "=========================================================\n",
      "2020-01-12 15:41:47.482616\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.8032 - dense_315_loss: 0.5001 - dense_316_loss: 0.5682 - dense_317_loss: 0.6046 - dense_318_loss: 0.6171 - dense_319_loss: 0.6255 - dense_320_loss: 0.5740 - dense_321_loss: 0.5136 - dense_322_loss: 0.4486 - dense_323_loss: 0.4113 - dense_324_loss: 0.3945 - dense_315_accuracy: 0.7620 - dense_316_accuracy: 0.7111 - dense_317_accuracy: 0.6722 - dense_318_accuracy: 0.6653 - dense_319_accuracy: 0.6603 - dense_320_accuracy: 0.6961 - dense_321_accuracy: 0.7174 - dense_322_accuracy: 0.7596 - dense_323_accuracy: 0.7752 - dense_324_accuracy: 0.7800 - val_loss: 1.9462 - val_dense_315_loss: 0.5465 - val_dense_316_loss: 0.6034 - val_dense_317_loss: 0.6292 - val_dense_318_loss: 0.6494 - val_dense_319_loss: 0.6550 - val_dense_320_loss: 0.6294 - val_dense_321_loss: 0.5821 - val_dense_322_loss: 0.5341 - val_dense_323_loss: 0.4767 - val_dense_324_loss: 0.4656 - val_dense_315_accuracy: 0.7375 - val_dense_316_accuracy: 0.6939 - val_dense_317_accuracy: 0.6598 - val_dense_318_accuracy: 0.6402 - val_dense_319_accuracy: 0.6290 - val_dense_320_accuracy: 0.6564 - val_dense_321_accuracy: 0.6900 - val_dense_322_accuracy: 0.7191 - val_dense_323_accuracy: 0.7521 - val_dense_324_accuracy: 0.7644\n",
      "2020-01-12 15:41:47.924262\t\tValid\tMAP:\t0.534459245427948\tFPA:\t0.7375489647453833\n",
      "=========================================================\n",
      "2020-01-12 15:41:48.422216\t\tStopping at epoch 12, best epoch was 12 with MAP 0.534459245427948\n",
      "=========================================================\n",
      "2020-01-12 15:41:48.422331\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7913 - dense_315_loss: 0.4972 - dense_316_loss: 0.5699 - dense_317_loss: 0.5936 - dense_318_loss: 0.6108 - dense_319_loss: 0.6149 - dense_320_loss: 0.5731 - dense_321_loss: 0.5045 - dense_322_loss: 0.4441 - dense_323_loss: 0.4028 - dense_324_loss: 0.3902 - dense_315_accuracy: 0.7612 - dense_316_accuracy: 0.7104 - dense_317_accuracy: 0.6885 - dense_318_accuracy: 0.6646 - dense_319_accuracy: 0.6580 - dense_320_accuracy: 0.6968 - dense_321_accuracy: 0.7337 - dense_322_accuracy: 0.7627 - dense_323_accuracy: 0.7798 - dense_324_accuracy: 0.7863 - val_loss: 1.9373 - val_dense_315_loss: 0.5449 - val_dense_316_loss: 0.6104 - val_dense_317_loss: 0.6266 - val_dense_318_loss: 0.6469 - val_dense_319_loss: 0.6579 - val_dense_320_loss: 0.6252 - val_dense_321_loss: 0.5615 - val_dense_322_loss: 0.5063 - val_dense_323_loss: 0.4560 - val_dense_324_loss: 0.4405 - val_dense_315_accuracy: 0.7370 - val_dense_316_accuracy: 0.6821 - val_dense_317_accuracy: 0.6609 - val_dense_318_accuracy: 0.6368 - val_dense_319_accuracy: 0.6279 - val_dense_320_accuracy: 0.6676 - val_dense_321_accuracy: 0.7180 - val_dense_322_accuracy: 0.7415 - val_dense_323_accuracy: 0.7644 - val_dense_324_accuracy: 0.7818\n",
      "2020-01-12 15:41:48.917040\t\tValid\tMAP:\t0.5389547613020208\tFPA:\t0.7369893676552882\n",
      "=========================================================\n",
      "2020-01-12 15:41:49.427552\t\tStopping at epoch 13, best epoch was 13 with MAP 0.5389547613020208\n",
      "=========================================================\n",
      "2020-01-12 15:41:49.427718\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7772 - dense_315_loss: 0.4938 - dense_316_loss: 0.5628 - dense_317_loss: 0.5905 - dense_318_loss: 0.6111 - dense_319_loss: 0.6099 - dense_320_loss: 0.5695 - dense_321_loss: 0.5064 - dense_322_loss: 0.4428 - dense_323_loss: 0.3989 - dense_324_loss: 0.3915 - dense_315_accuracy: 0.7633 - dense_316_accuracy: 0.7181 - dense_317_accuracy: 0.6873 - dense_318_accuracy: 0.6653 - dense_319_accuracy: 0.6662 - dense_320_accuracy: 0.6997 - dense_321_accuracy: 0.7291 - dense_322_accuracy: 0.7654 - dense_323_accuracy: 0.7856 - dense_324_accuracy: 0.7784 - val_loss: 1.9441 - val_dense_315_loss: 0.5569 - val_dense_316_loss: 0.6093 - val_dense_317_loss: 0.6370 - val_dense_318_loss: 0.6557 - val_dense_319_loss: 0.6520 - val_dense_320_loss: 0.6151 - val_dense_321_loss: 0.5473 - val_dense_322_loss: 0.4817 - val_dense_323_loss: 0.4300 - val_dense_324_loss: 0.4141 - val_dense_315_accuracy: 0.7426 - val_dense_316_accuracy: 0.6861 - val_dense_317_accuracy: 0.6491 - val_dense_318_accuracy: 0.6307 - val_dense_319_accuracy: 0.6335 - val_dense_320_accuracy: 0.6771 - val_dense_321_accuracy: 0.7090 - val_dense_322_accuracy: 0.7426 - val_dense_323_accuracy: 0.7706 - val_dense_324_accuracy: 0.7823\n",
      "2020-01-12 15:41:49.873543\t\tValid\tMAP:\t0.5365552868953755\tFPA:\t0.7425853385562395\n",
      "=========================================================\n",
      "2020-01-12 15:41:49.874273\t\tStopping at epoch 14, best epoch was 13 with MAP 0.5389547613020208\n",
      "=========================================================\n",
      "2020-01-12 15:41:49.874350\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7649 - dense_315_loss: 0.4897 - dense_316_loss: 0.5587 - dense_317_loss: 0.5831 - dense_318_loss: 0.6071 - dense_319_loss: 0.6149 - dense_320_loss: 0.5699 - dense_321_loss: 0.5062 - dense_322_loss: 0.4441 - dense_323_loss: 0.4124 - dense_324_loss: 0.3922 - dense_315_accuracy: 0.7633 - dense_316_accuracy: 0.7134 - dense_317_accuracy: 0.6947 - dense_318_accuracy: 0.6680 - dense_319_accuracy: 0.6648 - dense_320_accuracy: 0.7020 - dense_321_accuracy: 0.7326 - dense_322_accuracy: 0.7642 - dense_323_accuracy: 0.7728 - dense_324_accuracy: 0.7793 - val_loss: 1.9506 - val_dense_315_loss: 0.5607 - val_dense_316_loss: 0.6128 - val_dense_317_loss: 0.6394 - val_dense_318_loss: 0.6569 - val_dense_319_loss: 0.6518 - val_dense_320_loss: 0.6091 - val_dense_321_loss: 0.5473 - val_dense_322_loss: 0.4822 - val_dense_323_loss: 0.4336 - val_dense_324_loss: 0.4155 - val_dense_315_accuracy: 0.7403 - val_dense_316_accuracy: 0.6855 - val_dense_317_accuracy: 0.6503 - val_dense_318_accuracy: 0.6240 - val_dense_319_accuracy: 0.6240 - val_dense_320_accuracy: 0.6799 - val_dense_321_accuracy: 0.7107 - val_dense_322_accuracy: 0.7426 - val_dense_323_accuracy: 0.7666 - val_dense_324_accuracy: 0.7762\n",
      "2020-01-12 15:41:50.335285\t\tValid\tMAP:\t0.5341532628253689\tFPA:\t0.7403469501958589\n",
      "=========================================================\n",
      "2020-01-12 15:41:50.335885\t\tStopping at epoch 15, best epoch was 13 with MAP 0.5389547613020208\n",
      "=========================================================\n",
      "2020-01-12 15:41:50.336153\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7628 - dense_315_loss: 0.4848 - dense_316_loss: 0.5520 - dense_317_loss: 0.5852 - dense_318_loss: 0.6063 - dense_319_loss: 0.6105 - dense_320_loss: 0.5664 - dense_321_loss: 0.5048 - dense_322_loss: 0.4428 - dense_323_loss: 0.4104 - dense_324_loss: 0.3882 - dense_315_accuracy: 0.7735 - dense_316_accuracy: 0.7188 - dense_317_accuracy: 0.6920 - dense_318_accuracy: 0.6698 - dense_319_accuracy: 0.6723 - dense_320_accuracy: 0.7041 - dense_321_accuracy: 0.7354 - dense_322_accuracy: 0.7696 - dense_323_accuracy: 0.7792 - dense_324_accuracy: 0.7861 - val_loss: 1.9233 - val_dense_315_loss: 0.5444 - val_dense_316_loss: 0.6037 - val_dense_317_loss: 0.6260 - val_dense_318_loss: 0.6504 - val_dense_319_loss: 0.6496 - val_dense_320_loss: 0.6121 - val_dense_321_loss: 0.5531 - val_dense_322_loss: 0.4929 - val_dense_323_loss: 0.4388 - val_dense_324_loss: 0.4207 - val_dense_315_accuracy: 0.7482 - val_dense_316_accuracy: 0.6950 - val_dense_317_accuracy: 0.6514 - val_dense_318_accuracy: 0.6424 - val_dense_319_accuracy: 0.6307 - val_dense_320_accuracy: 0.6827 - val_dense_321_accuracy: 0.7084 - val_dense_322_accuracy: 0.7381 - val_dense_323_accuracy: 0.7622 - val_dense_324_accuracy: 0.7784\n",
      "2020-01-12 15:41:50.779052\t\tValid\tMAP:\t0.5427041076280452\tFPA:\t0.7481813094571909\n",
      "=========================================================\n",
      "2020-01-12 15:41:50.874811\t\tStopping at epoch 16, best epoch was 16 with MAP 0.5427041076280452\n",
      "=========================================================\n",
      "2020-01-12 15:41:50.875499\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7475 - dense_315_loss: 0.4762 - dense_316_loss: 0.5499 - dense_317_loss: 0.5761 - dense_318_loss: 0.6029 - dense_319_loss: 0.6088 - dense_320_loss: 0.5671 - dense_321_loss: 0.5030 - dense_322_loss: 0.4383 - dense_323_loss: 0.3992 - dense_324_loss: 0.3882 - dense_315_accuracy: 0.7725 - dense_316_accuracy: 0.7195 - dense_317_accuracy: 0.6912 - dense_318_accuracy: 0.6752 - dense_319_accuracy: 0.6705 - dense_320_accuracy: 0.7037 - dense_321_accuracy: 0.7349 - dense_322_accuracy: 0.7626 - dense_323_accuracy: 0.7819 - dense_324_accuracy: 0.7817 - val_loss: 1.9441 - val_dense_315_loss: 0.5546 - val_dense_316_loss: 0.6151 - val_dense_317_loss: 0.6381 - val_dense_318_loss: 0.6535 - val_dense_319_loss: 0.6555 - val_dense_320_loss: 0.6138 - val_dense_321_loss: 0.5462 - val_dense_322_loss: 0.4753 - val_dense_323_loss: 0.4253 - val_dense_324_loss: 0.4074 - val_dense_315_accuracy: 0.7353 - val_dense_316_accuracy: 0.6794 - val_dense_317_accuracy: 0.6413 - val_dense_318_accuracy: 0.6290 - val_dense_319_accuracy: 0.6228 - val_dense_320_accuracy: 0.6754 - val_dense_321_accuracy: 0.7174 - val_dense_322_accuracy: 0.7431 - val_dense_323_accuracy: 0.7862 - val_dense_324_accuracy: 0.7840\n",
      "2020-01-12 15:41:51.321232\t\tValid\tMAP:\t0.5347907773598772\tFPA:\t0.7353105763850027\n",
      "=========================================================\n",
      "2020-01-12 15:41:51.321454\t\tStopping at epoch 17, best epoch was 16 with MAP 0.5427041076280452\n",
      "=========================================================\n",
      "2020-01-12 15:41:51.322174\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7530 - dense_315_loss: 0.4844 - dense_316_loss: 0.5544 - dense_317_loss: 0.5781 - dense_318_loss: 0.6031 - dense_319_loss: 0.6065 - dense_320_loss: 0.5634 - dense_321_loss: 0.4986 - dense_322_loss: 0.4372 - dense_323_loss: 0.3982 - dense_324_loss: 0.3831 - dense_315_accuracy: 0.7680 - dense_316_accuracy: 0.7174 - dense_317_accuracy: 0.6913 - dense_318_accuracy: 0.6727 - dense_319_accuracy: 0.6719 - dense_320_accuracy: 0.7027 - dense_321_accuracy: 0.7356 - dense_322_accuracy: 0.7684 - dense_323_accuracy: 0.7837 - dense_324_accuracy: 0.7835 - val_loss: 1.9610 - val_dense_315_loss: 0.5664 - val_dense_316_loss: 0.6209 - val_dense_317_loss: 0.6411 - val_dense_318_loss: 0.6537 - val_dense_319_loss: 0.6572 - val_dense_320_loss: 0.6152 - val_dense_321_loss: 0.5454 - val_dense_322_loss: 0.4728 - val_dense_323_loss: 0.4236 - val_dense_324_loss: 0.4063 - val_dense_315_accuracy: 0.7286 - val_dense_316_accuracy: 0.6872 - val_dense_317_accuracy: 0.6396 - val_dense_318_accuracy: 0.6402 - val_dense_319_accuracy: 0.6167 - val_dense_320_accuracy: 0.6805 - val_dense_321_accuracy: 0.7230 - val_dense_322_accuracy: 0.7555 - val_dense_323_accuracy: 0.7918 - val_dense_324_accuracy: 0.7806\n",
      "2020-01-12 15:41:51.778802\t\tValid\tMAP:\t0.5356314911817283\tFPA:\t0.7285954113038612\n",
      "=========================================================\n",
      "2020-01-12 15:41:51.779425\t\tStopping at epoch 18, best epoch was 16 with MAP 0.5427041076280452\n",
      "=========================================================\n",
      "2020-01-12 15:41:51.780086\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7414 - dense_315_loss: 0.4730 - dense_316_loss: 0.5495 - dense_317_loss: 0.5828 - dense_318_loss: 0.5968 - dense_319_loss: 0.6043 - dense_320_loss: 0.5621 - dense_321_loss: 0.5035 - dense_322_loss: 0.4317 - dense_323_loss: 0.3979 - dense_324_loss: 0.3873 - dense_315_accuracy: 0.7816 - dense_316_accuracy: 0.7178 - dense_317_accuracy: 0.6904 - dense_318_accuracy: 0.6786 - dense_319_accuracy: 0.6750 - dense_320_accuracy: 0.7017 - dense_321_accuracy: 0.7365 - dense_322_accuracy: 0.7700 - dense_323_accuracy: 0.7798 - dense_324_accuracy: 0.7859 - val_loss: 1.9489 - val_dense_315_loss: 0.5557 - val_dense_316_loss: 0.6178 - val_dense_317_loss: 0.6432 - val_dense_318_loss: 0.6564 - val_dense_319_loss: 0.6567 - val_dense_320_loss: 0.6126 - val_dense_321_loss: 0.5441 - val_dense_322_loss: 0.4722 - val_dense_323_loss: 0.4213 - val_dense_324_loss: 0.4026 - val_dense_315_accuracy: 0.7431 - val_dense_316_accuracy: 0.6810 - val_dense_317_accuracy: 0.6351 - val_dense_318_accuracy: 0.6295 - val_dense_319_accuracy: 0.6279 - val_dense_320_accuracy: 0.6844 - val_dense_321_accuracy: 0.7219 - val_dense_322_accuracy: 0.7594 - val_dense_323_accuracy: 0.7969 - val_dense_324_accuracy: 0.7907\n",
      "2020-01-12 15:41:52.239147\t\tValid\tMAP:\t0.540787837959491\tFPA:\t0.7431449356463347\n",
      "=========================================================\n",
      "2020-01-12 15:41:52.240060\t\tStopping at epoch 19, best epoch was 16 with MAP 0.5427041076280452\n",
      "=========================================================\n",
      "2020-01-12 15:41:52.240456\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7505 - dense_315_loss: 0.4777 - dense_316_loss: 0.5495 - dense_317_loss: 0.5827 - dense_318_loss: 0.5991 - dense_319_loss: 0.6088 - dense_320_loss: 0.5673 - dense_321_loss: 0.5015 - dense_322_loss: 0.4429 - dense_323_loss: 0.4005 - dense_324_loss: 0.3841 - dense_315_accuracy: 0.7691 - dense_316_accuracy: 0.7186 - dense_317_accuracy: 0.6913 - dense_318_accuracy: 0.6775 - dense_319_accuracy: 0.6752 - dense_320_accuracy: 0.7017 - dense_321_accuracy: 0.7315 - dense_322_accuracy: 0.7637 - dense_323_accuracy: 0.7816 - dense_324_accuracy: 0.7894 - val_loss: 1.9391 - val_dense_315_loss: 0.5500 - val_dense_316_loss: 0.6115 - val_dense_317_loss: 0.6420 - val_dense_318_loss: 0.6527 - val_dense_319_loss: 0.6567 - val_dense_320_loss: 0.6164 - val_dense_321_loss: 0.5479 - val_dense_322_loss: 0.4726 - val_dense_323_loss: 0.4205 - val_dense_324_loss: 0.4069 - val_dense_315_accuracy: 0.7403 - val_dense_316_accuracy: 0.6905 - val_dense_317_accuracy: 0.6379 - val_dense_318_accuracy: 0.6340 - val_dense_319_accuracy: 0.6368 - val_dense_320_accuracy: 0.6715 - val_dense_321_accuracy: 0.7224 - val_dense_322_accuracy: 0.7571 - val_dense_323_accuracy: 0.7918 - val_dense_324_accuracy: 0.7801\n",
      "2020-01-12 15:41:52.702250\t\tValid\tMAP:\t0.540004571752429\tFPA:\t0.7403469501958589\n",
      "=========================================================\n",
      "2020-01-12 15:41:52.702775\t\tStopping at epoch 20, best epoch was 16 with MAP 0.5427041076280452\n",
      "=========================================================\n",
      "2020-01-12 15:41:52.703007\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7440 - dense_315_loss: 0.4764 - dense_316_loss: 0.5561 - dense_317_loss: 0.5796 - dense_318_loss: 0.5999 - dense_319_loss: 0.6061 - dense_320_loss: 0.5615 - dense_321_loss: 0.4979 - dense_322_loss: 0.4392 - dense_323_loss: 0.4015 - dense_324_loss: 0.3830 - dense_315_accuracy: 0.7752 - dense_316_accuracy: 0.7202 - dense_317_accuracy: 0.6939 - dense_318_accuracy: 0.6765 - dense_319_accuracy: 0.6713 - dense_320_accuracy: 0.7091 - dense_321_accuracy: 0.7349 - dense_322_accuracy: 0.7610 - dense_323_accuracy: 0.7789 - dense_324_accuracy: 0.7889 - val_loss: 1.9435 - val_dense_315_loss: 0.5535 - val_dense_316_loss: 0.6138 - val_dense_317_loss: 0.6404 - val_dense_318_loss: 0.6550 - val_dense_319_loss: 0.6561 - val_dense_320_loss: 0.6161 - val_dense_321_loss: 0.5455 - val_dense_322_loss: 0.4731 - val_dense_323_loss: 0.4203 - val_dense_324_loss: 0.4049 - val_dense_315_accuracy: 0.7420 - val_dense_316_accuracy: 0.6777 - val_dense_317_accuracy: 0.6424 - val_dense_318_accuracy: 0.6351 - val_dense_319_accuracy: 0.6301 - val_dense_320_accuracy: 0.6766 - val_dense_321_accuracy: 0.7191 - val_dense_322_accuracy: 0.7549 - val_dense_323_accuracy: 0.7879 - val_dense_324_accuracy: 0.7913\n",
      "2020-01-12 15:41:53.149575\t\tValid\tMAP:\t0.5380270264492454\tFPA:\t0.7420257414661444\n",
      "=========================================================\n",
      "2020-01-12 15:42:05.143335\t\tAverage best MAP over all folds:\t\t0.5065646053435627...\n",
      "=========================================================\n",
      "2020-01-12 15:42:05.144113\t----- FOLD 1 -----\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7972 - dense_315_loss: 0.5017 - dense_316_loss: 0.5690 - dense_317_loss: 0.5908 - dense_318_loss: 0.6179 - dense_319_loss: 0.6223 - dense_320_loss: 0.5799 - dense_321_loss: 0.5064 - dense_322_loss: 0.4422 - dense_323_loss: 0.3986 - dense_324_loss: 0.3789 - dense_315_accuracy: 0.7599 - dense_316_accuracy: 0.7098 - dense_317_accuracy: 0.6814 - dense_318_accuracy: 0.6620 - dense_319_accuracy: 0.6576 - dense_320_accuracy: 0.6926 - dense_321_accuracy: 0.7302 - dense_322_accuracy: 0.7610 - dense_323_accuracy: 0.7816 - dense_324_accuracy: 0.7901 - val_loss: 1.8053 - val_dense_315_loss: 0.5005 - val_dense_316_loss: 0.5690 - val_dense_317_loss: 0.6028 - val_dense_318_loss: 0.6073 - val_dense_319_loss: 0.6176 - val_dense_320_loss: 0.5782 - val_dense_321_loss: 0.5204 - val_dense_322_loss: 0.4563 - val_dense_323_loss: 0.4192 - val_dense_324_loss: 0.4008 - val_dense_315_accuracy: 0.7739 - val_dense_316_accuracy: 0.7152 - val_dense_317_accuracy: 0.6917 - val_dense_318_accuracy: 0.6855 - val_dense_319_accuracy: 0.6821 - val_dense_320_accuracy: 0.7208 - val_dense_321_accuracy: 0.7499 - val_dense_322_accuracy: 0.7790 - val_dense_323_accuracy: 0.7806 - val_dense_324_accuracy: 0.7902\n",
      "2020-01-12 15:42:05.620556\t\tValid\tMAP:\t0.5822734207208549\tFPA:\t0.7739227756015669\n",
      "=========================================================\n",
      "2020-01-12 15:42:06.067970\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5822734207208549\n",
      "=========================================================\n",
      "2020-01-12 15:42:06.068415\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7796 - dense_315_loss: 0.4966 - dense_316_loss: 0.5601 - dense_317_loss: 0.5911 - dense_318_loss: 0.6141 - dense_319_loss: 0.6157 - dense_320_loss: 0.5749 - dense_321_loss: 0.5039 - dense_322_loss: 0.4350 - dense_323_loss: 0.3957 - dense_324_loss: 0.3757 - dense_315_accuracy: 0.7637 - dense_316_accuracy: 0.7143 - dense_317_accuracy: 0.6831 - dense_318_accuracy: 0.6643 - dense_319_accuracy: 0.6676 - dense_320_accuracy: 0.6972 - dense_321_accuracy: 0.7316 - dense_322_accuracy: 0.7665 - dense_323_accuracy: 0.7810 - dense_324_accuracy: 0.7901 - val_loss: 1.8444 - val_dense_315_loss: 0.5106 - val_dense_316_loss: 0.5811 - val_dense_317_loss: 0.6100 - val_dense_318_loss: 0.6286 - val_dense_319_loss: 0.6316 - val_dense_320_loss: 0.5833 - val_dense_321_loss: 0.5313 - val_dense_322_loss: 0.4708 - val_dense_323_loss: 0.4423 - val_dense_324_loss: 0.4175 - val_dense_315_accuracy: 0.7633 - val_dense_316_accuracy: 0.7140 - val_dense_317_accuracy: 0.6788 - val_dense_318_accuracy: 0.6542 - val_dense_319_accuracy: 0.6503 - val_dense_320_accuracy: 0.7090 - val_dense_321_accuracy: 0.7303 - val_dense_322_accuracy: 0.7224 - val_dense_323_accuracy: 0.7146 - val_dense_324_accuracy: 0.7258\n",
      "2020-01-12 15:42:06.531265\t\tValid\tMAP:\t0.5520310564224449\tFPA:\t0.7632904308897593\n",
      "=========================================================\n",
      "2020-01-12 15:42:06.531451\t\tStopping at epoch 1, best epoch was 0 with MAP 0.5822734207208549\n",
      "=========================================================\n",
      "2020-01-12 15:42:06.532074\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7682 - dense_315_loss: 0.4897 - dense_316_loss: 0.5571 - dense_317_loss: 0.5857 - dense_318_loss: 0.6086 - dense_319_loss: 0.6163 - dense_320_loss: 0.5704 - dense_321_loss: 0.5024 - dense_322_loss: 0.4311 - dense_323_loss: 0.3957 - dense_324_loss: 0.3777 - dense_315_accuracy: 0.7670 - dense_316_accuracy: 0.7179 - dense_317_accuracy: 0.6874 - dense_318_accuracy: 0.6704 - dense_319_accuracy: 0.6722 - dense_320_accuracy: 0.6958 - dense_321_accuracy: 0.7393 - dense_322_accuracy: 0.7655 - dense_323_accuracy: 0.7774 - dense_324_accuracy: 0.7894 - val_loss: 1.8430 - val_dense_315_loss: 0.5022 - val_dense_316_loss: 0.5843 - val_dense_317_loss: 0.6218 - val_dense_318_loss: 0.6324 - val_dense_319_loss: 0.6341 - val_dense_320_loss: 0.5845 - val_dense_321_loss: 0.5286 - val_dense_322_loss: 0.4651 - val_dense_323_loss: 0.4309 - val_dense_324_loss: 0.4093 - val_dense_315_accuracy: 0.7857 - val_dense_316_accuracy: 0.7118 - val_dense_317_accuracy: 0.6665 - val_dense_318_accuracy: 0.6503 - val_dense_319_accuracy: 0.6458 - val_dense_320_accuracy: 0.6973 - val_dense_321_accuracy: 0.7292 - val_dense_322_accuracy: 0.7443 - val_dense_323_accuracy: 0.7353 - val_dense_324_accuracy: 0.7426\n",
      "2020-01-12 15:42:06.975031\t\tValid\tMAP:\t0.558170460213972\tFPA:\t0.7856743144935646\n",
      "=========================================================\n",
      "2020-01-12 15:42:06.975660\t\tStopping at epoch 2, best epoch was 0 with MAP 0.5822734207208549\n",
      "=========================================================\n",
      "2020-01-12 15:42:06.975893\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7641 - dense_315_loss: 0.4868 - dense_316_loss: 0.5594 - dense_317_loss: 0.5865 - dense_318_loss: 0.6081 - dense_319_loss: 0.6086 - dense_320_loss: 0.5668 - dense_321_loss: 0.5023 - dense_322_loss: 0.4331 - dense_323_loss: 0.3958 - dense_324_loss: 0.3779 - dense_315_accuracy: 0.7672 - dense_316_accuracy: 0.7151 - dense_317_accuracy: 0.6887 - dense_318_accuracy: 0.6711 - dense_319_accuracy: 0.6754 - dense_320_accuracy: 0.6993 - dense_321_accuracy: 0.7332 - dense_322_accuracy: 0.7707 - dense_323_accuracy: 0.7812 - dense_324_accuracy: 0.7905 - val_loss: 1.8285 - val_dense_315_loss: 0.5047 - val_dense_316_loss: 0.5800 - val_dense_317_loss: 0.6124 - val_dense_318_loss: 0.6187 - val_dense_319_loss: 0.6243 - val_dense_320_loss: 0.5810 - val_dense_321_loss: 0.5255 - val_dense_322_loss: 0.4581 - val_dense_323_loss: 0.4239 - val_dense_324_loss: 0.4054 - val_dense_315_accuracy: 0.7745 - val_dense_316_accuracy: 0.7057 - val_dense_317_accuracy: 0.6788 - val_dense_318_accuracy: 0.6816 - val_dense_319_accuracy: 0.6553 - val_dense_320_accuracy: 0.7079 - val_dense_321_accuracy: 0.7364 - val_dense_322_accuracy: 0.7700 - val_dense_323_accuracy: 0.7566 - val_dense_324_accuracy: 0.7504\n",
      "2020-01-12 15:42:07.416712\t\tValid\tMAP:\t0.5669460756333041\tFPA:\t0.774482372691662\n",
      "=========================================================\n",
      "2020-01-12 15:42:07.417270\t\tStopping at epoch 3, best epoch was 0 with MAP 0.5822734207208549\n",
      "=========================================================\n",
      "2020-01-12 15:42:07.417506\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7614 - dense_315_loss: 0.4867 - dense_316_loss: 0.5518 - dense_317_loss: 0.5836 - dense_318_loss: 0.6096 - dense_319_loss: 0.6153 - dense_320_loss: 0.5665 - dense_321_loss: 0.5013 - dense_322_loss: 0.4324 - dense_323_loss: 0.3995 - dense_324_loss: 0.3747 - dense_315_accuracy: 0.7704 - dense_316_accuracy: 0.7189 - dense_317_accuracy: 0.6933 - dense_318_accuracy: 0.6704 - dense_319_accuracy: 0.6592 - dense_320_accuracy: 0.6982 - dense_321_accuracy: 0.7350 - dense_322_accuracy: 0.7662 - dense_323_accuracy: 0.7845 - dense_324_accuracy: 0.7943 - val_loss: 1.8357 - val_dense_315_loss: 0.5008 - val_dense_316_loss: 0.5845 - val_dense_317_loss: 0.6186 - val_dense_318_loss: 0.6261 - val_dense_319_loss: 0.6316 - val_dense_320_loss: 0.5820 - val_dense_321_loss: 0.5275 - val_dense_322_loss: 0.4606 - val_dense_323_loss: 0.4262 - val_dense_324_loss: 0.4073 - val_dense_315_accuracy: 0.7818 - val_dense_316_accuracy: 0.7135 - val_dense_317_accuracy: 0.6665 - val_dense_318_accuracy: 0.6609 - val_dense_319_accuracy: 0.6586 - val_dense_320_accuracy: 0.7034 - val_dense_321_accuracy: 0.7303 - val_dense_322_accuracy: 0.7616 - val_dense_323_accuracy: 0.7521 - val_dense_324_accuracy: 0.7471\n",
      "2020-01-12 15:42:07.870071\t\tValid\tMAP:\t0.5641596089616175\tFPA:\t0.7817571348628987\n",
      "=========================================================\n",
      "2020-01-12 15:42:07.870621\t\tStopping at epoch 4, best epoch was 0 with MAP 0.5822734207208549\n",
      "=========================================================\n",
      "2020-01-12 15:42:07.870977\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7569 - dense_315_loss: 0.4855 - dense_316_loss: 0.5553 - dense_317_loss: 0.5829 - dense_318_loss: 0.6072 - dense_319_loss: 0.6057 - dense_320_loss: 0.5709 - dense_321_loss: 0.5023 - dense_322_loss: 0.4368 - dense_323_loss: 0.4020 - dense_324_loss: 0.3814 - dense_315_accuracy: 0.7672 - dense_316_accuracy: 0.7223 - dense_317_accuracy: 0.6904 - dense_318_accuracy: 0.6722 - dense_319_accuracy: 0.6804 - dense_320_accuracy: 0.6960 - dense_321_accuracy: 0.7370 - dense_322_accuracy: 0.7665 - dense_323_accuracy: 0.7789 - dense_324_accuracy: 0.7889 - val_loss: 1.8201 - val_dense_315_loss: 0.5006 - val_dense_316_loss: 0.5760 - val_dense_317_loss: 0.6079 - val_dense_318_loss: 0.6198 - val_dense_319_loss: 0.6235 - val_dense_320_loss: 0.5818 - val_dense_321_loss: 0.5258 - val_dense_322_loss: 0.4587 - val_dense_323_loss: 0.4238 - val_dense_324_loss: 0.4032 - val_dense_315_accuracy: 0.7734 - val_dense_316_accuracy: 0.7146 - val_dense_317_accuracy: 0.6743 - val_dense_318_accuracy: 0.6782 - val_dense_319_accuracy: 0.6553 - val_dense_320_accuracy: 0.7062 - val_dense_321_accuracy: 0.7353 - val_dense_322_accuracy: 0.7790 - val_dense_323_accuracy: 0.7728 - val_dense_324_accuracy: 0.7750\n",
      "2020-01-12 15:42:08.312923\t\tValid\tMAP:\t0.5698351185382857\tFPA:\t0.7733631785114717\n",
      "=========================================================\n",
      "2020-01-12 15:42:08.408567\t\tAverage best MAP over all folds:\t\t0.5211239929161189...\n",
      "=========================================================\n",
      "2020-01-12 15:42:08.409547\t----- FOLD 2 -----\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7785 - dense_315_loss: 0.5012 - dense_316_loss: 0.5514 - dense_317_loss: 0.5904 - dense_318_loss: 0.6077 - dense_319_loss: 0.6109 - dense_320_loss: 0.5688 - dense_321_loss: 0.4993 - dense_322_loss: 0.4305 - dense_323_loss: 0.3947 - dense_324_loss: 0.3750 - dense_315_accuracy: 0.7518 - dense_316_accuracy: 0.7171 - dense_317_accuracy: 0.6799 - dense_318_accuracy: 0.6683 - dense_319_accuracy: 0.6653 - dense_320_accuracy: 0.7027 - dense_321_accuracy: 0.7346 - dense_322_accuracy: 0.7687 - dense_323_accuracy: 0.7821 - dense_324_accuracy: 0.7924 - val_loss: 1.7599 - val_dense_315_loss: 0.4761 - val_dense_316_loss: 0.5606 - val_dense_317_loss: 0.5839 - val_dense_318_loss: 0.6091 - val_dense_319_loss: 0.6131 - val_dense_320_loss: 0.5782 - val_dense_321_loss: 0.5078 - val_dense_322_loss: 0.4378 - val_dense_323_loss: 0.4015 - val_dense_324_loss: 0.3800 - val_dense_315_accuracy: 0.8019 - val_dense_316_accuracy: 0.7348 - val_dense_317_accuracy: 0.7185 - val_dense_318_accuracy: 0.6866 - val_dense_319_accuracy: 0.6749 - val_dense_320_accuracy: 0.6967 - val_dense_321_accuracy: 0.7543 - val_dense_322_accuracy: 0.7862 - val_dense_323_accuracy: 0.7963 - val_dense_324_accuracy: 0.8097\n",
      "2020-01-12 15:42:08.887539\t\tValid\tMAP:\t0.5993226357622158\tFPA:\t0.8019026301063235\n",
      "=========================================================\n",
      "2020-01-12 15:42:08.984119\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5993226357622158\n",
      "=========================================================\n",
      "2020-01-12 15:42:08.984395\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7459 - dense_315_loss: 0.4875 - dense_316_loss: 0.5427 - dense_317_loss: 0.5837 - dense_318_loss: 0.6060 - dense_319_loss: 0.6076 - dense_320_loss: 0.5638 - dense_321_loss: 0.5002 - dense_322_loss: 0.4347 - dense_323_loss: 0.3912 - dense_324_loss: 0.3828 - dense_315_accuracy: 0.7670 - dense_316_accuracy: 0.7274 - dense_317_accuracy: 0.6918 - dense_318_accuracy: 0.6766 - dense_319_accuracy: 0.6708 - dense_320_accuracy: 0.7027 - dense_321_accuracy: 0.7388 - dense_322_accuracy: 0.7715 - dense_323_accuracy: 0.7890 - dense_324_accuracy: 0.7884 - val_loss: 1.7951 - val_dense_315_loss: 0.4822 - val_dense_316_loss: 0.5698 - val_dense_317_loss: 0.6009 - val_dense_318_loss: 0.6242 - val_dense_319_loss: 0.6239 - val_dense_320_loss: 0.5848 - val_dense_321_loss: 0.5242 - val_dense_322_loss: 0.4529 - val_dense_323_loss: 0.4190 - val_dense_324_loss: 0.3904 - val_dense_315_accuracy: 0.8036 - val_dense_316_accuracy: 0.7264 - val_dense_317_accuracy: 0.7101 - val_dense_318_accuracy: 0.6788 - val_dense_319_accuracy: 0.6777 - val_dense_320_accuracy: 0.6900 - val_dense_321_accuracy: 0.7096 - val_dense_322_accuracy: 0.7521 - val_dense_323_accuracy: 0.7420 - val_dense_324_accuracy: 0.7661\n",
      "2020-01-12 15:42:09.437205\t\tValid\tMAP:\t0.5758465049784968\tFPA:\t0.8035814213766088\n",
      "=========================================================\n",
      "2020-01-12 15:42:09.437392\t\tStopping at epoch 1, best epoch was 0 with MAP 0.5993226357622158\n",
      "=========================================================\n",
      "2020-01-12 15:42:09.437467\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7413 - dense_315_loss: 0.4793 - dense_316_loss: 0.5455 - dense_317_loss: 0.5772 - dense_318_loss: 0.6004 - dense_319_loss: 0.6055 - dense_320_loss: 0.5568 - dense_321_loss: 0.4918 - dense_322_loss: 0.4336 - dense_323_loss: 0.3929 - dense_324_loss: 0.3735 - dense_315_accuracy: 0.7718 - dense_316_accuracy: 0.7256 - dense_317_accuracy: 0.7000 - dense_318_accuracy: 0.6726 - dense_319_accuracy: 0.6711 - dense_320_accuracy: 0.7037 - dense_321_accuracy: 0.7453 - dense_322_accuracy: 0.7672 - dense_323_accuracy: 0.7837 - dense_324_accuracy: 0.7964 - val_loss: 1.7582 - val_dense_315_loss: 0.4677 - val_dense_316_loss: 0.5658 - val_dense_317_loss: 0.5882 - val_dense_318_loss: 0.6112 - val_dense_319_loss: 0.6132 - val_dense_320_loss: 0.5742 - val_dense_321_loss: 0.5120 - val_dense_322_loss: 0.4395 - val_dense_323_loss: 0.4079 - val_dense_324_loss: 0.3847 - val_dense_315_accuracy: 0.8092 - val_dense_316_accuracy: 0.7236 - val_dense_317_accuracy: 0.7051 - val_dense_318_accuracy: 0.6866 - val_dense_319_accuracy: 0.6855 - val_dense_320_accuracy: 0.7124 - val_dense_321_accuracy: 0.7487 - val_dense_322_accuracy: 0.7890 - val_dense_323_accuracy: 0.7879 - val_dense_324_accuracy: 0.8036\n",
      "2020-01-12 15:42:09.890997\t\tValid\tMAP:\t0.5971975487173047\tFPA:\t0.8091773922775601\n",
      "=========================================================\n",
      "2020-01-12 15:42:09.891601\t\tStopping at epoch 2, best epoch was 0 with MAP 0.5993226357622158\n",
      "=========================================================\n",
      "2020-01-12 15:42:09.891844\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7415 - dense_315_loss: 0.4804 - dense_316_loss: 0.5466 - dense_317_loss: 0.5773 - dense_318_loss: 0.5966 - dense_319_loss: 0.6062 - dense_320_loss: 0.5613 - dense_321_loss: 0.4953 - dense_322_loss: 0.4332 - dense_323_loss: 0.3909 - dense_324_loss: 0.3790 - dense_315_accuracy: 0.7712 - dense_316_accuracy: 0.7216 - dense_317_accuracy: 0.6926 - dense_318_accuracy: 0.6786 - dense_319_accuracy: 0.6713 - dense_320_accuracy: 0.7053 - dense_321_accuracy: 0.7407 - dense_322_accuracy: 0.7707 - dense_323_accuracy: 0.7882 - dense_324_accuracy: 0.7914 - val_loss: 1.7804 - val_dense_315_loss: 0.4708 - val_dense_316_loss: 0.5679 - val_dense_317_loss: 0.6046 - val_dense_318_loss: 0.6206 - val_dense_319_loss: 0.6219 - val_dense_320_loss: 0.5795 - val_dense_321_loss: 0.5202 - val_dense_322_loss: 0.4493 - val_dense_323_loss: 0.4215 - val_dense_324_loss: 0.3912 - val_dense_315_accuracy: 0.8058 - val_dense_316_accuracy: 0.7375 - val_dense_317_accuracy: 0.7012 - val_dense_318_accuracy: 0.6732 - val_dense_319_accuracy: 0.6766 - val_dense_320_accuracy: 0.7107 - val_dense_321_accuracy: 0.7292 - val_dense_322_accuracy: 0.7538 - val_dense_323_accuracy: 0.7426 - val_dense_324_accuracy: 0.7633\n",
      "2020-01-12 15:42:10.331950\t\tValid\tMAP:\t0.5816245705208654\tFPA:\t0.8058198097369894\n",
      "=========================================================\n",
      "2020-01-12 15:42:10.332103\t\tStopping at epoch 3, best epoch was 0 with MAP 0.5993226357622158\n",
      "=========================================================\n",
      "2020-01-12 15:42:10.332160\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7367 - dense_315_loss: 0.4787 - dense_316_loss: 0.5405 - dense_317_loss: 0.5760 - dense_318_loss: 0.5962 - dense_319_loss: 0.6052 - dense_320_loss: 0.5580 - dense_321_loss: 0.4983 - dense_322_loss: 0.4311 - dense_323_loss: 0.3888 - dense_324_loss: 0.3773 - dense_315_accuracy: 0.7679 - dense_316_accuracy: 0.7315 - dense_317_accuracy: 0.6974 - dense_318_accuracy: 0.6796 - dense_319_accuracy: 0.6698 - dense_320_accuracy: 0.7024 - dense_321_accuracy: 0.7339 - dense_322_accuracy: 0.7637 - dense_323_accuracy: 0.7863 - dense_324_accuracy: 0.7886 - val_loss: 1.7949 - val_dense_315_loss: 0.4786 - val_dense_316_loss: 0.5702 - val_dense_317_loss: 0.6034 - val_dense_318_loss: 0.6305 - val_dense_319_loss: 0.6273 - val_dense_320_loss: 0.5820 - val_dense_321_loss: 0.5227 - val_dense_322_loss: 0.4501 - val_dense_323_loss: 0.4205 - val_dense_324_loss: 0.3909 - val_dense_315_accuracy: 0.7952 - val_dense_316_accuracy: 0.7247 - val_dense_317_accuracy: 0.6973 - val_dense_318_accuracy: 0.6486 - val_dense_319_accuracy: 0.6581 - val_dense_320_accuracy: 0.7040 - val_dense_321_accuracy: 0.7107 - val_dense_322_accuracy: 0.7426 - val_dense_323_accuracy: 0.7409 - val_dense_324_accuracy: 0.7577\n",
      "2020-01-12 15:42:10.782710\t\tValid\tMAP:\t0.5696375218308432\tFPA:\t0.7951874650251819\n",
      "=========================================================\n",
      "2020-01-12 15:42:10.783372\t\tStopping at epoch 4, best epoch was 0 with MAP 0.5993226357622158\n",
      "=========================================================\n",
      "2020-01-12 15:42:10.783653\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7321 - dense_315_loss: 0.4767 - dense_316_loss: 0.5436 - dense_317_loss: 0.5759 - dense_318_loss: 0.6013 - dense_319_loss: 0.6029 - dense_320_loss: 0.5585 - dense_321_loss: 0.4973 - dense_322_loss: 0.4262 - dense_323_loss: 0.3906 - dense_324_loss: 0.3769 - dense_315_accuracy: 0.7719 - dense_316_accuracy: 0.7210 - dense_317_accuracy: 0.6993 - dense_318_accuracy: 0.6768 - dense_319_accuracy: 0.6782 - dense_320_accuracy: 0.7032 - dense_321_accuracy: 0.7407 - dense_322_accuracy: 0.7728 - dense_323_accuracy: 0.7889 - dense_324_accuracy: 0.7936 - val_loss: 1.8005 - val_dense_315_loss: 0.4852 - val_dense_316_loss: 0.5723 - val_dense_317_loss: 0.6013 - val_dense_318_loss: 0.6261 - val_dense_319_loss: 0.6260 - val_dense_320_loss: 0.5784 - val_dense_321_loss: 0.5210 - val_dense_322_loss: 0.4510 - val_dense_323_loss: 0.4319 - val_dense_324_loss: 0.3978 - val_dense_315_accuracy: 0.8030 - val_dense_316_accuracy: 0.7364 - val_dense_317_accuracy: 0.7124 - val_dense_318_accuracy: 0.6749 - val_dense_319_accuracy: 0.6721 - val_dense_320_accuracy: 0.7006 - val_dense_321_accuracy: 0.7168 - val_dense_322_accuracy: 0.7443 - val_dense_323_accuracy: 0.7230 - val_dense_324_accuracy: 0.7431\n",
      "2020-01-12 15:42:11.249273\t\tValid\tMAP:\t0.5749957111126901\tFPA:\t0.8030218242865137\n",
      "=========================================================\n",
      "2020-01-12 15:42:11.337102\t\tAverage best MAP over all folds:\t\t0.5337366772461345...\n",
      "=========================================================\n",
      "2020-01-12 15:42:11.339038\t----- FOLD 3 -----\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7316 - dense_315_loss: 0.4714 - dense_316_loss: 0.5441 - dense_317_loss: 0.5747 - dense_318_loss: 0.5942 - dense_319_loss: 0.6037 - dense_320_loss: 0.5625 - dense_321_loss: 0.5007 - dense_322_loss: 0.4288 - dense_323_loss: 0.3899 - dense_324_loss: 0.3763 - dense_315_accuracy: 0.7731 - dense_316_accuracy: 0.7224 - dense_317_accuracy: 0.6922 - dense_318_accuracy: 0.6820 - dense_319_accuracy: 0.6764 - dense_320_accuracy: 0.7049 - dense_321_accuracy: 0.7360 - dense_322_accuracy: 0.7705 - dense_323_accuracy: 0.7875 - dense_324_accuracy: 0.7935 - val_loss: 1.7107 - val_dense_315_loss: 0.4623 - val_dense_316_loss: 0.5235 - val_dense_317_loss: 0.5728 - val_dense_318_loss: 0.5981 - val_dense_319_loss: 0.6084 - val_dense_320_loss: 0.5718 - val_dense_321_loss: 0.4988 - val_dense_322_loss: 0.4331 - val_dense_323_loss: 0.4129 - val_dense_324_loss: 0.3769 - val_dense_315_accuracy: 0.7952 - val_dense_316_accuracy: 0.7661 - val_dense_317_accuracy: 0.7202 - val_dense_318_accuracy: 0.6911 - val_dense_319_accuracy: 0.6754 - val_dense_320_accuracy: 0.7118 - val_dense_321_accuracy: 0.7459 - val_dense_322_accuracy: 0.7795 - val_dense_323_accuracy: 0.7364 - val_dense_324_accuracy: 0.7767\n",
      "2020-01-12 15:42:11.799238\t\tValid\tMAP:\t0.5963100365796588\tFPA:\t0.7951874650251819\n",
      "=========================================================\n",
      "2020-01-12 15:42:11.901192\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5963100365796588\n",
      "=========================================================\n",
      "2020-01-12 15:42:11.901290\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7175 - dense_315_loss: 0.4647 - dense_316_loss: 0.5401 - dense_317_loss: 0.5687 - dense_318_loss: 0.5966 - dense_319_loss: 0.6016 - dense_320_loss: 0.5682 - dense_321_loss: 0.4980 - dense_322_loss: 0.4299 - dense_323_loss: 0.3937 - dense_324_loss: 0.3808 - dense_315_accuracy: 0.7771 - dense_316_accuracy: 0.7227 - dense_317_accuracy: 0.7071 - dense_318_accuracy: 0.6843 - dense_319_accuracy: 0.6778 - dense_320_accuracy: 0.7059 - dense_321_accuracy: 0.7337 - dense_322_accuracy: 0.7700 - dense_323_accuracy: 0.7883 - dense_324_accuracy: 0.7912 - val_loss: 1.7582 - val_dense_315_loss: 0.4795 - val_dense_316_loss: 0.5476 - val_dense_317_loss: 0.5823 - val_dense_318_loss: 0.6074 - val_dense_319_loss: 0.6128 - val_dense_320_loss: 0.5761 - val_dense_321_loss: 0.5059 - val_dense_322_loss: 0.4529 - val_dense_323_loss: 0.4383 - val_dense_324_loss: 0.3853 - val_dense_315_accuracy: 0.7957 - val_dense_316_accuracy: 0.7487 - val_dense_317_accuracy: 0.7168 - val_dense_318_accuracy: 0.6861 - val_dense_319_accuracy: 0.6766 - val_dense_320_accuracy: 0.6933 - val_dense_321_accuracy: 0.7219 - val_dense_322_accuracy: 0.7292 - val_dense_323_accuracy: 0.7107 - val_dense_324_accuracy: 0.7644\n",
      "2020-01-12 15:42:12.345427\t\tValid\tMAP:\t0.5776693458840148\tFPA:\t0.795747062115277\n",
      "=========================================================\n",
      "2020-01-12 15:42:12.346201\t\tStopping at epoch 1, best epoch was 0 with MAP 0.5963100365796588\n",
      "=========================================================\n",
      "2020-01-12 15:42:12.346456\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7123 - dense_315_loss: 0.4667 - dense_316_loss: 0.5413 - dense_317_loss: 0.5743 - dense_318_loss: 0.5945 - dense_319_loss: 0.6022 - dense_320_loss: 0.5532 - dense_321_loss: 0.4900 - dense_322_loss: 0.4207 - dense_323_loss: 0.3905 - dense_324_loss: 0.3757 - dense_315_accuracy: 0.7767 - dense_316_accuracy: 0.7256 - dense_317_accuracy: 0.6990 - dense_318_accuracy: 0.6814 - dense_319_accuracy: 0.6738 - dense_320_accuracy: 0.7119 - dense_321_accuracy: 0.7431 - dense_322_accuracy: 0.7785 - dense_323_accuracy: 0.7893 - dense_324_accuracy: 0.7926 - val_loss: 1.7858 - val_dense_315_loss: 0.4850 - val_dense_316_loss: 0.5570 - val_dense_317_loss: 0.5907 - val_dense_318_loss: 0.6177 - val_dense_319_loss: 0.6194 - val_dense_320_loss: 0.5802 - val_dense_321_loss: 0.5090 - val_dense_322_loss: 0.4747 - val_dense_323_loss: 0.4635 - val_dense_324_loss: 0.4105 - val_dense_315_accuracy: 0.7952 - val_dense_316_accuracy: 0.7504 - val_dense_317_accuracy: 0.7152 - val_dense_318_accuracy: 0.6771 - val_dense_319_accuracy: 0.6782 - val_dense_320_accuracy: 0.6782 - val_dense_321_accuracy: 0.7079 - val_dense_322_accuracy: 0.6877 - val_dense_323_accuracy: 0.7029 - val_dense_324_accuracy: 0.7359\n",
      "2020-01-12 15:42:12.794650\t\tValid\tMAP:\t0.5677042956760731\tFPA:\t0.7951874650251819\n",
      "=========================================================\n",
      "2020-01-12 15:42:12.794820\t\tStopping at epoch 2, best epoch was 0 with MAP 0.5963100365796588\n",
      "=========================================================\n",
      "2020-01-12 15:42:12.795464\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7183 - dense_315_loss: 0.4663 - dense_316_loss: 0.5331 - dense_317_loss: 0.5717 - dense_318_loss: 0.5992 - dense_319_loss: 0.6044 - dense_320_loss: 0.5566 - dense_321_loss: 0.4914 - dense_322_loss: 0.4292 - dense_323_loss: 0.3885 - dense_324_loss: 0.3719 - dense_315_accuracy: 0.7793 - dense_316_accuracy: 0.7356 - dense_317_accuracy: 0.7032 - dense_318_accuracy: 0.6799 - dense_319_accuracy: 0.6754 - dense_320_accuracy: 0.7087 - dense_321_accuracy: 0.7428 - dense_322_accuracy: 0.7736 - dense_323_accuracy: 0.7876 - dense_324_accuracy: 0.7921 - val_loss: 1.7769 - val_dense_315_loss: 0.4780 - val_dense_316_loss: 0.5444 - val_dense_317_loss: 0.5864 - val_dense_318_loss: 0.6178 - val_dense_319_loss: 0.6233 - val_dense_320_loss: 0.5866 - val_dense_321_loss: 0.5140 - val_dense_322_loss: 0.4927 - val_dense_323_loss: 0.4852 - val_dense_324_loss: 0.4247 - val_dense_315_accuracy: 0.8013 - val_dense_316_accuracy: 0.7555 - val_dense_317_accuracy: 0.7297 - val_dense_318_accuracy: 0.6698 - val_dense_319_accuracy: 0.6754 - val_dense_320_accuracy: 0.6620 - val_dense_321_accuracy: 0.6900 - val_dense_322_accuracy: 0.6788 - val_dense_323_accuracy: 0.6989 - val_dense_324_accuracy: 0.7297\n",
      "2020-01-12 15:42:13.243397\t\tValid\tMAP:\t0.5660296033945516\tFPA:\t0.8013430330162283\n",
      "=========================================================\n",
      "2020-01-12 15:42:13.243617\t\tStopping at epoch 3, best epoch was 0 with MAP 0.5963100365796588\n",
      "=========================================================\n",
      "2020-01-12 15:42:13.244443\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7098 - dense_315_loss: 0.4700 - dense_316_loss: 0.5355 - dense_317_loss: 0.5667 - dense_318_loss: 0.5905 - dense_319_loss: 0.5950 - dense_320_loss: 0.5552 - dense_321_loss: 0.4899 - dense_322_loss: 0.4259 - dense_323_loss: 0.3884 - dense_324_loss: 0.3741 - dense_315_accuracy: 0.7756 - dense_316_accuracy: 0.7302 - dense_317_accuracy: 0.6983 - dense_318_accuracy: 0.6870 - dense_319_accuracy: 0.6766 - dense_320_accuracy: 0.7118 - dense_321_accuracy: 0.7372 - dense_322_accuracy: 0.7813 - dense_323_accuracy: 0.7959 - dense_324_accuracy: 0.7985 - val_loss: 1.7831 - val_dense_315_loss: 0.4876 - val_dense_316_loss: 0.5528 - val_dense_317_loss: 0.5835 - val_dense_318_loss: 0.6111 - val_dense_319_loss: 0.6188 - val_dense_320_loss: 0.5850 - val_dense_321_loss: 0.5104 - val_dense_322_loss: 0.4806 - val_dense_323_loss: 0.4689 - val_dense_324_loss: 0.4217 - val_dense_315_accuracy: 0.7778 - val_dense_316_accuracy: 0.7532 - val_dense_317_accuracy: 0.7252 - val_dense_318_accuracy: 0.6827 - val_dense_319_accuracy: 0.6855 - val_dense_320_accuracy: 0.6721 - val_dense_321_accuracy: 0.7012 - val_dense_322_accuracy: 0.6844 - val_dense_323_accuracy: 0.7029 - val_dense_324_accuracy: 0.7314\n",
      "2020-01-12 15:42:13.692340\t\tValid\tMAP:\t0.56433155579482\tFPA:\t0.7778399552322328\n",
      "=========================================================\n",
      "2020-01-12 15:42:13.692499\t\tStopping at epoch 4, best epoch was 0 with MAP 0.5963100365796588\n",
      "=========================================================\n",
      "2020-01-12 15:42:13.693055\t\tPredicting for test set...\n",
      "Train on 7147 samples, validate on 1787 samples\n",
      "7147/7147 - 0s - loss: 1.7114 - dense_315_loss: 0.4665 - dense_316_loss: 0.5356 - dense_317_loss: 0.5749 - dense_318_loss: 0.5904 - dense_319_loss: 0.5962 - dense_320_loss: 0.5543 - dense_321_loss: 0.4959 - dense_322_loss: 0.4256 - dense_323_loss: 0.3883 - dense_324_loss: 0.3737 - dense_315_accuracy: 0.7747 - dense_316_accuracy: 0.7297 - dense_317_accuracy: 0.6982 - dense_318_accuracy: 0.6845 - dense_319_accuracy: 0.6843 - dense_320_accuracy: 0.7097 - dense_321_accuracy: 0.7405 - dense_322_accuracy: 0.7712 - dense_323_accuracy: 0.7882 - dense_324_accuracy: 0.7945 - val_loss: 1.7834 - val_dense_315_loss: 0.4863 - val_dense_316_loss: 0.5471 - val_dense_317_loss: 0.5866 - val_dense_318_loss: 0.6122 - val_dense_319_loss: 0.6219 - val_dense_320_loss: 0.5856 - val_dense_321_loss: 0.5145 - val_dense_322_loss: 0.4858 - val_dense_323_loss: 0.4784 - val_dense_324_loss: 0.4283 - val_dense_315_accuracy: 0.7851 - val_dense_316_accuracy: 0.7543 - val_dense_317_accuracy: 0.7213 - val_dense_318_accuracy: 0.6738 - val_dense_319_accuracy: 0.6710 - val_dense_320_accuracy: 0.6704 - val_dense_321_accuracy: 0.6905 - val_dense_322_accuracy: 0.6844 - val_dense_323_accuracy: 0.7001 - val_dense_324_accuracy: 0.7308\n",
      "2020-01-12 15:42:14.132120\t\tValid\tMAP:\t0.562392563068877\tFPA:\t0.7851147174034695\n",
      "=========================================================\n",
      "2020-01-12 15:42:14.222018\t\tAverage best MAP over all folds:\t\t0.5424274215980129...\n",
      "=========================================================\n",
      "2020-01-12 15:42:14.223803\t----- FOLD 4 -----\n",
      "Train on 7148 samples, validate on 1786 samples\n",
      "7148/7148 - 12s - loss: 1.7037 - dense_315_loss: 0.4674 - dense_316_loss: 0.5383 - dense_317_loss: 0.5603 - dense_318_loss: 0.5909 - dense_319_loss: 0.5977 - dense_320_loss: 0.5548 - dense_321_loss: 0.4927 - dense_322_loss: 0.4263 - dense_323_loss: 0.3863 - dense_324_loss: 0.3834 - dense_315_accuracy: 0.7784 - dense_316_accuracy: 0.7287 - dense_317_accuracy: 0.7078 - dense_318_accuracy: 0.6806 - dense_319_accuracy: 0.6785 - dense_320_accuracy: 0.7033 - dense_321_accuracy: 0.7447 - dense_322_accuracy: 0.7753 - dense_323_accuracy: 0.7939 - dense_324_accuracy: 0.7949 - val_loss: 1.7630 - val_dense_315_loss: 0.4714 - val_dense_316_loss: 0.5372 - val_dense_317_loss: 0.5823 - val_dense_318_loss: 0.6211 - val_dense_319_loss: 0.6196 - val_dense_320_loss: 0.5777 - val_dense_321_loss: 0.5101 - val_dense_322_loss: 0.4945 - val_dense_323_loss: 0.4801 - val_dense_324_loss: 0.4548 - val_dense_315_accuracy: 0.8141 - val_dense_316_accuracy: 0.7699 - val_dense_317_accuracy: 0.7195 - val_dense_318_accuracy: 0.6568 - val_dense_319_accuracy: 0.6545 - val_dense_320_accuracy: 0.6842 - val_dense_321_accuracy: 0.7066 - val_dense_322_accuracy: 0.6898 - val_dense_323_accuracy: 0.7128 - val_dense_324_accuracy: 0.7240\n",
      "2020-01-12 15:42:26.735811\t\tValid\tMAP:\t0.5750365965255083\tFPA:\t0.8141097424412094\n",
      "=========================================================\n",
      "2020-01-12 15:42:27.067356\t\tStopping at epoch 0, best epoch was 0 with MAP 0.5750365965255083\n",
      "=========================================================\n",
      "2020-01-12 15:42:27.067702\t\tPredicting for test set...\n",
      "Train on 7148 samples, validate on 1786 samples\n",
      "7148/7148 - 0s - loss: 1.6884 - dense_315_loss: 0.4650 - dense_316_loss: 0.5315 - dense_317_loss: 0.5592 - dense_318_loss: 0.5840 - dense_319_loss: 0.5923 - dense_320_loss: 0.5570 - dense_321_loss: 0.4933 - dense_322_loss: 0.4213 - dense_323_loss: 0.3830 - dense_324_loss: 0.3726 - dense_315_accuracy: 0.7825 - dense_316_accuracy: 0.7366 - dense_317_accuracy: 0.7122 - dense_318_accuracy: 0.6897 - dense_319_accuracy: 0.6814 - dense_320_accuracy: 0.7101 - dense_321_accuracy: 0.7430 - dense_322_accuracy: 0.7757 - dense_323_accuracy: 0.7956 - dense_324_accuracy: 0.7973 - val_loss: 1.7402 - val_dense_315_loss: 0.4623 - val_dense_316_loss: 0.5303 - val_dense_317_loss: 0.5734 - val_dense_318_loss: 0.6098 - val_dense_319_loss: 0.6067 - val_dense_320_loss: 0.5778 - val_dense_321_loss: 0.5165 - val_dense_322_loss: 0.4978 - val_dense_323_loss: 0.4795 - val_dense_324_loss: 0.4639 - val_dense_315_accuracy: 0.7912 - val_dense_316_accuracy: 0.7783 - val_dense_317_accuracy: 0.7228 - val_dense_318_accuracy: 0.6697 - val_dense_319_accuracy: 0.6909 - val_dense_320_accuracy: 0.6892 - val_dense_321_accuracy: 0.7027 - val_dense_322_accuracy: 0.6881 - val_dense_323_accuracy: 0.7122 - val_dense_324_accuracy: 0.7212\n",
      "2020-01-12 15:42:27.528123\t\tValid\tMAP:\t0.5752706679806012\tFPA:\t0.7911534154535275\n",
      "=========================================================\n",
      "2020-01-12 15:42:28.005856\t\tStopping at epoch 1, best epoch was 1 with MAP 0.5752706679806012\n",
      "=========================================================\n",
      "2020-01-12 15:42:28.006058\t\tPredicting for test set...\n",
      "Train on 7148 samples, validate on 1786 samples\n",
      "7148/7148 - 0s - loss: 1.6608 - dense_315_loss: 0.4474 - dense_316_loss: 0.5258 - dense_317_loss: 0.5541 - dense_318_loss: 0.5782 - dense_319_loss: 0.5882 - dense_320_loss: 0.5440 - dense_321_loss: 0.4821 - dense_322_loss: 0.4144 - dense_323_loss: 0.3810 - dense_324_loss: 0.3642 - dense_315_accuracy: 0.7864 - dense_316_accuracy: 0.7374 - dense_317_accuracy: 0.7142 - dense_318_accuracy: 0.6946 - dense_319_accuracy: 0.6901 - dense_320_accuracy: 0.7226 - dense_321_accuracy: 0.7458 - dense_322_accuracy: 0.7841 - dense_323_accuracy: 0.7970 - dense_324_accuracy: 0.7955 - val_loss: 1.7670 - val_dense_315_loss: 0.4769 - val_dense_316_loss: 0.5299 - val_dense_317_loss: 0.5822 - val_dense_318_loss: 0.6075 - val_dense_319_loss: 0.6056 - val_dense_320_loss: 0.5820 - val_dense_321_loss: 0.5244 - val_dense_322_loss: 0.5235 - val_dense_323_loss: 0.5156 - val_dense_324_loss: 0.4928 - val_dense_315_accuracy: 0.7749 - val_dense_316_accuracy: 0.7660 - val_dense_317_accuracy: 0.7245 - val_dense_318_accuracy: 0.6848 - val_dense_319_accuracy: 0.6887 - val_dense_320_accuracy: 0.6809 - val_dense_321_accuracy: 0.6797 - val_dense_322_accuracy: 0.6775 - val_dense_323_accuracy: 0.7060 - val_dense_324_accuracy: 0.7172\n",
      "2020-01-12 15:42:28.458894\t\tValid\tMAP:\t0.5662985940767993\tFPA:\t0.77491601343785\n",
      "=========================================================\n",
      "2020-01-12 15:42:28.459502\t\tStopping at epoch 2, best epoch was 1 with MAP 0.5752706679806012\n",
      "=========================================================\n",
      "2020-01-12 15:42:28.459805\t\tPredicting for test set...\n",
      "Train on 7148 samples, validate on 1786 samples\n",
      "7148/7148 - 0s - loss: 1.6540 - dense_315_loss: 0.4410 - dense_316_loss: 0.5199 - dense_317_loss: 0.5521 - dense_318_loss: 0.5777 - dense_319_loss: 0.5880 - dense_320_loss: 0.5420 - dense_321_loss: 0.4863 - dense_322_loss: 0.4181 - dense_323_loss: 0.3802 - dense_324_loss: 0.3698 - dense_315_accuracy: 0.7911 - dense_316_accuracy: 0.7403 - dense_317_accuracy: 0.7175 - dense_318_accuracy: 0.6905 - dense_319_accuracy: 0.6883 - dense_320_accuracy: 0.7222 - dense_321_accuracy: 0.7489 - dense_322_accuracy: 0.7834 - dense_323_accuracy: 0.7978 - dense_324_accuracy: 0.7966 - val_loss: 1.7345 - val_dense_315_loss: 0.4634 - val_dense_316_loss: 0.5292 - val_dense_317_loss: 0.5759 - val_dense_318_loss: 0.6016 - val_dense_319_loss: 0.6047 - val_dense_320_loss: 0.5683 - val_dense_321_loss: 0.5072 - val_dense_322_loss: 0.4890 - val_dense_323_loss: 0.4929 - val_dense_324_loss: 0.4518 - val_dense_315_accuracy: 0.8001 - val_dense_316_accuracy: 0.7632 - val_dense_317_accuracy: 0.7262 - val_dense_318_accuracy: 0.6904 - val_dense_319_accuracy: 0.6932 - val_dense_320_accuracy: 0.7004 - val_dense_321_accuracy: 0.7189 - val_dense_322_accuracy: 0.6909 - val_dense_323_accuracy: 0.7094 - val_dense_324_accuracy: 0.7200\n",
      "2020-01-12 15:42:28.905164\t\tValid\tMAP:\t0.5805923531071007\tFPA:\t0.8001119820828667\n",
      "=========================================================\n",
      "2020-01-12 15:42:29.329321\t\tStopping at epoch 3, best epoch was 3 with MAP 0.5805923531071007\n",
      "=========================================================\n",
      "2020-01-12 15:42:29.329469\t\tPredicting for test set...\n",
      "Train on 7148 samples, validate on 1786 samples\n",
      "7148/7148 - 0s - loss: 1.6322 - dense_315_loss: 0.4341 - dense_316_loss: 0.5163 - dense_317_loss: 0.5454 - dense_318_loss: 0.5714 - dense_319_loss: 0.5828 - dense_320_loss: 0.5377 - dense_321_loss: 0.4783 - dense_322_loss: 0.4144 - dense_323_loss: 0.3753 - dense_324_loss: 0.3677 - dense_315_accuracy: 0.7987 - dense_316_accuracy: 0.7452 - dense_317_accuracy: 0.7265 - dense_318_accuracy: 0.7045 - dense_319_accuracy: 0.6917 - dense_320_accuracy: 0.7238 - dense_321_accuracy: 0.7494 - dense_322_accuracy: 0.7883 - dense_323_accuracy: 0.8051 - dense_324_accuracy: 0.8009 - val_loss: 1.7698 - val_dense_315_loss: 0.4682 - val_dense_316_loss: 0.5401 - val_dense_317_loss: 0.5771 - val_dense_318_loss: 0.6065 - val_dense_319_loss: 0.6147 - val_dense_320_loss: 0.5756 - val_dense_321_loss: 0.5300 - val_dense_322_loss: 0.5272 - val_dense_323_loss: 0.5605 - val_dense_324_loss: 0.5266 - val_dense_315_accuracy: 0.8012 - val_dense_316_accuracy: 0.7587 - val_dense_317_accuracy: 0.7172 - val_dense_318_accuracy: 0.6797 - val_dense_319_accuracy: 0.6820 - val_dense_320_accuracy: 0.6999 - val_dense_321_accuracy: 0.6792 - val_dense_322_accuracy: 0.6769 - val_dense_323_accuracy: 0.7038 - val_dense_324_accuracy: 0.7161\n",
      "2020-01-12 15:42:29.788580\t\tValid\tMAP:\t0.5693948572284876\tFPA:\t0.8012318029115342\n",
      "=========================================================\n",
      "2020-01-12 15:42:29.789065\t\tStopping at epoch 4, best epoch was 3 with MAP 0.5805923531071007\n",
      "=========================================================\n",
      "2020-01-12 15:42:29.789329\t\tPredicting for test set...\n",
      "Train on 7148 samples, validate on 1786 samples\n",
      "7148/7148 - 0s - loss: 1.6131 - dense_315_loss: 0.4236 - dense_316_loss: 0.5064 - dense_317_loss: 0.5390 - dense_318_loss: 0.5659 - dense_319_loss: 0.5783 - dense_320_loss: 0.5331 - dense_321_loss: 0.4703 - dense_322_loss: 0.4102 - dense_323_loss: 0.3781 - dense_324_loss: 0.3653 - dense_315_accuracy: 0.8071 - dense_316_accuracy: 0.7483 - dense_317_accuracy: 0.7325 - dense_318_accuracy: 0.7090 - dense_319_accuracy: 0.6952 - dense_320_accuracy: 0.7255 - dense_321_accuracy: 0.7531 - dense_322_accuracy: 0.7883 - dense_323_accuracy: 0.7949 - dense_324_accuracy: 0.8048 - val_loss: 1.7426 - val_dense_315_loss: 0.4446 - val_dense_316_loss: 0.5272 - val_dense_317_loss: 0.5776 - val_dense_318_loss: 0.6087 - val_dense_319_loss: 0.6153 - val_dense_320_loss: 0.5749 - val_dense_321_loss: 0.5464 - val_dense_322_loss: 0.5340 - val_dense_323_loss: 0.5590 - val_dense_324_loss: 0.5291 - val_dense_315_accuracy: 0.8108 - val_dense_316_accuracy: 0.7716 - val_dense_317_accuracy: 0.7021 - val_dense_318_accuracy: 0.6680 - val_dense_319_accuracy: 0.6669 - val_dense_320_accuracy: 0.6926 - val_dense_321_accuracy: 0.6579 - val_dense_322_accuracy: 0.6769 - val_dense_323_accuracy: 0.7027 - val_dense_324_accuracy: 0.7172\n",
      "2020-01-12 15:42:30.232913\t\tValid\tMAP:\t0.5676062493140431\tFPA:\t0.8107502799552072\n",
      "=========================================================\n",
      "2020-01-12 15:42:30.233614\t\tStopping at epoch 5, best epoch was 3 with MAP 0.5805923531071007\n",
      "=========================================================\n",
      "2020-01-12 15:42:30.233985\t\tPredicting for test set...\n",
      "Train on 7148 samples, validate on 1786 samples\n",
      "7148/7148 - 0s - loss: 1.6202 - dense_315_loss: 0.4319 - dense_316_loss: 0.5051 - dense_317_loss: 0.5429 - dense_318_loss: 0.5751 - dense_319_loss: 0.5878 - dense_320_loss: 0.5361 - dense_321_loss: 0.4736 - dense_322_loss: 0.4123 - dense_323_loss: 0.3743 - dense_324_loss: 0.3656 - dense_315_accuracy: 0.7943 - dense_316_accuracy: 0.7520 - dense_317_accuracy: 0.7278 - dense_318_accuracy: 0.6956 - dense_319_accuracy: 0.6904 - dense_320_accuracy: 0.7224 - dense_321_accuracy: 0.7546 - dense_322_accuracy: 0.7822 - dense_323_accuracy: 0.7992 - dense_324_accuracy: 0.7992 - val_loss: 1.8501 - val_dense_315_loss: 0.4727 - val_dense_316_loss: 0.5517 - val_dense_317_loss: 0.5965 - val_dense_318_loss: 0.6388 - val_dense_319_loss: 0.6387 - val_dense_320_loss: 0.6054 - val_dense_321_loss: 0.6030 - val_dense_322_loss: 0.6216 - val_dense_323_loss: 0.6571 - val_dense_324_loss: 0.6400 - val_dense_315_accuracy: 0.7951 - val_dense_316_accuracy: 0.7648 - val_dense_317_accuracy: 0.7055 - val_dense_318_accuracy: 0.6265 - val_dense_319_accuracy: 0.6159 - val_dense_320_accuracy: 0.6316 - val_dense_321_accuracy: 0.6232 - val_dense_322_accuracy: 0.6635 - val_dense_323_accuracy: 0.7021 - val_dense_324_accuracy: 0.7156\n",
      "2020-01-12 15:42:30.674771\t\tValid\tMAP:\t0.5399140058225598\tFPA:\t0.7950727883538634\n",
      "=========================================================\n",
      "2020-01-12 15:42:30.675578\t\tStopping at epoch 6, best epoch was 3 with MAP 0.5805923531071007\n",
      "=========================================================\n",
      "2020-01-12 15:42:30.675942\t\tPredicting for test set...\n",
      "Train on 7148 samples, validate on 1786 samples\n",
      "7148/7148 - 0s - loss: 1.6219 - dense_315_loss: 0.4297 - dense_316_loss: 0.5090 - dense_317_loss: 0.5398 - dense_318_loss: 0.5667 - dense_319_loss: 0.5782 - dense_320_loss: 0.5335 - dense_321_loss: 0.4761 - dense_322_loss: 0.4093 - dense_323_loss: 0.3735 - dense_324_loss: 0.3625 - dense_315_accuracy: 0.8022 - dense_316_accuracy: 0.7514 - dense_317_accuracy: 0.7269 - dense_318_accuracy: 0.7101 - dense_319_accuracy: 0.7019 - dense_320_accuracy: 0.7269 - dense_321_accuracy: 0.7521 - dense_322_accuracy: 0.7879 - dense_323_accuracy: 0.7994 - dense_324_accuracy: 0.8033 - val_loss: 1.7439 - val_dense_315_loss: 0.4550 - val_dense_316_loss: 0.5300 - val_dense_317_loss: 0.5756 - val_dense_318_loss: 0.6117 - val_dense_319_loss: 0.6137 - val_dense_320_loss: 0.5739 - val_dense_321_loss: 0.5338 - val_dense_322_loss: 0.5083 - val_dense_323_loss: 0.5200 - val_dense_324_loss: 0.4871 - val_dense_315_accuracy: 0.7884 - val_dense_316_accuracy: 0.7626 - val_dense_317_accuracy: 0.7139 - val_dense_318_accuracy: 0.6579 - val_dense_319_accuracy: 0.6702 - val_dense_320_accuracy: 0.6909 - val_dense_321_accuracy: 0.6747 - val_dense_322_accuracy: 0.6909 - val_dense_323_accuracy: 0.7066 - val_dense_324_accuracy: 0.7184\n",
      "2020-01-12 15:42:31.109989\t\tValid\tMAP:\t0.564287265801141\tFPA:\t0.7883538633818589\n",
      "=========================================================\n",
      "2020-01-12 15:42:31.110455\t\tStopping at epoch 7, best epoch was 3 with MAP 0.5805923531071007\n",
      "=========================================================\n",
      "2020-01-12 15:42:31.110579\t\tPredicting for test set...\n",
      "Train on 7148 samples, validate on 1786 samples\n",
      "7148/7148 - 0s - loss: 1.6273 - dense_315_loss: 0.4372 - dense_316_loss: 0.5087 - dense_317_loss: 0.5444 - dense_318_loss: 0.5706 - dense_319_loss: 0.5805 - dense_320_loss: 0.5344 - dense_321_loss: 0.4766 - dense_322_loss: 0.4079 - dense_323_loss: 0.3754 - dense_324_loss: 0.3671 - dense_315_accuracy: 0.7969 - dense_316_accuracy: 0.7504 - dense_317_accuracy: 0.7206 - dense_318_accuracy: 0.7023 - dense_319_accuracy: 0.6922 - dense_320_accuracy: 0.7296 - dense_321_accuracy: 0.7555 - dense_322_accuracy: 0.7899 - dense_323_accuracy: 0.8011 - dense_324_accuracy: 0.8060 - val_loss: 1.7917 - val_dense_315_loss: 0.4794 - val_dense_316_loss: 0.5469 - val_dense_317_loss: 0.5829 - val_dense_318_loss: 0.6141 - val_dense_319_loss: 0.6229 - val_dense_320_loss: 0.5798 - val_dense_321_loss: 0.5366 - val_dense_322_loss: 0.5203 - val_dense_323_loss: 0.5410 - val_dense_324_loss: 0.5184 - val_dense_315_accuracy: 0.7912 - val_dense_316_accuracy: 0.7654 - val_dense_317_accuracy: 0.7150 - val_dense_318_accuracy: 0.6781 - val_dense_319_accuracy: 0.6680 - val_dense_320_accuracy: 0.6948 - val_dense_321_accuracy: 0.6579 - val_dense_322_accuracy: 0.6736 - val_dense_323_accuracy: 0.7021 - val_dense_324_accuracy: 0.7161\n",
      "2020-01-12 15:42:31.562124\t\tValid\tMAP:\t0.5622419190707266\tFPA:\t0.7911534154535275\n",
      "=========================================================\n",
      "2020-01-12 15:42:31.656507\t\tAverage best MAP over all folds:\t\t0.5489983380806972...\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, utils, Model, Input\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "\n",
    "# k-fold Cross-validation grouped on sessions\n",
    "k = 5\n",
    "n_epochs = 50\n",
    "test_predictions = []\n",
    "all_maps = []\n",
    "\n",
    "# Generate model\n",
    "model = generate_model(train_history, train_future)\n",
    "plot_model(model)\n",
    "for fold_id, (train_idx, valid_idx) in enumerate(KFold(n_splits = k).split(train_history)):\n",
    "    print('{0}\\t----- FOLD {1} -----'.format(datetime.datetime.now(),fold_id))\n",
    "    # Filter out training and testing data\n",
    "    h_train = train_history[train_idx]\n",
    "    h_valid = train_history[valid_idx]\n",
    "    f_train = train_future[train_idx]\n",
    "    f_valid = train_future[valid_idx]\n",
    "    l_train = train_labels[train_idx]\n",
    "    l_valid = train_labels[valid_idx]\n",
    "    s_train = train_session_len[train_idx]\n",
    "    s_valid = train_session_len[valid_idx]\n",
    "\n",
    "    # Loss weights\n",
    "    weights = np.asarray([(1 / val + sum((1 / (2*n)) for n in range(val + 1,11))) for val in range(1,11)])\n",
    "    weights /= weights.max()\n",
    "    \n",
    "    # Early stopping\n",
    "    best_map = .0\n",
    "    best_weights = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    # For every epoch\n",
    "    for epoch_id in range(n_epochs):\n",
    "        model.fit([h_train, f_train], [l_train[:,i] for i in range(10)],\n",
    "                  validation_data = ([h_valid, f_valid], [l_valid[:,i] for i in range(10)]),\n",
    "                  batch_size = 2048, epochs = 1, verbose = 2)\n",
    "        p_valid = model.predict([h_valid, f_valid], batch_size = 4096)\n",
    "        MAP, FPA = evaluation_MAP_FPA(s_valid, l_valid, np.swapaxes(np.round(p_valid),0,1))\n",
    "        print('{0}\\t\\tValid\\tMAP:\\t{1}\\tFPA:\\t{2}'.format(datetime.datetime.now(),MAP, FPA))\n",
    "        if MAP > best_map:\n",
    "            best_map = MAP\n",
    "            best_epoch = epoch_id\n",
    "            best_weights = model.get_weights()\n",
    "            model.save_weights(modelpath + '/model_weights_epoch_{}.h5'.format(epoch_id+1))\n",
    "        elif epoch_id - best_epoch >= 5:\n",
    "            break\n",
    "\n",
    "        print('=========================================================')\n",
    "        print('{0}\\t\\tStopping at epoch {1}, best epoch was {2} with MAP {3}'.format(datetime.datetime.now(),epoch_id, best_epoch, best_map))\n",
    "        print('=========================================================')\n",
    "        all_maps.append(best_map)\n",
    "\n",
    "        print('{0}\\t\\tPredicting for test set...'.format(datetime.datetime.now()))\n",
    "        # Reload best weights\n",
    "        model.set_weights(best_weights)\n",
    "\n",
    "    # Predict for test set\n",
    "    p_test = model.predict([test_history, test_future], batch_size = 4096)\n",
    "    test_predictions.append(np.swapaxes(p_test,0,1))\n",
    "\n",
    "    print('=========================================================')\n",
    "    print('{0}\\t\\tAverage best MAP over all folds:\\t\\t{1}...'.format(datetime.datetime.now(), np.mean(all_maps)))\n",
    "    print('=========================================================')\n",
    "    #print('{0}\\t\\tGenerating submission...'.format(datetime.datetime.now()))\n",
    "    # Geometric mean of predictions over folds\n",
    "    p_test = np.prod(test_predictions, axis = 0) ** (1.0 / len(test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FT62wHdEcc2b"
   },
   "source": [
    "We obtained a map of ~ 0.55, which is lower than the one obtained with the whole dataset, (as expected).\n",
    "\n",
    "\n",
    "\n",
    "# 7- Future Steps\n",
    "As future steps, we would like to:\n",
    "\n",
    "- Perform the evaluation over a bigger dataset, possibly using some distirbuted computing techniques / parallel processing to speed-up the feature engineering paert\n",
    "- Make an ensemble of the model with some other Machine Learning lcassifier (XGBoost was one of the most used together with RNN in the challenge!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70gXXiKacych"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Project-Anna&Daniele.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

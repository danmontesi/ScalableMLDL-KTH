{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Concept_Drift.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ElGZQ3AUYCUT","colab_type":"text"},"source":["\n","# Spotify Sequential Skip Prediction Challenge\n","\n","### Anna Martignano, Daniele Montesi, ID2223\n","With  this  project  we  would  like  to  use  Deep  Learning  to  model  a  solution  to  the  challenge  ”  Spotify Sequential  Skip  Prediction  Challenge  -  Predict  if  users  will  skip  or  listen  to  the  music  they’re  streamed” published by Spotify in collaboration with WSDM and CrowdAI.\n","\n","Citing the CrowdAI Challenge Website:The task is to predict whether individual tracks encountered in a listening session will beskippedby a particular user.  In order to do this, complete information about the first half of a user’slistening session is provided, while the prediction is to be carried out on the second half.  \n","\n","The output of a prediction is abinary variable for each track in the second half of the session indicating if it was skipped or not, with a 1 indicating that the track skipped, and a 0 indicating that the track was not skipped.The  problem  corresponds  hence  to  a  binary  classification.   However,  the  problem  is  hard  and  must take into account all the history of the user tracks listened, making the model that best fits this problem aRecurrent Neural Networks.\n","\n","\n","### Code organization\n","\n","\n","The problem will present the following sections:\n","\n","1. Data exploration \n","2. Data filtering/preprocessing \n","3. Dataset creation\n","4. Model reasoning and creation and fitting (First Baseline)\n","5. Re-Implementation of the 5th solution\n","6. Further model improvement\n"]},{"cell_type":"markdown","metadata":{"id":"URzWmxdoK9RF","colab_type":"text"},"source":["## 0 - Libraries import and useful paths"]},{"cell_type":"code","metadata":{"id":"AXf41_dSX9xb","colab_type":"code","colab":{}},"source":["!pip install -q pyyaml h5py  # Required to save models in HDF5 format"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wICoIwG8YQnY","colab_type":"code","outputId":"0b516885-761f-4795-e15a-719734913711","executionInfo":{"status":"ok","timestamp":1578845851717,"user_tz":-60,"elapsed":5431,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["%tensorflow_version 2.x"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gR7TY3KhYUoB","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers, utils, Model, Input\n","from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import model_from_json, load_model\n","\n","from pickle import dump\n","import pickle\n","from tqdm import tqdm\n","import string\n","import numpy as np\n","import random\n","from PIL import Image\n","import time\n","import os\n","import pandas as pd\n","from glob import glob\n","from joblib import delayed, Parallel\n","import matplotlib.pyplot as plt\n","import datetime\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.model_selection import GroupKFold, KFold"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iar1KBmXYcU6","colab_type":"code","outputId":"5fecaf47-2ef9-4cd8-85d0-45aa0c60409c","executionInfo":{"status":"ok","timestamp":1578845855346,"user_tz":-60,"elapsed":5914,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nwXWPtOYYeF7","colab_type":"code","outputId":"079d92f8-f6ea-4a62-9d3a-f3afa06e6a97","executionInfo":{"status":"ok","timestamp":1578845855347,"user_tz":-60,"elapsed":1534,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["dirpath = \"/content/drive/My Drive/Colab Notebooks/Project\"\n","modelpath = dirpath + \"/Model\"\n","datapath = dirpath + \"/Dataset\"\n","processedpath = dirpath + \"/ProcessedDataset\"\n","testpath = datapath + \"/test_set\"\n","trainpath = datapath + \"/training_set\"\n","featurespath = datapath + \"/track_features\"\n","\n","print(featurespath)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/Project/Dataset/track_features\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"f9j_IK9zK6uZ","colab_type":"text"},"source":["## 1 - Data Exploration\n","\n","For the sake of order, we will perform the exploration phase on a different notebook:\n","`SpotifySequentialChallenge - DataExploration`\n","\n","Here we upload the Training dataset and the Test Dataset.\n","\n","- The Train and the Test are belonging to the **same day**. This avoid to suffer from the **concept drift phenomena**\n","- Only a small subset of the training has been employed (the total Spotify Dataset contained 100+GB of data!)\n","- The day is a Summer day (15 July 2018)"]},{"cell_type":"code","metadata":{"id":"FxmGyRMCMmJr","colab_type":"code","outputId":"2144b4f6-ad44-41a1-bcd8-cb700f7cd1a1","executionInfo":{"status":"ok","timestamp":1578740721858,"user_tz":-60,"elapsed":20009,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}},"colab":{"base_uri":"https://localhost:8080/","height":479}},"source":["# Train\n","\n","data = pd.DataFrame()\n","\n","for i in range(2):\n","  data = pd.concat([data, pd.read_csv(trainpath+\"/log_{}_20180715_000000000000.csv\".format(i))])\n","\n","# Setting the flag will be used after we concatenate train-test into one dataset\n","data['train_flag'] = True\n","data.head(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>session_id</th>\n","      <th>session_position</th>\n","      <th>session_length</th>\n","      <th>track_id_clean</th>\n","      <th>skip_1</th>\n","      <th>skip_2</th>\n","      <th>skip_3</th>\n","      <th>not_skipped</th>\n","      <th>context_switch</th>\n","      <th>no_pause_before_play</th>\n","      <th>short_pause_before_play</th>\n","      <th>long_pause_before_play</th>\n","      <th>hist_user_behavior_n_seekfwd</th>\n","      <th>hist_user_behavior_n_seekback</th>\n","      <th>hist_user_behavior_is_shuffle</th>\n","      <th>hour_of_day</th>\n","      <th>date</th>\n","      <th>premium</th>\n","      <th>context_type</th>\n","      <th>hist_user_behavior_reason_start</th>\n","      <th>hist_user_behavior_reason_end</th>\n","      <th>train_flag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0_00006f66-33e5-4de7-a324-2d18e439fc1e</td>\n","      <td>1</td>\n","      <td>20</td>\n","      <td>t_0479f24c-27d2-46d6-a00c-7ec928f2b539</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>2018-07-15</td>\n","      <td>True</td>\n","      <td>editorial_playlist</td>\n","      <td>trackdone</td>\n","      <td>trackdone</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0_00006f66-33e5-4de7-a324-2d18e439fc1e</td>\n","      <td>2</td>\n","      <td>20</td>\n","      <td>t_9099cd7b-c238-47b7-9381-f23f2c1d1043</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>2018-07-15</td>\n","      <td>True</td>\n","      <td>editorial_playlist</td>\n","      <td>trackdone</td>\n","      <td>trackdone</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0_00006f66-33e5-4de7-a324-2d18e439fc1e</td>\n","      <td>3</td>\n","      <td>20</td>\n","      <td>t_fc5df5ba-5396-49a7-8b29-35d0d28249e0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>2018-07-15</td>\n","      <td>True</td>\n","      <td>editorial_playlist</td>\n","      <td>trackdone</td>\n","      <td>trackdone</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0_00006f66-33e5-4de7-a324-2d18e439fc1e</td>\n","      <td>4</td>\n","      <td>20</td>\n","      <td>t_23cff8d6-d874-4b20-83dc-94e450e8aa20</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>2018-07-15</td>\n","      <td>True</td>\n","      <td>editorial_playlist</td>\n","      <td>trackdone</td>\n","      <td>trackdone</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0_00006f66-33e5-4de7-a324-2d18e439fc1e</td>\n","      <td>5</td>\n","      <td>20</td>\n","      <td>t_64f3743c-f624-46bb-a579-0f3f9a07a123</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>2018-07-15</td>\n","      <td>True</td>\n","      <td>editorial_playlist</td>\n","      <td>trackdone</td>\n","      <td>trackdone</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                               session_id  ...  train_flag\n","0  0_00006f66-33e5-4de7-a324-2d18e439fc1e  ...        True\n","1  0_00006f66-33e5-4de7-a324-2d18e439fc1e  ...        True\n","2  0_00006f66-33e5-4de7-a324-2d18e439fc1e  ...        True\n","3  0_00006f66-33e5-4de7-a324-2d18e439fc1e  ...        True\n","4  0_00006f66-33e5-4de7-a324-2d18e439fc1e  ...        True\n","\n","[5 rows x 22 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"jxxQ47c21vvB","colab_type":"code","colab":{}},"source":["# Test\n","\n","test_data_hist  = pd.read_csv(testpath + '/log_prehistory_20180715_000000000000.csv')\n","test_data_hist['train_flag']  = False\n","\n","data = pd.concat([data,test_data_hist])\n","data.reset_index(drop=True, inplace=True)\n","\n","test_data_input = pd.read_csv(testpath + '/log_input_20180715_000000000000.csv')\n","test_data_input['train_flag']  = False"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pdcm8aUkOQv5","colab_type":"text"},"source":["# Feature engineering\n","\n","We have observed that the  tracks suggested by Spotify are sometimes repeated. We will take that into account encoding that as a feature."]},{"cell_type":"code","metadata":{"id":"_vGO7oG0OU2z","colab_type":"code","colab":{}},"source":["# Check for presence of duplicate tracks into a session\n","grouped_sess_tracks = data.groupby('session_id')['track_id_clean'].apply(lambda x: x.tolist()).tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ov5LooQN4EdI","colab_type":"code","outputId":"c1b9b6b3-fa47-452f-8a43-a53e40f1c4b9","executionInfo":{"status":"ok","timestamp":1578740868563,"user_tz":-60,"elapsed":128214,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["session_track_count = data.groupby(['session_id','track_id_clean'], as_index = False).size().reset_index().rename(columns = {0 : 'duplicated_time'})\n","data = data.merge(session_track_count, on = ['session_id', 'track_id_clean'], how = 'left')\n","print(\"On average, the number of duplicate songs in those sessions is --> \")\n","session_track_count.duplicated_time.mean()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["On average, the number of duplicate songs in those sessions is --> \n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["1.1278782639423945"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"JRnPlLD3Ohas","colab_type":"text"},"source":["### We now load the Tracks dataset and joint it with the dataset:"]},{"cell_type":"code","metadata":{"id":"uwaife0UOk5B","colab_type":"code","colab":{}},"source":["tf = pd.read_csv(featurespath+\"/tf_0.csv\")\n","tf = pd.concat([tf, pd.read_csv(featurespath+\"/tf_1.csv\")])\n","tf.reset_index(drop=True, inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"scAg_5jmOvVB","colab_type":"text"},"source":["# 2- Dataset Preprocessing\n","As we can see, the dataset is made of these columns:\n","\n","- SessionPosition: the position of the track on the session (which can be long up to 20)\n","- skip_1\tskip_2\tskip_3\tnot_skipped: The possible values for skipping. For this purpose, we will drop all of these except for the skip_2, which is the type of skip we are interested on"]},{"cell_type":"code","metadata":{"id":"AqNADOKiOtSG","colab_type":"code","colab":{}},"source":["data[\"label\"] = data[\"skip_2\"]\n","data.drop(['skip_1', 'skip_2','skip_3', 'not_skipped'], axis=1, inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Iht6g2sO3Xf","colab_type":"code","outputId":"fcc0e509-793c-4b90-e0f6-af081d33e45d","executionInfo":{"status":"ok","timestamp":1578676184398,"user_tz":-60,"elapsed":755473,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","for i in tqdm(data.index):\n","    leng = int(data.at[i, \"session_length\"])\n","    hour = int(data.at[i, \"hour_of_day\"])\n","    if leng < 15:\n","        data.at[i, \"session_length\"] = 0\n","    elif leng >= 15 and leng<20:\n","        data.at[i, \"session_length\"] = 1\n","    else:\n","        data.at[i, \"session_length\"] = 2\n","        \n","        \n","    if hour >7 and hour<=13:\n","        hour = data.at[i, \"hour_of_day\"] = 0\n","    elif hour >13 and hour<20:\n","        hour = data.at[i, \"hour_of_day\"] = 1\n","    elif hour >=20:\n","        hour = data.at[i, \"hour_of_day\"] = 2\n","    else:\n","        hour = data.at[i, \"hour_of_day\"] = 3\n","\n","   "],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 9645404/9645404 [09:23<00:00, 17103.32it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"vyJ4WGJkO46e","colab_type":"code","outputId":"5b9b0eed-d4a8-4ea5-b4b6-b8a976857bdb","executionInfo":{"status":"ok","timestamp":1578676272059,"user_tz":-60,"elapsed":835377,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# TO DO: Optimize or remove\n","\n","for i in tqdm(tf.index):\n","    yr = int(tf.at[i, \"release_year\"])\n","    if yr < 1960:\n","        tf.at[i, \"release_year\"] = 0\n","    elif yr >= 1960 and yr < 1970:\n","        tf.at[i, \"release_year\"] = 1\n","    elif yr > 1970 and yr <= 1980:\n","        tf.at[i, \"release_year\"] = 2\n","    elif yr > 1980 and yr <= 1990:\n","        tf.at[i, \"release_year\"] = 3\n","    elif yr > 1990 and yr <= 2000:\n","        tf.at[i, \"release_year\"] = 4\n","    elif yr > 2000 and yr <= 2005:\n","        tf.at[i, \"release_year\"] = 5\n","    elif yr > 2005 and yr <= 2010:\n","        tf.at[i, \"release_year\"] = 6\n","    elif yr > 2010 and yr <= 2015:\n","        tf.at[i, \"release_year\"] = 7"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 3706388/3706388 [01:27<00:00, 42382.51it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"VvseFa91dUzO","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w_2dk5LxO7qP","colab_type":"text"},"source":["### Scaling and one-hot-encoding"]},{"cell_type":"code","metadata":{"id":"oKNNEZe4PDfR","colab_type":"code","outputId":"003eb397-ab44-42e1-dd84-2e8b9a3759bb","executionInfo":{"status":"ok","timestamp":1578682898838,"user_tz":-60,"elapsed":42246,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Generate dummy columns for categorical variables\n","# Scaler\n","print('{0}\\t\\tNormalising context features...'.format(datetime.datetime.now()))\n","features = list(set(data.columns) - set(['session_id','track_id_clean','date', 'label', 'session_position', 'train_flag', \"context_switch\", 'session_length', 'no_pause_before_play','short_pause_before_play','long_pause_before_play','hist_user_behavior_n_seekfwd','hist_user_behavior_is_shuffle','premium','hour_of_day', 'hist_user_behavior_reason_start', 'hist_user_behavior_reason_end', 'session_length', 'context_type']))\n","data[features] = StandardScaler().fit_transform(data[features])\n","\n","features_t = list(set(tf.columns) - set(['track_id' , 'release_year', 'time_signature', 'mode', 'key']))\n","tf[features_t] = StandardScaler().fit_transform(tf[features_t])\n","\n","features.append('label')\n","features.append('session_position')\n","\n","print('{0}\\t\\tOne-hot-encoding categorical variables...'.format(datetime.datetime.now()))\n","data = pd.get_dummies(data, columns = ['context_type'], prefix = ['context_type'])\n","data = pd.get_dummies(data, columns = ['hist_user_behavior_reason_start'], prefix = ['hist_user_behavior_reason_start'])\n","data = pd.get_dummies(data, columns = ['hist_user_behavior_reason_end'], prefix = ['hist_user_behavior_reason_end'])\n","data = pd.get_dummies(data, columns = ['session_length'], prefix = ['session_len'])\n","data = pd.get_dummies(data, columns = ['hour_of_day'], prefix = ['hour_of_day'])\n","# Features to use\n","\n","tf = pd.get_dummies(tf, columns = ['release_year'], prefix = ['rel_year'])\n","tf = pd.get_dummies(tf, columns = ['mode'], prefix = ['mode'])\n","tf = pd.get_dummies(tf, columns = ['key'], prefix = ['key'])\n","tf = pd.get_dummies(tf, columns = ['time_signature'], prefix = ['time_signature'])\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2020-01-10 19:00:59.746477\t\tNormalising context features...\n","2020-01-10 19:01:15.228463\t\tOne-hot-encoding categorical variables...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZgHMLmJlVt8P","colab_type":"code","colab":{}},"source":["missing_tdi_feat = set(data.columns.values) - set(test_data_input.columns.values) \n","for f in list(missing_tdi_feat):\n","  test_data_input[f] = 0.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5834eInNXNZK","colab_type":"code","colab":{}},"source":["data = data[:10000]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X2LbuhjO7aCY","colab_type":"code","colab":{}},"source":["data = pd.concat([data,test_data_input], sort=False)\n","data.fillna(.0,inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"farpaZ2BPFkG","colab_type":"text"},"source":["# 3- Dataset Creation\n","\n","The dataset is formed analogously to whast reported in the 5th place solution paper: http://adrem.uantwerpen.be//bibrem/pubs/WSDMCupJeunen2019.pdf\n","\n","![alt test](pics/model.png)\n","\n","(https://github.com/olivierjeunen/sequential-skip-prediction/blob/master/src/RNN.py in generate_model() )\n","\n","where Xh and Xf are respectively 2 matrices for the session history and future. \n","### The steps to the dataset creation are:\n","\n","1. Merge track features and session features\n","2. Split based on the time, and drop that feature (with other useless features)\n","3. create the History-Matrix and Future Matrix. Create duplicate files for each new song to predict its label "]},{"cell_type":"markdown","metadata":{"id":"lJd2YmVfPbJM","colab_type":"text"},"source":["## Keep only tracks present in the training set\n","\n","Then, we will explore how frequent they are :)"]},{"cell_type":"code","metadata":{"id":"IfdLpBr6PWM2","colab_type":"code","outputId":"de8fff04-04d8-4582-cbe1-10defd29a6e9","executionInfo":{"status":"ok","timestamp":1578676381915,"user_tz":-60,"elapsed":2800,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["set_tracks_present = set(data.track_id_clean.values)\n","tracks_set = set(tf.track_id)\n","print(\"There are\", len(set_tracks_present), \"tracks in the dataset \\nInstead, in the tracks_data there are\", len(tracks_set) )\n","print(\"Missing tracks on dataset:\", len(set(set_tracks_present)-tracks_set)*100/len(set(set_tracks_present)), \"%\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["There are 701495 tracks in the dataset \n","Instead, in the tracks_data there are 3706388\n","Missing tracks on dataset: 0.0 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"istZVuiqPlAb","colab_type":"code","colab":{}},"source":["#TODO Correct: must output a number\n","data['date'] = pd.to_datetime(data['date'])\n","data[\"hist_user_behavior_is_shuffle\"] = data['hist_user_behavior_is_shuffle'].astype(np.int64)\n","data[\"premium\"] = data['premium'].astype(np.int64)\n","data[\"label\"] = data['label'].astype(np.int64)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IbWZ12BdPm6W","colab_type":"code","colab":{}},"source":["# Sorting by session & position of track in session\n","data.sort_values(['session_id','session_position', \"date\"], inplace = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Jgy869XPulw","colab_type":"code","colab":{}},"source":["tf[\"track_id_clean\"] = tf[\"track_id\"]\n","tf.drop([\"track_id\"], axis=1, inplace=True)\n","\n","data.drop(\"date\", axis=1, inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q_qQ_YxvPwbv","colab_type":"code","colab":{}},"source":["dataset = data.merge(tf, on = 'track_id_clean', how = 'left')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UorYGDmQd35R","colab_type":"text"},"source":["# Feature engineering:\n","\n","Feature engineering is the key for winning competitions of Data Science. Now that we have the whole dataset we are going to compute:\n","1. Popularity of the songs liked/disliked\n","2. Duration of songs liked/disliked\n","3. Release date estimate\n","4. Acoustic vector correlation\n","\n","\n","Following the work of the 7th place solution, we are going to first, select some tracks:\n","- Positive tracks (i.e. the ones that are listened by the user)\n","- Negative tracks (i.e. the ones that are skipped by the user)\n","- Song considered' feature\n","\n","Then, we are saving the features as the DELTA between the\n","- positive_feat - song-feat\n","- negative-feat - song_feat\n","\n","Where positive_feat is the feature (i.e. populatiry) **belonging to ALL the songs LIKED by the user, averaged**.\n","\n","Moreover, we are going to compute the **dot product** between the acoustic vector of the song and the one of the positive/negative tracks, trying to see whether there is a correlation between the 2.\n","\n","\n","As addiction, we are computing some similarities of those (Cosine Similarity, Manhattan Distance, ...)\n","\n","We are choosing to compute Cosine Similarity\n"]},{"cell_type":"code","metadata":{"id":"Jh2Kd5P9i4Zx","colab_type":"code","colab":{}},"source":["features = ['us_popularity_estimate', 'acousticness', 'beat_strength', 'bounciness', 'danceability',\n","       'dyn_range_mean', 'energy', 'flatness', 'instrumentalness',\n","       'liveness', 'loudness', 'mechanism','tempo' , 'organism', 'speechiness', 'valence']\n","\n","\"\"\"\n","'pos_year': year_pos,\n","'neg_year': year_neg,\n","\"\"\"\n","\n","def extract_features_session_track(song, positive_tracks, negative_tracks):\n","    mean_duration_pos = (positive_tracks['duration'] - song['duration']).mean()\n","    mean_duration_neg = (negative_tracks['duration'] - song['duration']).mean()\n","    \"\"\"\n","    year_pos = (positive_tracks['release_date_estimate'] - song['release_date_estimate']).mean()\n","    year_neg = (negative_tracks['release_date_estimate'] - song['release_date_estimate']).mean()\n","   \"\"\"\n","    pop_pos = (positive_tracks['us_popularity_estimate'] - song['us_popularity_estimate']).mean()\n","    pop_neg = (negative_tracks['us_popularity_estimate'] - song['us_popularity_estimate']).mean()\n"," \n","    latent_vectors = ['acoustic_vector_0', 'acoustic_vector_1', 'acoustic_vector_2', 'acoustic_vector_3', \n","                      'acoustic_vector_4', 'acoustic_vector_5', 'acoustic_vector_6', 'acoustic_vector_7']\n","    mean_dot_vector_pos = positive_tracks[latent_vectors].dot(song[latent_vectors]).mean()\n","    mean_dot_vector_neg = negative_tracks[latent_vectors].dot(song[latent_vectors]).mean()\n","\n","    return {'pos_duration': mean_duration_pos, \n","        'neg_duration': mean_duration_neg,\n","        'pop_neg': pop_neg,\n","        'pop_pos': pop_pos,\n","        'pos_dot': mean_dot_vector_pos,\n","        'neg_dot': mean_dot_vector_neg\n","        }\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KtX44chV10x-","colab_type":"code","colab":{}},"source":["tf_old = tf.copy()\n","tf.set_index('track_id_clean', inplace=True)\n","\n","dataset.reset_index(inplace=True, drop=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"89OZ-1MDT49g","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2vuetQ8UYPVv","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9KZZ8eQmd3N_","colab_type":"code","outputId":"9695c57c-7788-4924-a0b0-eed80c15b647","executionInfo":{"status":"error","timestamp":1578815523671,"user_tz":-60,"elapsed":2101,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["current_index = 0\n","num_lines_completed = 0\n","completed_index = 0\n","\n","\"\"\"\n","'pos_year': [],\n","'neg_year': [],\n","\"\"\"\n","\n","# Initialize the list\n","dict_feat = {'pos_duration': [], \n","        'neg_duration': [],\n","        'pop_neg': [],\n","        'pop_pos': [],\n","        'pos_dot': [],\n","        'neg_dot': []\n","        }\n","\n","# Iterating over the whole dataset:\n","\n","pbar = tqdm(total=len(dataset)-1)\n","\n","while current_index < len(dataset)-1:\n","  \n","  partial_length = dataset['session_length'].iloc[current_index]-dataset['session_position'].iloc[current_index]+1\n","  half = int(np.floor(partial_length / 2.0))\n","  last_session_tracks = dataset.loc[current_index+(half):current_index+partial_length-1]\n","\n","  \"\"\"\n","  if completed_index < num_lines_completed:\n","      current_index += partial_length \n","      completed_index += len(last_session_tracks)\n","      continue\n","  \"\"\"\n","\n","  if current_index>9980:\n","    print(current_index, partial_length)\n","  first_session_tracks = dataset.loc[current_index:current_index+(half)-1]\n","  last_session_item = first_session_tracks.iloc[-1]\n","            \n","  skipped = first_session_tracks[first_session_tracks['label'] == 1]['track_id_clean']\n","  completed = first_session_tracks[first_session_tracks['label'] == 0]['track_id_clean']\n","  \n","  skipped = tf.loc[skipped]\n","  completed = tf.loc[completed]\n","  \n","  # Put at zero the new engineered features\n","  for k,v in dict_feat.items():\n","    dict_feat[k] = dict_feat[k]+[0]*len(first_session_tracks)\n","\n","  for j, session_track_row in last_session_tracks.iterrows():\n","    song_row = tf.loc[session_track_row['track_id_clean']]\n","    track_features = extract_features_track(song_row)\n","    track_features[\"distance\"] = session_track_row[\"session_position\"] - last_session_item[\"session_position\"]\n","\n","    session_track_features = extract_features_session_track(song_row, completed, skipped)\n","\n","    # appending to the whole feature dictionary\n","    for k,v in session_track_features.items():\n","      dict_feat[k] = dict_feat[k]+[v]\n","\n","  current_index += partial_length\n","  pbar.update(partial_length)\n","\n","pbar.close()\n","\n","for k,v in dict_feat.items():\n","  dataset.loc[:, k] = v"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-07921d48bd5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Iterating over the whole dataset:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mcurrent_index\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"]}]},{"cell_type":"code","metadata":{"id":"IfRzXXv4lEkU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5cPnu_QnPzLg","colab_type":"text"},"source":["## Organize the dataset into samples. \n","\n","Let n be the maximal session length. With k = ⌊n/2].\n","\n","As session lengths can vary, shorter sessions are padded with zeroes to match the size of the longest ones. Historical tracks Xh are padded before the track sequence, and future tracks Xf are padded after the track sequence.\n","\n","Input of the network is made of 2 np.arrays-matrices:\n","- **session_histories** old_matrix R^ (f x k x t), where k is the first half of the features. Made of all the features of thefirst half of the tracks\n","- **session_futures** new_matrix: R^ (f x (n-k) x t), where n is the total length of features. It is made of all the features in the second half of the tracks (without their labels)\n","\n","To do so, create 4 lists containing:\n","1. session_histories as np.array\n","2. session_futures as np.array\n","3. session_labels as lists\n","4. sizes as list of int"]},{"cell_type":"code","metadata":{"id":"uenLTJmnP50F","colab_type":"code","colab":{}},"source":["max_seq_len = 20\n","\n","def process_sessions(data, sess_features, track_feature_list, label_index):\n","    session_histories = []\n","    session_futures   = []\n","    session_labels    = []\n","    session_sizes     = []\n","    \n","\n","    for key, subgroup in tqdm(data.groupby('session_id', sort = False)):\n","        # Split session in two\n","        half = int(np.floor(len(subgroup) / 2.0))\n","        first = subgroup[sess_features + track_feature_list].head(half).values\n","        second = subgroup[sess_features + track_feature_list].tail(subgroup.shape[0] - half)\n","        current_labels = second[label_index].astype(int).values\n","\n","        second = second.values\n","        \n","        # Generate padding for session history\n","        padding = np.zeros((int(max_seq_len / 2.0) - first.shape[0], len(sess_features) + len(track_feature_list)))\n","        \n","        #stack all in a np array\n","        session_history = np.vstack([padding,first])\n","        \n","        # Generate padding for session future\n","        padding = np.zeros((int(max_seq_len / 2.0) - second.shape[0], len(track_feature_list)))\n","        session_future = np.vstack([second[:,-len(track_feature_list):],padding])\n","        \n","        # Save results\n","        session_histories.append(session_history)\n","        session_futures.append(session_future)\n","        \n","        s_l = list(current_labels)\n","        s_l = [x[0] for x in current_labels]\n","        times=int(max_seq_len / 2.0) - current_labels.shape[0]\n","        if times > 0:\n","           for i in range(times):\n","              np.array(s_l.append(0))\n","        session_labels.append(s_l)\n","        session_sizes.append(len(current_labels))\n","            \n","    return session_histories, session_futures, session_labels, session_sizes \n","        \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEIEstJ10s7j","colab_type":"code","colab":{}},"source":["max_seq_len = 20\n","\n","def process_sessions_train(data, sess_features, track_feature_list, label_index):\n","    session_histories = []\n","    session_futures   = []\n","    session_labels    = []\n","    session_sizes     = []\n","    \n","    for key, subgroup in tqdm(data.groupby('session_id', sort = False)):\n","        # Split session in two\n","        half = int(np.floor(len(subgroup) / 2.0))\n","        first = subgroup[sess_features + track_feature_list].head(half).values\n","        second = subgroup[sess_features + track_feature_list].tail(subgroup.shape[0] - half)\n","        \n","         \n","        # Generate labels\n","        current_labels = second[label_index].astype(int).values\n","        \n","        second = second.values\n","        # Generate padding for session history\n","        padding = np.zeros((int(max_seq_len / 2.0) - first.shape[0], len(sess_features) + len(track_feature_list)))\n","        \n","        #stack all in a np array\n","        session_history = np.vstack([padding,first])\n","        \n","        # Generate padding for session future\n","        padding = np.zeros((int(max_seq_len / 2.0) - second.shape[0], len(track_feature_list)))\n","        session_future = np.vstack([second[:,-len(track_feature_list):],padding])\n","        \n","        # Save results\n","        session_histories.append(session_history)\n","        session_futures.append(session_future)\n","        \n","        \n","        s_l = list(current_labels)\n","        s_l = [x[0] for x in current_labels]\n","        times=int(max_seq_len / 2.0) - current_labels.shape[0]\n","        if times > 0:\n","            for i in range(times):\n","                np.array(s_l.append(0))\n","        session_labels.append(s_l)\n","        session_sizes.append(len(current_labels))\n","\n","    return session_histories, session_futures, session_labels, session_sizes \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dNzNm2AY0uQF","colab_type":"code","colab":{}},"source":["max_seq_len = 20\n","\n","def process_sessions_test(data, sess_features, track_feature_list, label_index):\n","    session_histories = []\n","    session_futures   = []\n","    session_labels    = []\n","    session_sizes     = []\n","    \n","    for key, subgroup in tqdm(data.groupby('session_id', sort = False)):\n","        # Split session in two\n","        half = int(np.floor(len(subgroup) / 2.0))\n","        first = subgroup[sess_features + track_feature_list].head(half).values\n","        second = subgroup[sess_features + track_feature_list].tail(subgroup.shape[0] - half)\n","        \n","        label_shape = second.shape[0]\n","        second = second.values\n","        # Generate padding for session history\n","        padding = np.zeros((int(max_seq_len / 2.0) - first.shape[0], len(sess_features) + len(track_feature_list)))\n","        \n","        #stack all in a np array\n","        session_history = np.vstack([padding,first])\n","        \n","        # Generate padding for session future\n","        padding = np.zeros((int(max_seq_len / 2.0) - second.shape[0], len(track_feature_list)))\n","        session_future = np.vstack([second[:,-len(track_feature_list):],padding])\n","        \n","        # Save results\n","        session_histories.append(session_history)\n","        session_futures.append(session_future)\n","        \n","        session_sizes.append(label_shape)\n","\n","    return session_histories, session_futures, session_sizes\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PGbSEYH30y8X","colab_type":"code","colab":{}},"source":["sf = list(data.columns.values)\n","for el in [\"track_id_clean\", \"session_id\", \"train_flag\"]:\n","    sf.remove(el)\n","tf_f = list(tf.columns.values)\n","for el in [\"track_id_clean\"]: \n","    tf_f.remove(el)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7QPzmKmC1DW1","colab_type":"code","colab":{}},"source":["#Prepare training dataset and save it\n","\n","train_dataset = dataset[dataset.train_flag == True]\n","history_train, future_train, labels_train, session_len_train = process_sessions_train(train_dataset, sf, tf_f, [\"label\"])\n","#Save\n","\n","# Encode sessions as numpy arrays\n","history_train = np.asarray(history_train)\n","future_train = np.asarray(future_train)\n","labels_train = np.asarray(labels_train)\n","session_len_train = np.asarray(session_len_train)\n","\n","# Dump to file\n","np.savez_compressed(processedpath + \"/trainprova.npz\",\n","    history_train  = history_train,\n","    future_train = future_train,\n","    labels_train = labels_train,\n","    session_len_train = session_len_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pdmvz8pb1ITF","colab_type":"code","outputId":"b6c2edf6-8a4c-4c3b-e487-49f7ea2a115c","executionInfo":{"status":"ok","timestamp":1578670470956,"user_tz":-60,"elapsed":168017,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#Prepare training dataset and save it\n","test_dataset = dataset.loc[dataset['train_flag'] == False]\n","history_test, future_test, session_len_test = process_sessions_test(test_dataset, sf, tf_f, [\"label\"])\n","\n","#Save\n","\n","# Encode sessions as numpy arrays\n","history_test = np.asarray(history_test)\n","future_test = np.asarray(future_test)\n","session_len_test = np.asarray(session_len_test)\n","\n","# Dump to file\n","np.savez_compressed(processedpath + \"/test.npz\",\n","    history_test  = history_test,\n","    future_test = future_test,\n","    session_len_test = session_len_test)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 58/58 [00:00<00:00, 271.49it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"n4OB0B7EZgcp","colab_type":"text"},"source":["### Problem: concept drift: must order by data and split according to that\n","\n","As we are dealing with temporal data, new songs start to appear throughout the data, songs’ popularities change over time, et cetera. This phenomenon, widely known as concept drift, can heavily influence the performance of learning algorithms, if not dealt with properly.\n","\n","Links: G. Widmer and M. Kubat. 1996. Learning in the Presence of Concept Drift and Hidden Contexts. Machine Learning 23, 1 (01 Apr 1996), 69–101."]},{"cell_type":"code","metadata":{"id":"ah57G8WtZhNv","colab_type":"code","outputId":"ef75603c-5de8-4fd1-a8fc-879f4b285b00","executionInfo":{"status":"ok","timestamp":1578670470957,"user_tz":-60,"elapsed":167987,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["\"\"\" #split todo\n","def getData(percentage=0.9):\n","    restored = np.load(processedpath + \"/train2.npz\")\n","    history = restored['history']\n","    future = restored['future']\n","    labels = restored['labels']\n","    session_len = restored['session_len']\n","    \n","    # Make sure we shuffle classes\n","    indices = np.arange(len(history))\n","    np.random.shuffle(indices)\n","    history = history[indices]\n","    future = future[indices]\n","    labels = labels[indices]\n","    session_len = session_len[indices]\n","        \n","    num = int(len(indices)*percentage)\n","    \n","    del restored\n","    return history[:num], history[num:], future[:num], future[num:], labels[:num], labels[num:], session_len[:num], session_len[num:]\n","\n","train_history, test_history, train_future, test_future, train_labels, test_labels, train_session_len, test_session_len = getData(percentage=0.6)\n","\n","\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' #split todo\\ndef getData(percentage=0.9):\\n    restored = np.load(processedpath + \"/train2.npz\")\\n    history = restored[\\'history\\']\\n    future = restored[\\'future\\']\\n    labels = restored[\\'labels\\']\\n    session_len = restored[\\'session_len\\']\\n    \\n    # Make sure we shuffle classes\\n    indices = np.arange(len(history))\\n    np.random.shuffle(indices)\\n    history = history[indices]\\n    future = future[indices]\\n    labels = labels[indices]\\n    session_len = session_len[indices]\\n        \\n    num = int(len(indices)*percentage)\\n    \\n    del restored\\n    return history[:num], history[num:], future[:num], future[num:], labels[:num], labels[num:], session_len[:num], session_len[num:]\\n\\ntrain_history, test_history, train_future, test_future, train_labels, test_labels, train_session_len, test_session_len = getData(percentage=0.6)\\n\\n'"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"r4Pb8c6_krs7","colab_type":"text"},"source":["# Loading saving dataset"]},{"cell_type":"code","metadata":{"id":"Ewm3a_NYk1NV","colab_type":"code","colab":{}},"source":["import numpy as np\n","# save np.load\n","np_load_old = np.load"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bCvZT99xvwVH","colab_type":"code","colab":{}},"source":["np.load = np_load_old\n","\n","# modify the default parameters of np.load\n","np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n","\n","\n","restored = np.load(processedpath + \"/trainJuly.npz\")\n","train_history = restored['history_train']\n","train_future = restored['future_train']\n","train_labels = restored['labels_train']\n","train_session_len = restored['session_len_train']\n","\n","del restored\n","\n","# restore np.load for future normal usage\n","np.load = np_load_old"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nbjch25FCTNP","colab_type":"code","colab":{}},"source":["np.load = np_load_old\n","import numpy as np\n","# save np.load\n","np_load_old = np.load\n","\n","# modify the default parameters of np.load\n","np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n","\n","restored2 = np.load(processedpath + \"/test.npz\")\n","test_history = restored2['history_test']\n","test_future = restored2['future_test']\n","test_session_len = restored2['session_len_test']\n","\n","del restored2\n","\n","# restore np.load for future normal usage\n","np.load = np_load_old"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BTqAzySxaIEw","colab_type":"text"},"source":["# 4- Model Fitting ad Evaluation Metrics \n","\n","To evaluate the performance of our solution we are going to use the same metrics proposed for this competition. \n","\n","The primary one is the Mean Average Accuracy:\n","\n","\\begin{equation*}\n","AA   =  \\sum_{i=1}^{T} \\frac{A(i)L(i)}{T}\n","\\end{equation*}\n","\n","The metric aims to weight higher the correct good prediction, instead of predicting the\n","\n","\n","- T is the number of tracks to be predicted for the given session\n","- A(i) is the accuracy at position i of the sequence\n","- L(i) is the boolean indicator for if the i‘th prediction was correct\n","\n","Moreover, we are going to use the accuracy for the second half of the dataset\n"]},{"cell_type":"code","metadata":{"id":"_ULyS9rmZywn","colab_type":"code","colab":{}},"source":["def evaluation_MAP_FPA(sizes, label_matrix, prediction_matrix):\n","    '''\n","    Return MAA and first-prediction-accuracy\n","    :param sizes:    A list of session sizes\n","    :param labels:      The correct labels list [ [1,0,1,1,0,... size_of_truth_labels], ...]\n","    :param predictions: The predicted labels list [ [1,0,1,0,0,... size_of_predicted_labels], ...]\n","    :returns:           Mean Average Precision and First Prediction Accuracy\n","    '''\n","    # Set variables\n","    score = 0.0\n","    first_acc = 0.0\n","    # For every session\n","    for size, label_row, prediction_row in zip(sizes, label_matrix, prediction_matrix):\n","        # Set variables\n","        n_correct    = 0\n","        session      = 0.0\n","        # For the first 'size' predictions\n","        for i in range(size):        \n","            # If the prediction is correct:\n","            if label_row[i] == prediction_row[i]:\n","                # Increase counter of correct predictions\n","                n_correct += 1\n","                session += n_correct / (i + 1)\n","                # If first prediction\n","                if i == 0:\n","                    first_acc += 1\n","        # Save session score\n","        score += session / size\n","    return score/sizes.shape[0], first_acc/sizes.shape[0]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7nfXdbjlbo1F","colab_type":"text"},"source":["## Model Description\n","\n","RNN architecture: to capture the time-correlation among data input LSTM layers are used to model the input sequences.\n","Instead, dense layers are connecet to future input tracks, the ones needed to be predicted.\n","Input sequences are modelled using doubly\n","The outputs of these LSTM and dense layers are concatenated and fed into the tail of the network. All dense layers are followed by\n","exponential linear units (ELUs).\n","\n","Avoid overfitting: dropout is applied, and as well batch normalization to allow more effective weights updates. When training, we have also adopted early stopping, when the results do not improve for 5 epochs consecutively the best weights are \n","\n","Personalization of the loss measure: since the challenge gives more importance to the accuracy of the prediction of the first tracks in the session, we have opt to use a weighted variant of binary cross-entropy as loss function."]},{"cell_type":"code","metadata":{"id":"ClTFbsThZ2G2","colab_type":"code","colab":{}},"source":["from tensorflow.keras import losses\n","from tensorflow.keras.utils import plot_model\n","\n","def generate_model(history, future):\n","    h_input  = tf.keras.layers.Input(shape = (history.shape[1], history.shape[2]))\n","    h_subnet = tf.keras.layers.Dropout(.35)(h_input)\n","    h_subnet = tf.compat.v1.keras.layers.CuDNNLSTM(256, return_sequences = True)(h_subnet)\n","    h_subnet = tf.compat.v1.keras.layers.CuDNNLSTM(256)(h_subnet)\n","\n","    f_input  = tf.keras.layers.Input(shape = (future.shape[1], future.shape[2]))\n","    f_subnet = tf.keras.layers.Dropout(.35)(f_input)\n","    f_subnet = tf.compat.v1.keras.layers.CuDNNLSTM(128, return_sequences = True)(f_subnet)\n","    f_subnet = tf.compat.v1.keras.layers.CuDNNLSTM(128)(f_subnet)\n","\n","    td_subnet = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(32))(f_input)\n","    td_subnet = tf.keras.layers.Flatten()(td_subnet)\n","\n","    concat = tf.keras.layers.concatenate([h_subnet, f_subnet, td_subnet])\n","    concat = tf.keras.layers.Dropout(.25)(concat)\n","\n","    dense1  = tf.keras.layers.Dense(512, activation = 'elu')(concat)\n","    dense1  = tf.keras.layers.BatchNormalization()(dense1)\n","    dense2  = tf.keras.layers.Dense(512, activation = 'elu')(dense1)\n","\n","    outputs = [tf.keras.layers.Dense(1, activation = 'sigmoid')(dense2) for _ in range(10)]\n","\n","    model = tf.keras.Model(inputs = [h_input, f_input], outputs = outputs)\n","    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iGclA9DAZ5oM","colab_type":"code","outputId":"9de0b546-7925-4bc3-e469-74fb9f53e2d2","executionInfo":{"status":"error","timestamp":1578839593165,"user_tz":-60,"elapsed":26827,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}},"colab":{"base_uri":"https://localhost:8080/","height":515}},"source":["from tensorflow.keras import layers, utils, Model, Input\n","from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n","\n","# k-fold Cross-validation grouped on sessions\n","k = 5\n","n_epochs = 50\n","test_predictions = []\n","all_maps = []\n","\n","# Generate model\n","#plot_model(model)\n","for fold_id, (train_idx, valid_idx) in enumerate(KFold(n_splits = k).split(train_history)):\n","    print('{0}\\t----- FOLD {1} -----'.format(datetime.datetime.now(),fold_id))\n","    # Filter out training and testing data\n","    h_train = train_history[train_idx]\n","    h_valid = train_history[valid_idx]\n","    f_train = train_future[train_idx]\n","    f_valid = train_future[valid_idx]\n","    l_train = train_labels[train_idx]\n","    l_valid = train_labels[valid_idx]\n","    s_train = train_session_len[train_idx]\n","    s_valid = train_session_len[valid_idx]\n","\n","    # Loss weights\n","    weights = np.asarray([(1 / val + sum((1 / (2*n)) for n in range(val + 1,11))) for val in range(1,11)])\n","    weights /= weights.max()\n","\n","    model = generate_model(train_history, train_future)\n","    model.compile(loss='binary_crossentropy',\n","                                   optimizer = tf.keras.optimizers.Adam(lr = 0.002, amsgrad = True),\n","                                   metrics=['binary_accuracy'],\n","                                   loss_weights = weights.tolist())\n","    \n","    # Early stopping\n","    best_map = .0\n","    best_weights = None\n","    best_epoch = 0\n","\n","    # For every epoch\n","    for epoch_id in range(n_epochs):\n","        model.fit([h_train, f_train], [l_train[:,i] for i in range(10)],\n","                  validation_data = ([h_valid, f_valid], [l_valid[:,i] for i in range(10)]),\n","                  batch_size = 2048, epochs = 1, verbose = 2)\n","        p_valid = model.predict([h_valid, f_valid], batch_size = 4096)\n","        MAP, FPA = evaluation_MAP_FPA(s_valid, l_valid, np.swapaxes(np.round(p_valid),0,1))\n","        print('{0}\\t\\tValid\\tMAP:\\t{1}\\tFPA:\\t{2}'.format(datetime.datetime.now(),MAP, FPA))\n","        if MAP > best_map:\n","            best_map = MAP\n","            best_epoch = epoch_id\n","            best_weights = parallel_model.get_weights()\n","            # save model architecture on file\n","            model.save_weights(modelpath + '/model_best_epoch_{}.h5'.format(epoch_id+1))\n","        elif epoch_id - best_epoch >= 5:\n","            break\n","\n","        print('=========================================================')\n","        print('{0}\\t\\tStopping at epoch {1}, best epoch was {2} with MAP {3}'.format(datetime.datetime.now(),epoch_id, best_epoch, best_map))\n","        print('=========================================================')\n","        all_maps.append(best_map)\n","\n","        print('{0}\\t\\tPredicting for test set...'.format(datetime.datetime.now()))\n","        # Reload best weights\n","        model.set_weights(best_weights)\n","\n","    # Predict for test set\n","    p_test = model.predict([test_history, test_future], batch_size = 4096)\n","    test_predictions.append(np.swapaxes(p_test,0,1))\n","\n","    print('=========================================================')\n","    print('{0}\\t\\tAverage best MAP over all folds:\\t\\t{1}...'.format(datetime.datetime.now(), np.mean(all_maps)))\n","    print('=========================================================')\n","    #print('{0}\\t\\tGenerating submission...'.format(datetime.datetime.now()))\n","    # Geometric mean of predictions over folds\n","    p_test = np.prod(test_predictions, axis = 0) ** (1.0 / len(test_predictions))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["2020-01-12 14:32:47.954826\t----- FOLD 0 -----\n","Train on 285011 samples, validate on 71253 samples\n","285011/285011 - 20s - loss: nan - dense_16_loss: nan - dense_17_loss: nan - dense_18_loss: nan - dense_19_loss: nan - dense_20_loss: nan - dense_21_loss: nan - dense_22_loss: nan - dense_23_loss: nan - dense_24_loss: nan - dense_25_loss: nan - dense_16_binary_accuracy: 0.4545 - dense_17_binary_accuracy: 0.4566 - dense_18_binary_accuracy: 0.4616 - dense_19_binary_accuracy: 0.4758 - dense_20_binary_accuracy: 0.5032 - dense_21_binary_accuracy: 0.5552 - dense_22_binary_accuracy: 0.6128 - dense_23_binary_accuracy: 0.6579 - dense_24_binary_accuracy: 0.6955 - dense_25_binary_accuracy: 0.7229 - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan - val_dense_18_loss: nan - val_dense_19_loss: nan - val_dense_20_loss: nan - val_dense_21_loss: nan - val_dense_22_loss: nan - val_dense_23_loss: nan - val_dense_24_loss: nan - val_dense_25_loss: nan - val_dense_16_binary_accuracy: 0.4545 - val_dense_17_binary_accuracy: 0.4575 - val_dense_18_binary_accuracy: 0.4613 - val_dense_19_binary_accuracy: 0.4755 - val_dense_20_binary_accuracy: 0.5072 - val_dense_21_binary_accuracy: 0.5591 - val_dense_22_binary_accuracy: 0.6169 - val_dense_23_binary_accuracy: 0.6613 - val_dense_24_binary_accuracy: 0.6960 - val_dense_25_binary_accuracy: 0.7261\n","2020-01-12 14:33:13.200785\t\tValid\tMAP:\t0.0\tFPA:\t0.0\n","=========================================================\n","2020-01-12 14:33:13.201339\t\tStopping at epoch 0, best epoch was 0 with MAP 0.0\n","=========================================================\n","2020-01-12 14:33:13.201532\t\tPredicting for test set...\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-66b1efb5438f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{0}\\t\\tPredicting for test set...'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Reload best weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Predict for test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0mexpected_num_weights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mexpected_num_weights\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m       raise ValueError(\n\u001b[1;32m   1286\u001b[0m           \u001b[0;34m'You called `set_weights(weights)` on layer \"%s\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"urujZthnSAxe","colab":{"base_uri":"https://localhost:8080/","height":401},"outputId":"28ef5ec0-1d99-4154-b0c9-2960de3a2932","executionInfo":{"status":"error","timestamp":1578846031678,"user_tz":-60,"elapsed":1786,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}}},"source":["#Import the model with the highest performances\n","epochs = 14\n","\n","weights = np.asarray([(1 / val + sum((1 / (2*n)) for n in range(val + 1,11))) for val in range(1,11)])\n","weights /= weights.max()\n","\n","new_model = generate_model(train_history, train_future)\n","new_model.load_weights(modelpath + '/model_weights_epoch_{}.h5'.format(epochs))\n","new_model.compile(loss='binary_crossentropy',\n","                                   optimizer = tf.keras.optimizers.Adam(lr = 0.002, amsgrad = True),\n","                                   metrics=['accuracy'])\n"],"execution_count":13,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-1c5cdf6d89a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_future\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/model_weights_epoch_{}.h5'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m new_model.compile(loss='binary_crossentropy',\n\u001b[1;32m      9\u001b[0m                                    \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.002\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamsgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    232\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    233\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1220\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   1221\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    697\u001b[0m                        str(len(weight_values)) + ' elements.')\n\u001b[1;32m    698\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m   \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3321\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3322\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3323\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3324\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3325\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_handle_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m       \u001b[0mvalue_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[1;32m    821\u001b[0m           self.handle, value_tensor, name=name)\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \"\"\"\n\u001b[1;32m   1109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Shapes (91, 1024) and (88, 1024) are incompatible"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iezMnfAHSAxi","colab":{}},"source":["prediction = new_model.predict([test_history, test_future], batch_size = 4096)\n","MAP, FPA = evaluation_MAP_FPA(train_session_len, train_labels, np.swapaxes(np.round(prediction),0,1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"1bb3edd7-9921-4d3c-ba5b-8d158585b0e8","executionInfo":{"status":"ok","timestamp":1578766686876,"user_tz":-60,"elapsed":617,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}},"id":"-Y2lIsocSAxt","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(\"Performances--> MAP: {} | FPA: {}\".format(MAP,FPA))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Performances--> MAP: 0.5793433557729396 | FPA: 0.7734857745231073\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5zH-UFaASb_t","colab_type":"text"},"source":["### Not bad!\n","\n","With only few samples from the dataset and a good model we are able to perform a score similar to ~10th position of the rank in the global Leaderboard!"]},{"cell_type":"markdown","metadata":{"id":"bakrCZ3wv7Kr","colab_type":"text"},"source":["# 5- Concept Drift\n","\n","*Definition*\n","\n","Concept drift is a phenomena which occurs when dataset statistic properties change over time. This negative influnce predictive models that assume a statistic relathionship between input and output variables. When the concept drift occurs, the predictive performance can degrade significantly.\n","\n","\n","Even in this music streaming context, the presence of concept drift is tangible. For example, everyday new songs are published, the popularity vary, taste of users evolve over time, etc.\n","\n","To tackle this challange, and as well limit the training size, the solution proposed by the 5th classified of the Spotify challange suggests to train the model every day only on the log for that specific day.\n","\n","In order to understand how concept drift effects the performances of our model, we have decided to compare MAP e FPA metrics on both the proper test set, which date corresponds to the training dataset used to compute the optimal weights, and a test set obtained with the information of two months later."]},{"cell_type":"code","metadata":{"id":"w4dKJPtcV_7V","colab_type":"code","colab":{}},"source":["del train_future_a\n","del train_history_a\n","del train_labels_a\n","del train_session_len_a\n","\n","del test_future_a\n","del test_history_a\n","del test_session_len_a"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1vGh_zcc1e4V","colab_type":"code","colab":{}},"source":["#Load dataset test of September\n","\n","np.load = np_load_old\n","\n","import numpy as np\n","# save np.load\n","np_load_old = np.load\n","\n","# modify the default parameters of np.load\n","np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n","\n","\n","restored = np.load(processedpath + \"/trainSept.npz\")\n","train_history_a = restored['history_train']\n","train_future_a = restored['future_train']\n","train_labels_a = restored['labels_train']\n","train_session_len_a = restored['session_len_train']\n","\n","del restored\n","\n","# restore np.load for future normal usage\n","np.load = np_load_old"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hatQEZDCWRJc","colab_type":"code","outputId":"77928626-a054-4935-c522-c71cb1e317d0","executionInfo":{"status":"ok","timestamp":1578790245515,"user_tz":-60,"elapsed":605,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_future_a.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(175989, 10, 45)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"cG5thMndJCGP","colab_type":"code","colab":{}},"source":["np.load = np_load_old\n","\n","import numpy as np\n","# save np.load\n","np_load_old = np.load\n","\n","# modify the default parameters of np.load\n","np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n","\n","restored2 = np.load(processedpath + \"/testSept.npz\")\n","test_history_a = restored2['history_test']\n","test_future_a = restored2['future_test']\n","test_session_len_a = restored2['session_len_test']\n","\n","del restored2\n","\n","# restore np.load for future normal usage\n","np.load = np_load_old"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T_DRXafLQkpc","colab_type":"text"},"source":["## Performance on 2018/07/15"]},{"cell_type":"code","metadata":{"id":"tS9r2gM2clz5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"31cb21bf-cd5f-49ca-efe3-9e4482da4ec2","executionInfo":{"status":"ok","timestamp":1578846797111,"user_tz":-60,"elapsed":923,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}}},"source":["train_future.shape"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(178342, 10, 45)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"GB0Fsu1zGno4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":401},"outputId":"f83d60e8-badc-402c-e8cc-9d2f3ccfbd52","executionInfo":{"status":"error","timestamp":1578846809252,"user_tz":-60,"elapsed":1694,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}}},"source":["#Import the model with the highest performances\n","best_epochs = 14\n","\n","weights = np.asarray([(1 / val + sum((1 / (2*n)) for n in range(val + 1,11))) for val in range(1,11)])\n","weights /= weights.max()\n","\n","new_model = generate_model(train_history, train_future)\n","new_model.load_weights(modelpath + '/model_weights_epoch_{}.h5'.format(best_epochs))\n","\n","new_model.compile(loss='binary_crossentropy',\n","                                   optimizer = tf.keras.optimizers.Adam(lr = 0.002, amsgrad = True),\n","                                   metrics=['accuracy'])\n"],"execution_count":17,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-8cc626c19637>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_future\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/model_weights_epoch_{}.h5'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m new_model.compile(loss='binary_crossentropy',\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    232\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    233\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1220\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   1221\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    697\u001b[0m                        str(len(weight_values)) + ' elements.')\n\u001b[1;32m    698\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m   \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3321\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3322\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3323\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3324\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3325\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_handle_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m       \u001b[0mvalue_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[1;32m    821\u001b[0m           self.handle, value_tensor, name=name)\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \"\"\"\n\u001b[1;32m   1109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Shapes (91, 1024) and (88, 1024) are incompatible"]}]},{"cell_type":"code","metadata":{"id":"kVBKuY500xaO","colab_type":"code","colab":{}},"source":["prediction = new_model.predict([test_history, test_future], batch_size = 4096)\n","MAP, FPA = evaluation_MAP_FPA(train_session_len, train_labels, np.swapaxes(np.round(prediction),0,1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e-kQPdPsUIHf","colab_type":"code","outputId":"1bb3edd7-9921-4d3c-ba5b-8d158585b0e8","executionInfo":{"status":"ok","timestamp":1578766686876,"user_tz":-60,"elapsed":617,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(\"Performances--> MAP: {} | FPA: {}\".format(MAP,FPA))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Performances--> MAP: 0.5793433557729396 | FPA: 0.7734857745231073\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vpZzi3RES5rS","colab_type":"text"},"source":["# CONCEPT DRIFT - Prediction on a later day\n","## Performance on 2018/09/15\n"]},{"cell_type":"code","metadata":{"id":"rl9tBpLx1L_f","colab_type":"code","colab":{}},"source":["#Import the model with the highest performances\n","epochs = 1\n","\n","weights = np.asarray([(1 / val + sum((1 / (2*n)) for n in range(val + 1,11))) for val in range(1,11)])\n","weights /= weights.max()\n","\n","new_model = generate_model(test_history_a, test_future_a)\n","new_model.load_weights(modelpath + '/model_weights_epoch_{}.h5'.format(epochs))\n","new_model.compile(loss='binary_crossentropy',\n","                                   optimizer = tf.keras.optimizers.Adam(lr = 0.002, amsgrad = True),\n","                                   metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1yuaKRedFxek","colab_type":"code","colab":{}},"source":["prediction = new_model.predict([test_history_a, test_future_a], batch_size = 4096)\n","MAPa, FPAa = evaluation_MAP_FPA(train_session_len_a, train_labels_a, np.swapaxes(np.round(prediction),0,1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-vNBHWHkUh0C","colab_type":"code","outputId":"80009e6a-34e1-4ea9-d15b-f433d9a62b40","executionInfo":{"status":"ok","timestamp":1578790529954,"user_tz":-60,"elapsed":4559,"user":{"displayName":"Scalable MLDL","photoUrl":"","userId":"10782281522977554989"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(\"Performances--> MAP: {} | FPA: {}\".format(MAPa,FPAa))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Performances--> MAP: 0.0 | FPA: 0.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_yPCLBcESqGe","colab_type":"text"},"source":["# Feature engineered dataset: New training & predictions\n","\n","todo"]},{"cell_type":"code","metadata":{"id":"FKiDHZbKStix","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}